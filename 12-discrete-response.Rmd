---
output_yaml:
  - _output.yml
---


<!-- Outline: -->

<!-- 1) Preamble: Intro paragraph to classification in general, both 0/1 and multiple categories. Discuss different, all for solving the same problem. -->

<!-- 2) EDA of cces (meaning just the columns we care about). Start with the usual EDA of the subset of cces data which we are using (only 2018 for now), which will be state, age, gender, race, education.   -->

<!-- 3) Logistic Regression. Go through all our themes. -->

<!-- 4) CART -->

<!-- 5) Random Forest -->

<!-- 6) ? -->

<!-- Other stuff: -->

<!-- * Only use gt() -->

<!-- * Get rid of empirical logit nonsense.  -->

<!-- * No more bootstrap. We have demonstrated enough times that, with a bootstrap, you get the same answer as the lm/glm results. We don't need to prove it again. -->

<!-- * glm example should be rstanarm from start-to-finish. That is, you can just throw away more or less the whole thing and start from scratch. Cover the highlights on Gelman. -->

<!-- * Let's use cces and create a new variable which is 1 if you are Conservative or Very Conservative and 0 otherwise. We want to explore what things are associated with being Conservative. -->

<!-- * First a regression using just data from 2018 with rstanarm with conservative ~ age, gender, race, education. (Maybe leave out education because it is an ordered factor?) Within just this example, we can go through all of our themes.Rmd issues. First do gender, then age, then multivariate. Make the model, discuss meaning of the coefficients. Then, after multivariate, you are doing posterior_linpred(), posterior_pred(). -->

<!-- * Then CART using tidymodels on exactly this data and this problem. Maybe we go slowly, showing how CART works with one variable (numeric and categorical) first. Or maybe we just go straight to the full model.  -->

<!-- * Then random forests using tidymodels on exactly this data and this problem. -->

<!-- * Do we have time to explore Gelman's magic trick: Showing the values of key coefficients when the model is refit to each year separately? That is, how much is age associated with being conservative over time? I hope so! But maybe not. -->

<!-- * Do we have time to do a model which predicts a category with more than 0/1 possible values? I am not sure. If we were going to, we would want to pick something that is not an ordered category, I think. In this data, ony race would meet that criteria. Let's revisit this later. -->


# Discrete Response {#discrete-response}


Many questions have binary answers. These questions can be answered by any pair of two answers (such as yes/no). Two examples of these questions include: 
  a. Are students with below average grades more likely to binge drink?
  b. Is exposure to a particular chemical associated with a cancer diagnosis?
 
**Binary responses** take on only two values: success ($Y=1$) or failure ($Y=0$), yes ($Y=1$) or no ($Y=0$), et cetera. Examples (a) and (b) above have binary responses (Does a student binge drink?  Was a patient diagnosed with cancer?). Binary responses are one of the most common types of data that statisticians encounter.  We are often interested in modeling the probability of success, $p$, based on a set of covariates. As with regression, there are two broad categories of problems: *modeling for explanation* and *modeling for prediction*. Although terminology varies across fields, "regression" is generally used for situations in which our *dependent variable* is continuous. "Classification" applies to cases in which the dependent variable is binary.

All of the new models touched upon in this chapter speak to the wider discussion of fitting our model structures to better fit what is being measured in the world. Using logistical regressions of any type more accurately describe the measured effect when dealing with binary variables.  

In this chapter, we will look at three common techniques of **classification** of binary data.  First, we will consider logistic regression, which is similar conceptually to the linear regression models we considered in Chapters \@ref(pitfalls) and \@ref(continuous-response).  Second, we will consider classification and regression trees (CART).  Third, we will discuss random forests.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(skimr)
library(PPBDS.data)
library(knitr)
library(kableExtra)
library(gridExtra)
library(gt)
```

## Exploratory Data Analysis (EDA)

Before we start modelling, let's perform some exploratory analysis on the dataset we'll be working with, cces. Cces stands for the Cooperative Congressional Election Study, a study regarding the approval rating of individual voters to their sitting president. Each row captures one voter, some of their demographic information, and how highly they approve (or disaprprove) of the president. We will tweak the data by only looking at observations recorded in the year 2018 so that all the responses are about the same president.

```{r}
cces_ch12 <- cces %>%
  filter(year == 2018)
```

Let's first look at the raw data values by either looking at `cces_ch12` using RStudio's spreadsheet viewer or by using the `glimpse()` function from the **dplyr** package:

```{r}
glimpse(cces_ch12)
```

From this, we can gather that there are 16 variables. Notably, there are 60,000 observations even after filtering only for the year 2018. 

Let's also display a random sample of 5 rows of the 60,000 rows. We will be displaying these 5 random observations using a gt() table. Before that, though, we will select the variables that are currently of interest to us. 

```{r, eval = FALSE}
cces_ch12 <- cces_ch12 %>%
  select(state, age, gender, race, education, ideology, approval)

cces_ch12 %>% 
  sample_n(size = 5)
```

```{r}
cces_ch12 %>%
  sample_n(5) %>%
  gt() %>%
  cols_label(
    state = md("State"),
    age = md("Age"),
    gender = md("Gender"),
    race = md("Race"),
    education = md("Education"),
    ideology = md("Ideology"),
    approval = md("Approval of President")) %>%
  tab_source_note(md("A random sample of 5 out of the 60,000 voters"))
```

Now that we’ve looked at the raw values in `cces_ch12` and have a sense of the data, let’s compute summary statistics. Let’s use the `skim()` function from the `skimr` package.

```{r}
cces_ch12 %>% 
  skim()
```

You'll notice that we are missing data for our ideology and approval variables. The `complete_rate` column tells us that approval has 3% missing observations and ideology has 0.7% missing observations. 

To complete our exploratory data analysis, let's create some data visualizations. 

The primary response variable left in our dataset is `approval`, a numeric variable from 1-5 with 5 representing the highest approval of the president. So, let's start by looking at the overall distribution of `approval`.

```{r}
cces_ch12 %>%
  filter(is.na(approval) == FALSE) %>% #Removing NA observations
  ggplot(aes(x = approval)) +
  geom_bar(fill = "red") + 
  labs(y = "Count",
       x = "Presidential Approval",
       title = "Presidential Approval in 2018") 
```

According to this graph, the approval of the president in 2018 is very polarized: the peaks are at 1 and 5 and no one reported with a neutral 3. There seems to be more disapproval overall.
To make things more interesting, let's facet this approval variable by gender and then race.

```{r}
cces_ch12 %>%
  filter(is.na(approval) == FALSE) %>%
  ggplot(aes(x = approval, fill = gender)) +
  geom_bar() + 
  labs(y = "Count",
       x = "Presidential Approval",
       title = "Presidential Approval in 2018 by Gender") +
  facet_wrap(~ gender) + 
  theme(legend.position = "none")
```

The bimodal, polarized distribution holds across both genders. However, it seems that females have more higher rates of disapproval (more 1s and 2s). 

```{r}
cces_ch12 %>%
  filter(is.na(approval) == FALSE) %>%
  ggplot(aes(x = approval, fill = race)) +
  geom_bar() + 
  labs(y = "Count",
       x = "Presidential Approval",
       title = "Presidential Approval in 2018 by Race") +
  facet_wrap(~ race) + 
  theme(legend.position = "none") +
  scale_y_log10()
```

Note the log10 scale on the y-axis; this was done so that we can see clearly the distribution for races that have very few observations compared to white individuals. We can see here that the bimodel distribution is present across all races, with slight variations from race to race. 

Now, let's take a look at our ideology variable. `ideology` has values "Very Liberal", "Liberal", "Moderate", "Conservative", "Very Conservative", and "Not Sure". Let's make these variables numeric so that they are easier to work with by recoding them to 1, 2, 3, 4, 5, and NA, respectively. 

```{r}
cces_ch12 <- cces_ch12 %>%
  mutate(ideology = case_when(
    ideology == "Very Liberal" ~ 1,
    ideology == "Liberal" ~ 2,
    ideology == "Moderate" ~ 3,
    ideology == "Conservative" ~ 4,
    ideology == "Very Conservative" ~ 5,
    ideology == "Not Sure" ~  NA_real_))
```

Now that we have ideology as a numeric variable, we will make a barplot to get a sense of the distribution.

```{r}
cces_ch12 %>%
  filter(is.na(ideology) == FALSE) %>%
  ggplot(aes(x = ideology)) + 
  geom_bar(fill = "blue") + 
  labs(y = "Ideology",
       title = "Ideology in CCES 2018")
```

There is a unimodal disitribution in ideology, with most respondents being moderate. This is interesting, considering how polarized the presidential approval was before. Let's create a jitterplot between `ideology` and `approval` to look into this more. 

```{r}
cces_ch12 %>%
  filter(is.na(ideology) == FALSE) %>%
  filter(is.na(approval) == FALSE) %>%
  ggplot(aes(x = ideology, y = approval)) + 
  geom_jitter() + 
  labs(y = "Presidential Approval",
       x = "Ideology",
       title = "Presidential Approval by Ideology")
```

<!-- MB: just need to talk about this to finish EDA. Holding off until we figure out why there are literally no 3s-->

## Logistic regression

### What is logistic regression?

Now that we know our dataset a little better, let's begin our first way of modelling binary/discrete data: logistic regressions. 

Figure \@ref(fig:OLSlogistic) illustrates a data set with a binary (0 or 1) response ($Y$) and a single continuous predictor ($X$).  The blue line is a linear regression to model the probability of a success ($Y=1$) for a given value of $X$. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1. Probabilities can only range from 0 up to and including 1 as these represent a 0% and 100% chance of an event happening, respectively.

The red curve is the *logistic regression* curve.  Note that its characteristic "S" shape always produces predicted probabilities between 0 and 1.  Here is the formula for a logistic regression:

Where $p$ is the probability of a "yes" or "success" for a given set of predictors $X$.

<!-- Revisit nomenclature after chapter 5 -->

```{r, OLSlogistic, fig.align="center", out.width="60%", fig.cap='Linear vs. logistic regression models for binary response data.', echo=FALSE, warning=FALSE, message=FALSE}

set.seed(0)
dat <- tibble(x=runif(200, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(200, 1, p),
                  logit=log(p/(1-p)))

ggplot(dat, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y, color = "blue"), method = "lm", se=FALSE) +
  geom_line(aes(y = p, color = "red")) +
  scale_color_manual(name = 'Regression model', 
         values = c('blue', 'red'), 
         labels = c('Linear', 'Logistic'), guide = 'legend') +
  theme_minimal()
```

<!-- DK: How does the math work here? log(p/1-p) seems, to me, to map 0,1 to 0,infinity. -->

The mathematical function $log\left(\frac{p}{1 - p}\right)$ is called the *logit function* and it transforms variables from the space $(0, 1)$ (like probabilities) to $(-\infty, \infty)$.  The inverse of that function, the *standard logistic function*, is $\frac{1}{1 + e^{-x}}$ and transforms variables from the space $(-\infty, \infty)$ to $(0, 1)$.  From that latter function's name we get the terminology of *logistic regression*.

### One categorical explanatory variable

Let's start our modeling by predicting `approval` with a single categorical explanatory variable. However, there's one problem: approval is not a binary variable and, therefore, this wouldn't be a logistic regression. In order to make this a logistic regression, we have to turn approval into a binary variable. 1-2 will be coded to 0 to signify disapproval and 3-5 will be coded to 1 for approval.

```{r}
cces_ch12 <- cces_ch12 %>%
  mutate(approval = case_when(
    approval == 1 ~ 0,
    approval == 2 ~ 0,
    approval == 3 ~ 1,
    approval == 4 ~ 1,
    approval == 5 ~ 1))
```

<!-- MB: Jumping straight to multiple categories here rather than just a binary one. Should I do one for gender? -->

We'll start by modeling our new binary `approval` with the categorical variable `race`.  As we'll see, the syntax for running a logistic regression in R is very similar to that for running a linear regression.  In fact, we'll follow the same basic steps:

1. We first "fit" the logistic regression model using the `glm(y ~ x, family, data)` function and save it in `race_model`.
2. We get the regression table by applying the `tidy()` function from the **broom** package to `race_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.

Note that the key difference is that instead of using `lm()`, we are now using `glm()`.  `glm()` operates very similarly to `lm()`, but it has an additional argument: `family`.  To run a logistic regression, we use `family = binomial`. This means that it will be modelled along the red line in Figure \@ref(fig:OLSlogistic) rather than the blue line.

Next, let's fit a model and `tidy()` it:

```{r, eval=FALSE}
race_model <- glm(approval ~ race, family = binomial, data = cces_ch12)
race_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

```{r, echo=FALSE}
race_model <- glm(approval ~ race, family = binomial, data = cces_ch12)
tidy(race_model) %>%
  select(term, estimate) %>%
  gt()

```
The intercept here is the omitted category, Asian.

How can we interpret the coefficients?  Unlike linear regressions, these coefficients aren't directly interpretable.  Recall our logistic regression model equation:

\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]

A one-unit change in $X$ thus is associated with a one-unit change in $log\left(\frac{p}{1 - p}\right)$, where $p$ is the predicted probability of success. It is hard to understand intuitively what this means. We can directly calculate all the possible values of $p$ this model by using the *standard logistic function*:

\[
p = \frac{1}{1 + e^{-(\beta_0+\beta_1X)}} 
\]

We can first use this formula to fill $b_0$ with the intercept (representing Asians) and omit the $b_1$ as we are solving for the probability of an Asian American approving of the President.


\[
p_{pres\_approve} = \frac{1}{1 + e^{-(-1.06)}} = 0.257
\]

We can then fill in the $b_1$ term to calculate the probabilities of all races:
- White: $\frac{1}{1 + e^{-(-1.06 + 0.99)}} = 0.482$
- Black: $\frac{1}{1 + e^{-(-1.06-1.01)}} = 0.112$
- Hispanic: $\frac{1}{1 + e^{-(-1.06 + 0.036)}} = 0.264$
- Middle Eastern: $\frac{1}{1 + e^{-(-1.06 + 0.007)}} = 0.259$
- Native American: $\frac{1}{1 + e^{-(-1.06 + 1.1)}} = 0.51$
- Mixed: $\frac{1}{1 + e^{-(-1.06 + 0.071)}} = 0.271$
- Other: $\frac{1}{1 + e^{-(-1.06 + 1.3)}} = 0.559$

<!-- We can then interpret the effect of moving from one category to another.  For example, the predicted probability of a White voter approving of the President is 0.225 greater than an Asian voter. Note that we could have obtained this through the *divide by 4 rule*:  $0.482 / 4 \approx 0.16$. -->
<!-- MB: Hold off until numeric variable for divide by 4?-->

However, there is a way to calculate these predicted probabilities using R without doing all of the math of the standard logistic function..

We have previously defined the following three concepts for a linear regression:

1. Observed values $y$, or the observed value of the outcome variable
2. Fitted values $\widehat{y}$, or the value on the regression line for a given $x$ value
3. Residuals $y - \widehat{y}$, or the error between the observed value and the fitted value

We obtained these values and other values using the `augment()` function from the **broom** package. Recall too that we used the `.se.fit` column to construct confidence intervals.  We'll see here how we can apply these same concepts to logistic regression.

```{r, eval=FALSE}
regression_points <- race_model %>%
  augment() %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points
```

```{r, echo=FALSE}
regression_points <- augment(race_model) %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(approval, race,  .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 60,000 voters)"))
```

The syntax is the same, but the interpretation has to change, since the `.fitted`, `conf.low`, and `conf.high` columns are all on the logit scale.  While we could try to interpret these values, `augment()` has the argument `type.predict = "response"` that allow us to present the results in terms of *predicted probabilities*:

```{r, eval=FALSE}
regression_points <- race_model %>%
  augment(type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points
```

```{r, echo=FALSE}
regression_points <- augment(race_model,
                             type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 60,000 voters)"))
```

Now each of the `.fitted` values is a *predicted probability* of a Democratic victory from our model for a particular district and the confidence intervals are confidence intervals around that predicted probability. You'll notice how the fitted value in this table is the same as the probabilities we calculated by hand using the standard logistical function.

You may be wondering how to interpret the residuals.  The residuals reported by `augment()` for a logistic regression are called *deviance residuals*.  A deviance residual can be calculated for each observation using:

\[
\textrm{d}_i = 
\textrm{sign}(Y_i-\hat{p_i})\sqrt{-2 [ Y_i \text{log} \hat{p_i} + (1 - Y_i) \text{log} (1 - \hat{p_i}) ]}
\]

where $Y_i$ is the actual outcome and $p_i$ is the predicted probability from the logistic regression model.

The sum of the individual deviance residuals is referred to as the **deviance** or **residual deviance**. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred.

However, you can also have `augment()` report residuals as differences between the observed outcome and the predicted probabilities by using `type.residuals = "response"`:

```{r, eval=FALSE}
regression_points <- race_model %>%
  augment(type.predict = "response",
          type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points
```

```{r, echo=FALSE}
regression_points <- augment(race_model,
                             type.predict = "response",
                             type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(approval, race, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 60,000 voters)"))
```

Now, the `.resid` value is the difference between the actual outcome (`approval`) and the predicted probability.

### One numerical explanatory variable

We'll now predict `approval` with a single numerical explanatory variable, `age`. 

1. We first "fit" the logistic regression model using the `glm(y ~ x, family, data)` function and save it in `age_model`.
2. We get the regression table by applying the `tidy()` function from the **broom** package to `age_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.

```{r, eval=FALSE}
age_model <- glm(approval ~ age, family = binomial, data = cces_ch12)
age_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

```{r, echo=FALSE}
age_model <- glm(approval ~ age, family = binomial, data = cces_ch12)
tidy(age_model,
     conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  gt() %>%
  tab_source_note(md("Logistical Regression Table"))

```

How do we interpret the coefficients in this model?  Since the `age` coefficient is positive, that means that each additional year of age is associated with a higher approval of the President in 2018. 

If we wanted to learn the predicted probabilities for any given value of `age`, we can plug in our values of `age` into the standard logistic function, like so:

\[
p_{dem\_win} = \frac{1}{1 + e^{-(-1.43 + 0.023 \times year)}} 
\]

For example, the predicted probability of a 90-year-old approving of the President is 65.4% while the predicted probability of a 19-year-old approving of the President is 27%.

Note that since this is not a linear function, a one-unit change in `year` will be associated with various one-unit changes in `year`, depending on what `year` you are starting from.  Recall the figure we used to start the chapter:

A one-unit change in $X$ thus is associated with a one-unit change in $log\left(\frac{p}{1 - p}\right)$, where $p$ is the predicted probability of success. It is hard to understand intuitively what this means. We can directly calculate all the possible values of $p$ this model by using the *standard logistic function*:

```{r, echo = FALSE, message = FALSE}
ggplot(dat, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y, color = "blue"), method = "lm", se=FALSE) +
  geom_line(aes(y = p, color = "red")) +
  scale_color_manual(name = 'Regression model', 
         values = c('blue', 'red'), 
         labels = c('Linear', 'Logistic'), guide = 'legend') +
  theme_minimal()
```

A linear regression line (in blue) has a constant slope, which means that no matter what $x$ you start with, the effect of going from $x$ to $x + 1$ on $y$ is the same number.  However, take a look at the logistic regression curve (in red).  The value of the slope for very high or very low values of $x$ is smaller (approaching 0 as $x$ tends to negative or positive infinity), while the slope in the middle of the curve is highest.  The steepest part of the curve corresponds to that part of the curve where the predicted probability equals 0.5.  That is, the effect of a one-unit change in $x$ is the highest when the predicted probability for that $x$ is close to 0.5 and smallest when the predicted probability for that $x$ is close to 0 or 1.

You can always use R to calculate the predicted probabilities for any value of $x$ and thus calculate the effect of moving from a particular $x$ to $x + 1$.  But this can get complicated.  In particular, once you start employing logistic regression with multiple predictors, the effect of a one-unit change in a predictor $x$ depends not only on $x$, but on the values of all the other predictors in your model!  You can always plug in all the coefficients and values of your predictors into the logistic function to calculate predicted probabilities, but if you don't do that, how can you interpret the coefficients?

Here is where we can use the *divide by 4 rule* that we discussed before.  A logistic regression coefficient divided by 4 is the effect of that variable at the steepest part of the logistic regression curve, which, as we just saw, corresponds to where the predicted probability is 0.5.

Therefore, you can divide a logistic regression coefficient by 4 to get an upper bound on the effect a one-unit change in that predictor will have on the predicted probability of your outcome.  In this case, the approximation tells us that each additional year of age is associated with about a $0.023 / 4 = 0.005$ increase in the predicted probability of a voter approving of the President.

While `race_model` and `age_model` both tell us something interesting, we could learn more with an *interaction model* that includes both of our predictors.

### One numerical and one categorical explanatory variable

We'll now predict `approval` with two variables, `race` and `age`, as well as the interaction between the two.

This time, we'll do it slightly differently. Rather than using `glm()` like we did in the last two examples, we'll instead be using tools from a package called **tidymodels**. Rather than having to use a different function each time we construct a model (choosing between lm(), glm(), and other modelling function), **tidymodels** streamlines the synthax for any model we want to construct.

```{r, message = FALSE}
library(tidymodels)
```

**tidymodels** includes many packages, but we'll start by showing how to use **parsnip** to fit a logistic regression. 

First, in the **tidymodels** workflow, we have to save the *model specification*.  We do that using two functions: `logistic_reg()` and `set_engine()`.

```{r}
logistic_mod <- logistic_reg() %>%
  set_engine("glm")
```

`logistic_reg()` says that we want to fit a logistic regression, and `set_engine("glm")` specifies that we want to do it using `glm()`.  Behind the scenes, **parsnip** uses many other packages to fit its models, but by unifying the syntax, it means that you don't have to memorize how a lot of different functions work.

Note that our new object, `logistic_mod`, doesn't contain our data or a formula.  In order actually to fit our model, we need to feed `logistic_mod` to a function called `fit()`.  `fit()` is the general purpose function in **parsnip** for fitting any model specification.  It takes as its first argument the model specification, but otherwise it operates similarly to `lm()` and `glm()`. We have to wrap `approval` in `factor()`, because `fit()` is more careful than `glm()` in requiring that classification models actually have categorical outcomes.


```{r}
logistic_fit <- fit(logistic_mod,
                    factor(approval) ~ race * age,
                    data = cces_ch12)
```

One we have fit the model, how can we use it?  The `glm` object is still stored in `logistic_fit$fit`, so we can access that and use `tidy()`, just like we did before:

```{r}
logistic_fit$fit %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  gt()
```

As you can see, this generates the same results as when we used `glm()` directly. Now we can see how the effect of `age` varies by `race`. Looking at predicted probabilities of these numbers can put this model in perspective.  Let's use `augment()` to generate the predictions.  Remember that `type.predict = "response"` and `type.residuals = "response"` put the fitted values and the residuals on the probability scale.

```{r, eval=FALSE}

interact_model <- logistic_fit$fit %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)

regression_points <- house_interact_model %>%
  augment(type.predict = "response",
          type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points

```

```{r, echo=FALSE}

regression_points <- augment(house_interact_model,
                             type.predict = "response",
                             type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (First 10 out of 4,201 district-years)",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))

```

We can also use `augment` to make predictions for years that aren't in our data.  What would our model predict for the 2020 elections?  We use the `newdata` argument in `augment()` to make these predictions.

```{r}
house_interact_model %>%
  augment(newdata = tibble(year = rep(2020, 4),
                           region = c("Midwest", "Northeast", "South", "West")),
          type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit)
```

These can easily be plotted using `ggplot()`:

```{r}
house_interact_model %>%
  augment(newdata = tibble(year = rep(2020, 4),
                           region = c("Midwest", "Northeast", "South", "West")),
          type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  ggplot(aes(y = .fitted,
             ymin = conf.low,
             ymax = conf.high,
             x = region)) +
  geom_point() +
  geom_errorbar() +
  labs(y = "Predicted probability of a Democratic victory (2020)",
       x = "") +
  theme_classic()
```

<!-- Here is a great place to discuss unknown unknowns --> 






<!--   ^^^^ NEW WORK    vvvv OLD WORK  -->
