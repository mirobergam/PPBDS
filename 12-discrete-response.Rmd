---
output_yaml:
  - _output.yml
---

# Discrete Response {#classification}

Many questions have binary answers. These questions can be answered by any pair of two answers (such as yes/no). Two examples of these questions include: 
  a. Are students with below average grades more likely to binge drink?
  b. Is exposure to a particular chemical associated with a cancer diagnosis?
 
**Binary responses** take on only two values: success ($Y=1$) or failure ($Y=0$), yes ($Y=1$) or no ($Y=0$), et cetera. Examples (a) and (b) above have binary responses (Does a student binge drink?  Was a patient diagnosed with cancer?). Binary responses are one of the most common types of data that statisticians encounter.  We are often interested in modeling the probability of success, $p$, based on a set of covariates. As with regression, there are two broad categories of problems: *modeling for explanation* and *modeling for prediction*. Although terminology varies across fields, "regression" is generally used for situations in which our *dependent variable* is continuous. "Classification" applies to cases in which the dependent variable is binary.

All of the new models touched upon in this chapter speak to the wider discussion of fitting our model structures to better fit what is being measured in the world. Using logistical regressions of any type more accurately describe the measured effect when dealing with binary variables.  

In this chapter, we will look at three common techniques of **classification** of binary data.  First, we will consider logistic regression, which is similar conceptually to the linear regression models we considered in Chapters \@ref(regression) and \@ref(multiple-regression).  Second, we will consider classification and regression trees (CART).  Third, we will discuss random forests.

<!-- MB: Should the ref() thing be replaced by a chapter number? -->

<!-- MB: Only include this section if it uses new/unique packages -->

### Needed packages {-}

Recall from our discussion in Section \@ref(tidyverse-package) that loading the **tidyverse** package by running `library(tidyverse)` loads the following commonly used data science packages all at once:

* **ggplot2** for data visualization
* **dplyr** for data wrangling
* **tidyr** for converting data to "tidy" format
* **readr** for importing spreadsheet data into R
* As well as the more advanced **purrr**, **tibble**, **stringr**, and **forcats** packages

Read Section \@ref(packages) for information on how to install and load R packages. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(skimr)
library(fivethirtyeight)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text:
library(knitr)
library(kableExtra)
library(gridExtra)
library(gt)
```
   
## Logistic regression

### What is logistic regression?

Figure \@ref(fig:OLSlogistic) illustrates a data set with a binary (0 or 1) response ($Y$) and a single continuous predictor ($X$).  The blue line is a linear regression to model the probability of a success ($Y=1$) for a given value of $X$. With a binary response, the linear regression has an obvious problem: it can produce predicted probabilities below 0 and above 1. Probabilities can only range from 0 up to and including 1 as these represent a 0% and 100% chance of an event happening, respectively.

The red curve is the *logistic regression* curve.  Note that its characteristic "S" shape always produces predicted probabilities between 0 and 1.  Here is the formula for a logistic regression:

Where $p$ is the probability of a "yes" or "success" for a given set of predictors $X$.

<!-- MB: P should be Y? -->

```{r, OLSlogistic, fig.align="center", out.width="60%", fig.cap='Linear vs. logistic regression models for binary response data.', echo=FALSE, warning=FALSE, message=FALSE}

set.seed(0)
dat <- tibble(x=runif(200, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(200, 1, p),
                  logit=log(p/(1-p)))

ggplot(dat, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y, color = "blue"), method = "lm", se=FALSE) +
  geom_line(aes(y = p, color = "red")) +
  scale_color_manual(name = 'Regression model', 
         values = c('blue', 'red'), 
         labels = c('Linear', 'Logistic'), guide = 'legend') +
  theme_minimal()
```

<!-- DK: How does the math work here? log(p/1-p) seems, to me, to map 0,1 to 0,infinity. -->
<!-- MB: I also need to do more research to understand this -->

The mathematical function $log\left(\frac{p}{1 - p}\right)$ is called the *logit function* and it transforms variables from the space $(0, 1)$ (like probabilities) to $(-\infty, \infty)$.  The inverse of that function, the *standard logistic function*, is $\frac{1}{1 + e^{-x}}$ and transforms variables from the space $(-\infty, \infty)$ to $(0, 1)$.  From that latter function's name we get the terminology of *logistic regression*.

### House elections: exploratory data analysis

What affects whether a Democrat or Republican wins a race in the U.S. House of Representatives? This is an example of a binary response: either a Democrat wins (and a Republican loses) or a Republican wins (and a Democrat loses).^[It is rare that third party candidates mount serious bids in U.S. House elections, so it isn't a much of an oversimplication to think of the variable as binary.] In this section, we are going to consider several models predicting Democratic victory in House races. First, we will consider a single categorical variable as a predictor, the region in which a district lies: Midwest, Northeast, South, or West. Second, we will consider a single continuous variable: year. Finally, we fill fit a model that contains both separately, and than also an interaction between the two.

<!-- DK: Too much cognitive load to create these datasets on the fly. They need to be ready to go. -->

The data on House election results from 1976 to 2018 can be found in the `house_results` data frame in **politicaldata** package.  We'll create a version of this data frame called `house_ch12` that creates a new column `dem_win` that notes for each state if the Democratic candidate in a congressional district received more votes than the other candidates.  We'll also join it with the `state_info` data frame in the **fivethirtyeight** package to add the `region` of each state.

```{r}
library(politicaldata)

house_ch12 <- house_results %>%
  
  # Create dem_win variable
  
  mutate(dem = ifelse(is.na(dem), 0, dem),
         other = ifelse(is.na(other), 0, other),
         rep = ifelse(is.na(rep), 0, rep),
         dem_win = ifelse(dem > rep & dem > other, 1, 0)) %>%
  
  # Rename to join with state_info
  
  rename(state_abbrev = state_abb) %>%
  left_join(state_info) %>%
  select(region, state, district, year, dem_win)
```

<!-- DK: We use same dataset from other chapter, we need to modify this. But we won't have looked at everything! So, look at other stuff. Some is obvious, especially our new Y variable. But we can also look at stuff like number of elections per state, per year. Which state had the biggest districting rise and when? Indeed, the data exploration section in each chapter provides an occasion to learn more advanced R tricks. -->

Recall the three common steps in an exploratory data analysis we saw in Subsection \@ref(model1EDA):

1. Looking at the raw data values.
2. Computing summary statistics.
3. Creating data visualizations.

Let's first look at the raw data values by either looking at `house_ch12` using RStudio's spreadsheet viewer or by using the `glimpse()` function from the **dplyr** package:

```{r}
glimpse(house_ch12)
```

Let's also display a random sample of 5 rows of the 9,557 rows corresponding to different district-years. Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

```{r, eval = FALSE}
house_ch12 %>% 
  sample_n(size = 5)
```

```{r, echo = FALSE}

house_ch12 %>%
  sample_n(5) %>%
  gt() %>%
  cols_label(
    region = md("Region"),
    state = md("State"),
    district = md("Distrct"),
    year = md("Year"),
    dem_win = md("Democrat Win?")) %>%
  tab_source_note(md("A random sample of 5 out of the 4,201 district-years"))

```

Now that we’ve looked at the raw values in our `house_ch12` data frame and got a sense of the data, let’s compute summary statistics. As we've done in our exploratory data analyses before, let’s use the `skim()` function from the `skimr` package, being sure to only select() the variables of interest in our model:

```{r}
house_ch12 %>% 
  select(dem_win, region, year) %>% 
  skim()
```

Observe that we have no missing data, that we have 9,557 observations, and that the mean of `dem_win` is 0.54, indicating that Democrats won 54% of the House elections in this period (1976--2018).

Let’s now perform the last of the three common steps in an exploratory data analysis: creating data visualizations.

For our categorical variable, we'll look at histograms of `dem_win` faceted by `region`:

```{r}
house_ch12 %>%
  ggplot(aes(x = dem_win)) +
  geom_histogram(bins = 10, color = "white") +
  labs(x = "Democratic victory percentage, 1976-2018", 
       y = "Number of districts",
       title = "Histogram of distribution of Democratic victories by House district") +
  facet_wrap(~ region) +
  theme_minimal()
```

Wait!  That doesn't tell us very much, because our outcome variable only takes two values, 0 and 1.  Let's instead `group_by(district)` and `summarize()` to get a better sense of the distributions:

```{r}
house_ch12 %>%
  group_by(region, district) %>%
  summarize(dem_win = mean(dem_win)) %>%
  ggplot(aes(x = dem_win)) +
  geom_histogram(bins = 10, color = "white") +
  labs(x = "Democratic victory percentage, 1976-2018", 
       y = "Number of districts",
       title = "Histogram of distribution of Democratic victories by House district") +
  facet_wrap(~ region) + 
  theme_minimal()
```

This is much more informative!  We can see that the Midwest is highly bimodal, with many districts either electing Democrats for every year in this period or for none.  The Northeast and West have many districts that always elect Democrats but few that never do.  The South is the only region with a peak in the middle, indicating that there are many districts in the South that elected Democrats for about half the time during 1976-2018.

What happens if we create a scatterplot of our outcome variable `dem_win` and a continuous predictor, `year`?

```{r}
house_ch12 %>%
  ggplot(aes(x = year, y = dem_win)) +
  geom_point() +
  labs(x = "Year", y = "Democratic Victory") +
  geom_smooth(method = "lm", se = FALSE)
```

<!-- DK: The below seems like good stuff, but I don't really understand it. Needs more words. Why is log(p/1-p) an interesting thing. Is it really? -->

<!-- MB: I think I clarified it more, but may need more explaining. p/1-p is just the function that transforms data into logit form, which is why it's so frequently mentioned. Could be abstracted out some more though. -->

This is incomprehensible!  When dealing with binary data, it is more helpful to construct an *empirical logit* plot instead of a regular scatterplot. An *empirical logit* is a linear model on logit-trasnformed data rather than a generalized linear model. The steps for constructing such a plot are as follows:

1. `group_by` your continuous variable.
2. `summarize` the percentage of successes in your outcome variable.
3. Calculate the *empirical logit* for each group, using the logit function: $log\left(\frac{p}{1 - p}\right)$. This step transforms the data to fit the form of a logistical regression with a binary outcome. 
4. Plot the results.

The logit function exists in base R as `qlogis()`. Let's look at the empirical logit plot:

```{r}
house_ch12 %>%
  group_by(year) %>%
  summarize(perc_dem_win = mean(dem_win),
            emplogit = qlogis(perc_dem_win)) %>%
  ggplot(aes(x = year, y = emplogit)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Year",
       y = "Empirical logits") +
  theme_classic()
```

Now we see that after the logit transformation, there is roughly a linear relationship between our outcome variable and our explanatory variable `year`.  This means that a logistic regression model makes sense.  Some of the most visually apparent outliers will be familiar to students of American politics: 1994 (the "Republican Revolution"), 2008 (Obama's first election), and 2018. Yet in general it appears that over time the Democrats have performed worse in House elections.

We can follow the same steps to look at this relationship within Census regions (Midwest, Northeast, South and West):
 
```{r}
house_ch12 %>%
  group_by(region, year) %>%
  summarize(perc_dem_win = mean(dem_win),
            emplogit = qlogis(perc_dem_win)) %>%
  ggplot(aes(x = year, y = emplogit)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = "Year",
       y = "Empirical logits") +
  facet_wrap(~ region) +
  theme_classic()
```

We see roughly linear relationships after the logit transformation within-region as well, although the relationship looks more linear in the Midwest and South than in the Northeast and West.  We see that the Democratic Party's overall decline in House races is driven by the South and to a lesser extent the Midwest; Democratic performance has on average improved in the Northeast and West.  The sharp negative slope in the South will not be surprising if one is familiar with the collapse of the "Solid South."

### One categorical explanatory variable

Let's start our modeling by predicting `dem_win` with a single categorical explanatory variable.  We'll start with a binary variable for whether an observation is in the South and we will progress to the categorical variable `region`.  As we'll see, the syntax for running a logistic regression in R is very similar to that for running a linear regression.  In fact, we'll follow the same basic steps:

1. We first "fit" the logistic regression model using the `glm(y ~ x, family, data)` function and save it in `house_region_model`.
2. We get the regression table by applying the `tidy()` function from the **broom** package to `house_region_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.

Note that the key difference is that instead of using `lm()`, we are now using `glm()`.  `glm()` operates very similarly to `lm()`, but it has an additional argument: `family`.  To run a logistic regression, we use `family = binomial`.

Let's create a binary variable for whether an observation is in the South:

```{r}
house_ch12 <- house_ch12 %>%
  mutate(south = 1 * (region == "South"))
```

Next, let's fit a model and `tidy()` it:

```{r, eval=FALSE}
house_south_model <- glm(dem_win ~ south, family = binomial, data = house_ch12)
house_south_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate)
```

```{r, echo=FALSE}

house_south_model <- glm(dem_win ~ south, family = binomial, data = house_ch12)

tidy(house_south_model) %>%
  select(term, estimate) %>%
  gt()

```

How can we interpret the coefficients?  Unlike linear regressions, these coefficients aren't directly interpretable.  Recall our logistic regression model equation:

\[
\log\left(\frac{p}{1 - p}\right)=\beta_0+\beta_1X 
\]

A one-unit change in $X$ thus is associated with a one-unit change in $log\left(\frac{p}{1 - p}\right)$, where $p$ is the predicted probability of success. It is hard to understand intuitively what this means.

However, with only two possible values for the predictors (`south` = 0 or `south` = 1), we can directly calculate all the possible values of $p$ this model by using the *standard logistic function*:

\[
p = \frac{1}{1 + e^{-(\beta_0+\beta_1X)}} 
\]

This can easily be done using the `plogis()` function in R. 

Thus, for our example, we have:

\[
p_{dem\_win} = \frac{1}{1 + e^{-(0.230 - 0.249 \times south)}} 
\]

Therefore, the predicted probability for an observation in the South is $\frac{1}{1 + e^{-(0.230 - 0.249)}}$ (`r round(plogis(0.230 - 0.249), 3)`) and the predicted probability for an observation outside the South is $\frac{1}{1 + e^{-(0.230)}}$ (`r round(plogis(0.230), 3)`) a difference of `r round(plogis(0.230 - 0.249) - plogis(0.230), 3)`.  Thus, an observation being the South is associated with a `r round(plogis(0.230 - 0.249) - plogis(0.230), 3)` decrease in the predicted probability of a Democratic victory.

Note that you could also estimate this value through the so-called **"divide by 4" rule**: $-0.249 / 4 \approx 0.062$.  We'll discuss this further when we talk about interpreting the coefficiences of continuous predictors in logistic regression models.

<!-- Here would be a good place to begin a discusssion of validity --> 

Let's now estimate the uncertainty around our coefficients.  We'll start by showing how you can do this through the bootstrap.  We'll begin by loading the **infer** package.  We're going to use the function `rep_sample_n()` to resample from our data 1,000 times:

```{r}
library(infer)

x <- house_ch12 %>% 
  select(dem_win, south) %>% 
  rep_sample_n(size = nrow(house_ch12), replace = TRUE, reps = 1000)

x
```

For each `replicate`, we have `r nrow(house_ch12)` resamples of `dem_win` and `south`.

Next, we are going to `nest()` our data by `replicate`:

```{r}
x <- x %>% 
  group_by(replicate) %>% 
  nest()

x
```

After grouping by `replicate` and using `nest()`, we now have a dataset of 1,000 rows and a list column named `data`.  Each element of `data` is a tibble consisting of one of the resampled datasets we created using `rep_sample_n()`.

Now, we can use `map()` to run our logistic regression for each dataset:

```{r}
x <- x %>% 
  mutate(mod = map(data, ~ glm(dem_win ~ south, family = binomial, data = .)))

x
```

Now we have a new list column, `mod`, that contains the model objects created by `glm()`.  We will now want to `tidy()` the object created by `glm()`:

```{r}
x <- x %>% 
  mutate(reg_results = map(mod, ~ tidy(.)))

x
```

`tidy()` stores the coefficients in the `estimate` column, with each coefficient named in the `term` column.  Thus, if we `filter()` by `term` and `pull(estimate)`, we can get the logistic regression coefficient for each bootstrap sample:

```{r}
x <- x %>%
  mutate(disp_coef = map_dbl(reg_results, ~ filter(., term == "south") %>% pull(estimate)))

x
```

Now that we have 1,000 estimates from our bootstrap samples, we can construct a percentile-based confidence interval easily:

```{r}
x %>%
 pull(disp_coef) %>%
 quantile(c(0.025, 0.5, 0.975))
```

As we've seen before, these are very similar to what we observe using `tidy(conf.int = TRUE)`:

```{r, eval=FALSE}
house_south_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

```{r, echo=FALSE}
tidy(house_south_model, conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  gt() %>%
  tab_source_note(md("Logistical Regression Table"))
```

Thus, from now on, we'll just use `tidy(conf.int = TRUE)` when working with `glm()`, just as we did with `lm()`.

Let's try running a slightly more complicated version of the above model, where we use `region` as a predictor instead of `south`.

```{r, eval=FALSE}
house_region_model <- glm(dem_win ~ region, family = binomial, data = house_ch12)
house_region_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

```{r, echo=FALSE}

house_region_model <- glm(dem_win ~ region, family = binomial, data = house_ch12)

tidy(house_region_model,
     conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  gt() %>%
  tab_source_note(md("Logistical Regression Table"))
```

The intercept here is the omitted category, the Midwest.  We can calculate the predicted probabilities for each category:

- Midwest: $\frac{1}{1 + e^{-(-0.076)}} = 0.481$
- Northeast: $\frac{1}{1 + e^{-(-0.076 + 0.657)}} = 0.641$
- South: $\frac{1}{1 + e^{-(-0.076 + 0.056)}} = 0.495$
- West: $\frac{1}{1 + e^{-(-0.076 + 0.333)}} = 0.564$

We can then interpret the effect of moving from one category to another.  For example, the effect of being in the Northeast as opposed to the Midwest is 0.16; the predicted probability of a Democratic victory in the Northeast is 0.16 greater than in the Midwest. Note too that we could have obtained this through the "divide by 4" rule:  $0.657 / 4 \approx 0.16$.

### Observed/fitted values and residuals

We have previously defined the following three concepts for a linear regression:

1. Observed values $y$, or the observed value of the outcome variable
1. Fitted values $\widehat{y}$, or the value on the regression line for a given $x$ value
1. Residuals $y - \widehat{y}$, or the error between the observed value and the fitted value

We obtained these values and other values using the `augment()` function from the **broom** package. Recall too that we used the `.se.fit` column to construct confidence intervals.  We'll see here how we can apply these same concepts to logistic regression.

```{r, eval=FALSE}
regression_points <- house_region_model %>%
  augment() %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points
```
```{r, echo=FALSE}
regression_points <- augment(house_region_model) %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 4,201 district-years)"))
```

The syntax is the same, but the interpretation has to change, since the `.fitted`, `conf.low`, and `conf.high` columns are all on the logit scale.  While we could try to interpret these values, `augment()` has the argument `type.predict = "response"` that allow us to present the results in terms of *predicted probabilities*:

```{r, eval=FALSE}
regression_points <- house_region_model %>%
  augment(type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points
```
```{r, echo=FALSE}
regression_points <- augment(house_region_model,
                             type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 4,201 district-years)"))
```

Now each of the `.fitted` values is a *predicted probability* of a Democratic victory from our model for a particular district and the confidence intervals are confidence intervals around that predicted probability. 

You may be wondering how to interpret the residuals.  The residuals reported by `augment()` for a logistic regression are called *deviance residuals*.  A deviance residual can be calculated for each observation using:

\[
\textrm{d}_i = 
\textrm{sign}(Y_i-\hat{p_i})\sqrt{-2 [ Y_i \text{log} \hat{p_i} + (1 - Y_i) \text{log} (1 - \hat{p_i}) ]}
\]

where $Y_i$ is the actual outcome and $p_i$ is the predicted probability from the logistic regression model.

The sum of the individual deviance residuals is referred to as the **deviance** or **residual deviance**. The deviance is used to assess the model. As the name suggests, a model with a small deviance is preferred.

However, you can also have `augment()` report residuals as differences between the observed outcome and the predicted probabilities by using `type.residuals = "response"`:

```{r, eval=FALSE}
regression_points <- house_region_model %>%
  augment(type.predict = "response",
          type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points
```
```{r, echo=FALSE}
regression_points <- augment(house_region_model,
                             type.predict = "response",
                             type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  gt() %>%
  tab_source_note(md("Regression points (First 10 out of 4,201 district-years)"))
```

Now, the `.resid` value is the difference between the actual outcome (`dem_win`) and the predicted probability.

### One numerical explanatory variable

We'll now predict `dem_win` with a single numerical explanatory variable, `year`. 

1. We first "fit" the logistic regression model using the `glm(y ~ x, family, data)` function and save it in `house_year_model`.
1. We get the regression table by applying the `tidy()` function from the **broom** package to `house_year_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.

```{r, eval=FALSE}
house_year_model <- glm(dem_win ~ year, family = binomial, data = house_ch12)
house_year_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

```{r, echo=FALSE}

house_year_model <- glm(dem_win ~ year, family = binomial, data = house_ch12)

tidy(house_year_model,
     conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  gt() %>%
  tab_source_note(md("Logistical Regression Table"))

```

How do we interpret the coefficients in this model?  Since the `year` coefficient is negative, that means that each additional year is associated with a reduction in the chances of a Democratic victory. 

If we wanted to learn the predicted probabilities for any given value of `year`, we can plug in our values of `year` into the standard logistic function, like so:

\[
p_{dem\_win} = \frac{1}{1 + e^{-(34.487 - 0.017 \times year)}} 
\]

Note that since this is not a linear function, a one-unit change in `year` will be associated with various one-unit changes in `dem_win`, depending on what `year` you are starting from.  Recall the figure we used to start the chapter:

```{r, echo = FALSE, message = FALSE}
ggplot(dat, aes(x = x)) +
  geom_point(aes(y = y)) +
  geom_smooth(aes(y = y, color = "blue"), method = "lm", se=FALSE) +
  geom_line(aes(y = p, color = "red")) +
  scale_color_manual(name = 'Regression model', 
         values = c('blue', 'red'), 
         labels = c('Linear', 'Logistic'), guide = 'legend') +
  theme_minimal()
```

A linear regression line (in blue) has a constant slope, which means that no matter what $x$ you start with, the effect of going from $x$ to $x + 1$ on $y$ is the same number.  

However, take a look at the logistic regression curve (in red).  The value of the slope for very high or very low values of $x$ is smaller (approaching 0 as $x$ tends to negative or positive infinity), while the slope in the middle of the curve is highest.  The steepest part of the curve corresponds to that part of the curve where the predicted probability equals 0.5.  That is, the effect of a one-unit change in $x$ is the highest when the predicted probability for that $x$ is close to 0.5 and smallest when the predicted probability for that $x$ is close to 0 or 1.

You can always use R to calculate the predicted probabilities for any value of $x$ and thus calculate the effect of moving from a particular $x$ to $x + 1$.  But this can get complicated.  In particular, once you start employing logistic regression with multiple predictors, the effect of a one-unit change in a predictor $x$ depends not only on $x$, but on the values of all the other predictors in your model!  You can always plug in all the coefficients and values of your predictors into the logistic function to calculate predicted probabilities, but if you don't do that, how can you interpret the coefficients?

Here is where we can use the *divide by 4 rule* that we discussed before.  A logistic regression coefficient divided by 4 is the effect of that variable at the steepest part of the logistic regression curve, which, as we saw, corresponds to where the predicted probability is 0.5.

Therefore, you can divide a logistic regression coefficient by 4 to get an upper bound on the effect a one-unit change in that predictor will have on the predicted probability of your outcome.  In this case, the approximation tells us that each additional year is associated with about a $-0.004$ decrease in the predicted probability of a Democratic victory.  Since the predicted probability of a Democratic victory in this model never strays far from 0.5, this is a pretty good approximation.^[Indeed, it is almost identical to the coefficient you would obtain in a linear regression--try it out!]

While `house_region_model` and `house_year_model` both tell us something interesting, we could learn more with an *interaction model* that includes both of our predictors.

### One numerical and one categorical explanatory variable

We'll now predict `dem_win` with a two variable, `region` and `year`, as well as the interaction between the two 

1. We first "fit" the logistic regression model using the `glm(y ~ x1 * x2, family, data)` function and save it in `house_interact_model`.
1. We get the regression table by applying the `tidy()` function from the **broom** package to `house_interact_model`.  We'll print the `term`, `estimate`, `conf.low`, and `conf.high` columns.

```{r, eval=FALSE}
house_interact_model <- glm(dem_win ~ region * year, family = binomial data = house_ch12)
house_interact_model %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

```{r, echo=FALSE}
house_interact_model <- glm(dem_win ~ region * year, family = binomial, data = house_ch12)
tidy(house_interact_model,
     conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high) %>%
  gt() %>%
  tab_source_note(md("Logistical Regression Table"))
```

Now we can see how the effect of `year` varies by `region`.  While the passage of time is associated with more Democratic victories in the Northeast and the West, `year` is associated with declining Democratic fortunes in the Midwest (`year`) and the South (`regionSouth:year`).

Looking at predicted probabilities can also put this model in perspective.  Let's use `augment()` to generate the predictions.  Remember that `type.predict = "response"` and `type.residuals = "response"` put the fitted values and the residuals on the probability scale.

```{r, eval=FALSE}
regression_points <- house_interact_model %>%
  augment(type.predict = "response",
          type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points
```
```{r, echo=FALSE}
regression_points <- augment(house_interact_model,
                             type.predict = "response",
                             type.residuals = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(dem_win, region, .fitted, conf.low, conf.high, .resid)
regression_points %>%
  slice(1:10) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (First 10 out of 4,201 district-years)",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

We can also use `augment` to make predictions for years that aren't in our data.  What would our model predict for the 2020 elections?  We use the `newdata` argument in `augment()` to make these predictions.

```{r}
house_interact_model %>%
  augment(newdata = tibble(year = rep(2020, 4),
                           region = c("Midwest", "Northeast", "South", "West")),
          type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit)
```

These can easily be plotted using `ggplot()`:

```{r}
house_interact_model %>%
  augment(newdata = tibble(year = rep(2020, 4),
                           region = c("Midwest", "Northeast", "South", "West")),
          type.predict = "response") %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  ggplot(aes(y = .fitted,
             ymin = conf.low,
             ymax = conf.high,
             x = region)) +
  geom_point() +
  geom_errorbar() +
  labs(y = "Predicted probability of a Democratic victory (2020)",
       x = "") +
  theme_classic()
```

<!-- Here is a great place to discuss unknown unknowns --> 

### Fitting many models using `map()`

While it is interesting to see how Democrats perform by region over time, it would also be interesting to see how each state has changed in its partisan voting from 1976--2018.  Have any seen particularly large increases (or decreases) in the probability of a Democratic candidate winning?

The code to do this is very similar to the code we used for the gubernatorial forecasts in Chapter \@ref(regression) and the Seattle house prices in Chapter \@ref(multiple-regression).  However, we will use `glm()` instead of `lm()`.

First, we'll filter to the states that have at least 50 district-years in the dataset. Next, let's use `map()` to learn about these districts:

```{r}
infreq_states <- house_ch12 %>%
  count(state) %>%
  filter(n < 50) %>%
  pull(state)

house_state_model <- house_ch12 %>%
  filter(! state %in%  infreq_states) %>%
  group_by(state) %>%
  nest() %>%
  mutate(mod = map(data, ~ glm(dem_win ~ year, family = binomial, data = .)),
         reg_results = map(mod, ~ tidy(., conf.int = TRUE)),
         year_coef = map_dbl(reg_results, ~ filter(., term == "year") %>% pull(estimate)),
         year_low = map_dbl(reg_results, ~ filter(., term == "year") %>% pull(conf.low)),
         year_high = map_dbl(reg_results, ~ filter(., term == "year") %>% pull(conf.high)))

glimpse(house_state_model)
```

The easiest way to see the results of these models is to plot the coefficients:

```{r}
house_state_model %>%
  ungroup() %>%
  mutate(state = fct_reorder(state, year_coef)) %>%
  ggplot(aes(x = state, y = year_coef, ymin = year_low, ymax = year_high)) +
  geom_point() +
  geom_errorbar() +
  theme_minimal() +
  labs(x = "",
       y = "Coefficients of \"year\"",
       title = "Predicting Democratic victories in the U.S. House over time by state",
       subtitle = "Logistic regression coefficients for year plotted by state") +
  coord_flip()
```

Consistent with the account we saw when looking at the effect of `year` overall, there are more states where the odds of a Democratic victory have been decreasing by year than ones where they have been increasing. 

### Professional models

So far, we have been fitting linear regressions and logistic regressions using `lm()` and `glm()`.  While these functions are well-known and easy to use, what if we wanted to fit another model?  We'd have to learn a new function, which may have new syntax.  It'd be much easier if we could use the same syntax for every model we fit.

A collection of packages that helps address this issue is called **tidymodels**: 

```{r, message = FALSE}
library(tidymodels)
```

**tidymodels** includes many packages, but we'll start by showing how to use **parsnip** to fit a logistic regression. 

First, in the **tidymodels** workflow, we have to save the *model specification*.  We do that using two functions: `logistic_reg()` and `set_engine()`.

```{r}
logistic_mod <- logistic_reg() %>%
  set_engine("glm")
```

`logistic_reg()` says that we want to fit a logistic regression, and `set_engine("glm")` specifies that we want to do it using `glm()`.  Behind the scenes, **parsnip** uses many other packages to fit its models, but by unifying the syntax, it means that you don't have to memorize how a lot of different functions work.

Note that our new object, `logistic_mod`, doesn't contain our data or a formula.  In order actually to fit our model, we need to feed `logistic_mod` to a function called `fit()`.  `fit()` is the general purpose function in **parsnip** for fitting any model specification.  It takes as its first argument the model specification, but otherwise it operates similarly to `lm()` and `glm()`:

```{r}
logistic_fit <- fit(logistic_mod,
                    factor(dem_win) ~ region * year,
                    data = house_ch12)
```

(We have to wrap `dem_win` in `factor()`, because `fit()` is more careful than `glm()` in requiring that classification models actually have categorical outcomes.)

One we have fit the model, how can we use it?  The `glm` object is still stored in `logistic_fit$fit`, so we can access that and use `tidy()`, just like we did before:

```{r}
logistic_fit$fit %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

As you can see, this generates the same results as when we used `glm()` directly.

We'll use **tidymodels** when introducing CART and random forests in this chapter and machine learning in the next chapter.  While the models will change, the basic code structure will be very similar to how we fit the logistic regression above.

## Classification and regression trees (CART)

```{r, echo=FALSE}
img_path <- "images"
```

### What is CART?

We have learned how to fit models for binary responses using logistic regression.  However, logistic regression is just one of many methods we can use to model binary responses.  CART is another approach, which we'll learn about in this section.  In the next section, we'll learn about random forests.

<!-- MB: Validity/model fitting the world-->

A **tree** is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as _nodes_. Decision trees predict an outcome variable $Y$ by *partitioning* the predictors.

Decision trees like this are often used in practice. For example, to decide on a person's risk of poor outcome after having a heart attack, doctors use the following:

```{r, echo=FALSE, out.width="50%"}
# source https://www.researchgate.net/profile/Douglas_Walton/publication/228297479/figure/fig1/AS:301828123185152@1448972840090/Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png
knitr::include_graphics(file.path(img_path,"Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png"))
```

(Source: Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184^[https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&mirid=1&type=2].)

Here, the binary outcome is whether a patient is "High Risk" or "Low Risk."  We have three predictors: minimum systolic blood pressure over the initial 24-hour period, age, and presence of sinus tachycardia.  The tree presents a series of yes or no questions that allow us to use the predictors to classify a patient's risk level.

**Classification trees**, or decision trees, are used in prediction problems where the outcome is categorical.  (When the outcome is numerical, they are called **regression trees**; hence the acronym **CART**, standing for Classification and Regression Trees.)  The general idea here is to build a decision tree and, at the end of each _node_, obtain a predictor $\hat{y}$. A mathematical way to describe this is to say that we are partitioning the *predictor space* into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$, and then for any predictor $x$ that falls within region $R_j$, we estimate $f(x)$ with the class that is the most common among the data within the partition for which the associated predictor $x_i$ is also in $R_j$.

But how do we decide on which partitions to make  ($R_1, R_2, \ldots, R_J$) and how do we choose $J$, the total number of partitions? Here is where the algorithm gets a bit complicated.

Classification trees create partitions recursively. We start the algorithm with one partition, the entire predictor space (i.e., every observation is classified as 0 or 1). But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions, then four, then five, and so on. (We will describe how we decide when to stop later.)

Once we select a partition $\mathbf{x}$ to split in order to create the new partitions, we find a predictor $j$ and value $s$ that define two new partitions, which we will call $R_1(j,s)$ and $R_2(j,s)$, that split our observations in the current partition by asking if $x_j$ is bigger than $s$ (or if $x_j$ falls into a particular category $s$, if the predictor $j$ is categorical):

$$
R_1(j,s) = \{\mathbf{x} \mid x_j < s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
$$

Now, after we define the new partitions $R_1$ and $R_2$, and we decide to stop the partitioning process, we compute predictors by taking the most common category of all the observations $y$ for which the associated $\mathbf{x}$ is in $R_1$ and $R_2$. We refer to these two as $\hat{y}_{R_1}$ and $\hat{y}_{R_2}$ respectively. 

But how do we pick the predictor $j$ and the value $s$? One of the more popular ways for categorical data is the _Gini Index_.

In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The _Gini Index_ is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define $\hat{p}_{j,k}$ as the proportion of observations in partition $j$ that are of class $k$. The Gini Index is defined as 

$$
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
$$

If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above, since $\hat{p}_{j,k}(1-\hat{p}_{j,k}) = 0$ for all $k$.

Once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region. 

But when do we stop partitioning?  Every time we split and define two new partitions, the Gini Index improves. This is because with more partitions, our model has more flexibility to adapt to our data.  However, our model may therefore perform worse when exposed to new data (this problem is called *overfitting*). This connects to our discussion of validity and models, as the conditions used to create the model will be too specific to accurately extrapolate to new data points. To avoid this, the algorithm sets a minimum for how much the Gini Index must improve for another partition to be added. This parameter is referred to as the _complexity parameter_ ($c_p$). The measure of fit must improve by a factor of $c_p$ for the new partition to be added. Large values of $c_p$ will therefore force the algorithm to stop earlier which results in fewer nodes.

<!-- MB: Use overfitting to discuss validity/model fitting the world-->

Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough).  Finally, they can model human decision processes. However, in terms of accuracy, they are rarely the best performing method since they are not very flexible. Random forests, explained in the next section, improve on some of the shortcomings of classification trees.

### One categorical explanatory variable

To create classification trees, we'll use the `decision_tree()` model specification and the `"rpart"` engine.  The syntax is very similar to when we used `logistic_reg()`.  Note that our binary response variable has to be a factor, just like with `logistic_reg()`. 

```{r}
tree_mod <- decision_tree() %>%
  set_engine("rpart",
             model = TRUE) %>%
  set_mode("classification")
```

(Note that we added the argument `model = TRUE` to `set_engine()`.  This saves the model frame, which we will need to avoid a warning when we plot the trees later.)

The function `set_mode()` wasn't necessary when we did logistic regression.  Here it clarifies that we want a *classification* tree rather than a *regression* tree.

Now that we have the object `tree_mod`, we can use `fit()` in the **parsnip** package.  We'll start by predicting `dem_win` with `region`:

```{r}
house_region_tree <- fit(tree_mod,
                         factor(dem_win) ~ region, 
                         data = house_ch12)
```

See how when using **tidymodels**, this is exactly the same as how we would fit a logistic regression, but with our model specification saved in `tree_mod` rather than the model specification we saved in `logistic_fit`.

What was the result of our tree?

```{r}
house_region_tree
```

It's not especially helpful to look at the results of a tree as text.  In order to visualize the tree, we'll use the `prp()` function in the **rpart.plot** package.  Remember that the model object is stored in `house_region_tree$fit`.

```{r, message = FALSE}
library(rpart.plot)

house_region_tree$fit %>%
  prp(extra = 6, varlen = 0, faclen = 0)
```

The arguments `varlen = 0` and `faclen = 0` ensure that the full variable names and factor levels are printed.  The argument `extra = 6` shows the proportion of "yes" outcomes within a given partition. Since we'll be using these same arguments throughout the chapter, we'll create a new function that calls `prp()` but with these options as defaults:

```{r}
prp_ch13 <- function(x, ...) prp(x, extra = 6, varlen = 0, faclen = 0, ...)

house_region_tree$fit %>%
  prp_ch13()
```

How do we interpret this tree?  Here we have two partitions, based on whether an observation is in the Midwest or South or not in the Midwest or South (i.e., in the Northeast or West).  If an observation is in the Midwest or South, the tree classifies the observation as a 0, a Democratic loss. The "0.49" means that 49% of the observations in this node were Democratic wins. If an observation is not in the Midwest or South, the algorithm classifies the observation as a 1, a Democratic win; 60% of the observations in this node were Democratic wins.

As you can see, the algorithm is very simple when you have one categorical explanatory variable: it just classifies every observation based on the most common response per category.  Take a look at the following table:

```{r, echo = FALSE}
house_ch12 %>%
  group_by(region) %>%
  summarize(dem_win = mean(dem_win) * 100) %>%
  kable(digits = 1,
        col.names = c("Region", "Democratic Win Percentage"))
```

The two partitions just divide the observations by region into a) those regions where Democrats won a majority of the elections in the data and b) those where they lost a majority of the elections.

### One numerical explanatory variable

Once we have created our model specification `tree_mod`, it is easy to use it to fit new models with different formulae and data.  We can use the same approach to create a classification tree predicting `dem_win` with `year`:

```{r}
house_year_tree <- fit(tree_mod,
                       factor(dem_win) ~ year,
                       data = house_ch12)

house_year_tree$fit %>%
  prp_ch13()
```

Now, the algorithm creates cutpoints in the `year` variable in order to classify observations.  This tree will classify observations before 1993 (the 1992 election cycle or earlier, since House elections only occur in even-numbered years) or between 2005 and 2009 (the 2006 and 2008 cycles) as Democratic victories and all other observations as Democratic losses.

Eagle-eyed observers will notice that this is simply classifying every year based on whether the Democrats won a majority in that year, with one exception --- observations in 2018 are predicted to be Democratic losses.  Why is that?  Recall that the algorithm uses a *complexity parameter*, $c_p$, to avoid overfitting.  The default value of $c_p$ in `rpart()` (the engine we are using) is 0.01; setting it to 0 allows one to see the maximum number of partitions that the algorithm will do.

How can we change $c_p$?  We don't need to create an entirely new model specification.  Rather, we can use the `update()` function to change $c_p$ while leaving everything else about `tree_mod` the same.  All we need to do is specify the parameter we want to change, which here is called `cost_complexity`:

```{r}
house_year_tree_0 <- fit(update(tree_mod, cost_complexity = 0),
                         factor(dem_win) ~ year,
                         data = house_ch12)
```

We can then look at the new tree and see what changed:

```{r}
house_year_tree_0$fit %>%
  prp_ch13()
```

Now there is an additional partition that classifies observations where `year` $\geq 2017$ as 1.

But which tree is better?  We can answer that question using the `tune_grid()` function in the **tune** package, included as part of **tidymodels**.  But for now, we'll rely on the default value of $c_p$, keeping in mind that changing that value could change the tree.

<!-- AR: material below commented out; can be deleted unless it may be
repurposed for a discussion of the tune package at another time -->

<!-- The basic idea is that we want to test out various values of $c_p$. and select the model that performs best.  We will provide code on how to decide which model is best by calculating the accuracy on 25 bootstrapped samples of our data.  (We wouldn't want to calculate the accuracy on our full data, because a lower value of $c_p$ will always perform best on any particular dataset, but the resulting model may not perform well on out-of-sample observations; this is an example of a general phenomenon called *overfitting*.)

First, we need to update the model specification to note that we are tuning the `cost_complexity` parameter.  We can do that using the `update()` function and setting `cost_complexity = tune()`:

```{r}
tree_mod_tune <- tree_mod %>%
  update(cost_complexity = tune())
```

Next, we need to create our bootstrapped data.  We can do that using **tidymodels** with the `bootstraps()` function in the **rsample** package.  We will first call `set.seed()` in order to make sure our results are replicable, given the random nature of the bootstrap process.

```{r}
set.seed(12345)
house_boots <- bootstraps(house_ch12)
```

By default, the `bootstraps()` function generates 25 samples.  You can modify this with the `times` argument.

Next, we must set the values of `cost_complexity` that we will test.  We can select random values using the `grid_random()` function; by default, it selects 5, although you can adjust that with the `size` option.  We'll save the result in `cost_grid`.

```{r}
cost_grid <- grid_random(cost_complexity())
cost_grid
```

Now we have the elements necessary to run `tune_grid`.  The key arguments are our formula (here, `factor(dem_win) ~ year`), the `model` (`tree_mod_tune`), the `resamples` (`house_boots`), and the grid (`cost_grid`).  We'll save the result as `tree_res`:

```{r}
tree_res <- tune_grid(factor(dem_win) ~ year,
                      model = tree_mod_tune,
                      resamples = house_boots,
                      grid = cost_grid)
```

Now we need to know what value of `cost_complexity` performed the best.  For this, we use the function `select_best()`. We need to supply an argument called `metric` to `select_best()`; for this example, we'll choose `"accuracy"`, which is just the percentage of observations correctly classified.

```{r}
best_cost <- select_best(tree_res, metric = "accuracy")
best_cost
```

We can see from the output that `r best_cost` performed best on our bootstrap samples. We can visualize these results using `autoplot()`:

```{r}
autoplot(tree_res, metric = "accuracy")
```

All we have left to do is to `update()` our model specification based on `best_cost` and fit the results:

```{r}
house_year_tree_best <- fit(update(tree_mod, cost_complexity = best_cost),
                       factor(dem_win) ~ year,
                       data = house_ch12)
```

Let's look at the tree, saved in `house_year_tree_best$fit`:

```{r}
house_year_tree_best$fit %>%
  prp_ch13()
```

With the value of `cost_complexity` chosen by `tune_grid()`, we retain the partition based on `year < 2017`.-->

### Multiple explanatory variables

What if we wanted to predict `dem_win` based on `region` and `year`?  The process is similar to what we've seen before.  We'll fit a new model called `region_year_tree` using our model specification `tree_mod`.

```{r}
region_year_tree <- fit(tree_mod,
                        factor(dem_win) ~ region + year,
                        data = house_ch12)

region_year_tree$fit %>%
  prp_ch13()
```

This tree classifies observations in the Northeast and West as a Democratic win and observations in the Midwest and South as a Democratic win before 1993 and a Democratic loss afterward. 

In the section on logistic regression, we used `map()` to fit many models to visualize the effect of `year` by `state`.  When fitting classification trees, it is easy just to add `state` as a predictor and allow the complexity parameter to determine whether we ought to partition further states by `year`.  In fact, unlike with `lm()` and `glm()`, we can include both `region` and `state` in the model, and the algorithm will decide whether it wants to use the additional information provided by `state` or just create partitions based on `region`.  Let's take a look:

```{r}
state_year_tree <- fit(tree_mod,
                       factor(dem_win) ~ region + state + year,
                       data = house_ch12)

state_year_tree$fit %>%
  prp_ch13()
```

Unfortunately, this is very difficult to read, since `prp()` tries to plot all the state names on one line.  The [documentation](http://www.milbo.org/doc/prp.pdf) gives example code on how to wrap the factor labels across multiple lines, which we will adapt:

```{r}
split.fun <- function(x, labs, digits, varlen, faclen) {
  
  # Replace commas with spaces (needed for strwrap)
  
  labs <- gsub(",", " ", labs)
  
  for(i in 1:length(labs)) {
    
    # Split labs[i] into multiple lines
    
    labs[i] <- paste(strwrap(labs[i], width = 50), collapse = "\n")
  }
  
  labs
}
```

Now we need to apply `split.fun` to `prp()`; we'll also set `faclen = 2` in order to abbreviate the state names:

```{r}
state_year_tree$fit %>%
  prp(split.fun = split.fun,
      faclen = 2)
```

This experience shows a tradeoff when working with decision trees: as they get more complex, they become more accurate but also harder to read.

## Random forests

### What are random forests?

Random forests are a **very popular** machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.

The first step is _bootstrap aggregation_ or _bagging_. The general idea is to generate many predictors, each using classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees **randomly** different, and the combination of trees is the **forest**. The specific steps are as follows.

<!-- AR: do we explain training set/test set before this? -->

1\. Build $B$ decision trees using the training set. We refer to the fitted models as $T_1, T_2, \dots, T_B$. We later explain how we ensure they are different.

2\. For every observation in the test set, form a prediction $\hat{y}_j$ using tree $T_j$.

3\. For categorical data classification, predict $\hat{y}$ with majority vote (most frequent class among $\hat{y}_1, \dots, \hat{y}_T$).
     
So how do we get different decision trees from a single training set? For this, we use randomness in two ways which we explain in the steps below. Let $N$ be the number of observations in the training set. To create $T_j, \, j=1,\ldots,B$ from the training set we do the following:

1\. Create a bootstrap training set by sampling $N$ observations from the training set **with replacement**. This is the first way to induce randomness. 
    
2\. A large number of features is typical in machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way random forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy. 

### Fitting random forests

We will demonstrate by fitting a random forest to the House elections data, predicting `dem_win` with `region`, `state`, and `year`.  We will use the `rand_forest()` function to create our model specification, setting the engine to `"randomForest"` and the mode to `"classification"`.

```{r, message=FALSE, warning=FALSE}
forest_mod <- rand_forest() %>%
  set_engine("randomForest") %>%
  set_mode("classification")

house_forest <- fit(forest_mod,
                    factor(dem_win) ~ region + state + year,
                    data = house_ch12)

house_forest
```

We see under "OOB estimate of error rate" that this model has an error rate of 36% (or, looking at it the other way, an accuracy of 64%). We can see how the error rate of our algorithm changes as we add trees by looking at `house_forest$fit$err.rate[, "OOB"]`.  By default, `randomForest()` (the engine we specified) grows 500 trees.

```{r}
tibble(`Error rate` = house_forest$fit$err.rate[, "OOB"],
       Trees = 1:500) %>%
  ggplot(aes(x = Trees, y = `Error rate`)) +
  geom_line() +
  theme_classic()
```

We can see that in this case, the accuracy improves as we add more trees until about 300 trees where accuracy stabilizes.

Random forests often perform better than other methods. However, a disadvantage of random forests is that we lose interpretability---we don't get anything like the coefficients from a logistic regression or the single tree from CART.

<!-- AR: Cutting this; not sure how to do this in tidymodels -->

<!-- An approach that helps with interpretability is to examine _variable importance_. To define _variable importance_ we count how often a predictor is used in the individual trees. You can learn more about _variable importance_ in an advanced machine learning book.^[https://web.stanford.edu/~hastie/Papers/ESLII.pdf] The __caret__ package includes the function `varImp` that extracts variable importance from any model in which the calculation is implemented.  -->

<!-- ```{r} -->
<!-- caret::varImp(house_forest) -->
<!-- ``` -->

<!-- Thus, we can see that the state an observation is in plays the biggest role in our forest, followed by year and finally by region. -->

## Comparing the three approaches

We've explored three ways to model binary responses in this chapter.  Which one should you use?

This isn't as straightforward a question as it may seem.  First, we need some measure of which model is "better."  The most obvious way to compare models for binary data is simply to look at the predicted outcomes from the model and compare those predicted outcomes to the actual outcomes.  The percentage of outcomes correctly classified by the model is called the model's *accuracy*.

Let's compare the accuracy of the following models in predicting Democratic victories in House elections:

- Predicting a Democratic win (the modal outcome in our data) for every observation 
- A logistic regression predicting `dem_win` with `state`, `year`, and the interaction of `state` and `year`
- A linear regression with the same predictors as the logistic regression
- A classification tree using `state` and `year`
- A random forest using `state` and `year`

Those last two models we already have saved as `state_year_tree` and `house_forest` respectively.  We'll create the logistic and linear regressions and save them as `house_logistic` and `house_linear`:

```{r, warning = FALSE}

house_logistic <- fit(logistic_mod,
                      factor(dem_win) ~ state * year,
                      data = house_ch12)

house_linear <- fit(linear_reg(),
                    dem_win ~ state * year,
                    data = house_ch12)
```

Note that fitting a linear regression is as simple as using the model specification `linear_reg()`, which by default uses `lm()` as its engine.

How can we extract the predictions for each of these models?  **tidymodels** provides a unified function, `predict()`.  The first argument of the function is the fitted model object.  It also requires the argument `new_data`, which is the dataset on which you will be making your predictions.  Here, we are making predictions on our original data (`house_ch12`).  For example:

```{r}
predict(house_logistic, new_data = house_ch12)
```

The result is a tibble with the predicted outcomes stored in the column `.pred_class`.

For the linear model, the result is going to be a number, not a class:

```{r}
predict(house_linear, new_data = house_ch12)
```

We can interpret this as a predicted probability and classify responses $> 0.5$ as $1$ and $< 0.5$ as $0$:

```{r, eval = FALSE}
1 * (predict(house_linear, new_data = house_ch12) > 0.5)
```

Let's put it all together in a tibble called `house_preds`:

```{r}
house_preds <- house_ch12 %>%
  mutate(modal = 1,
         logistic = predict(house_logistic, new_data = house_ch12) %>% 
           pull(.pred_class),
         linear = 1 * (predict(house_linear, new_data = house_ch12) > 0.5),
         tree = predict(state_year_tree, new_data = house_ch12) %>% 
           pull(.pred_class),
         forest = predict(house_forest, new_data = house_ch12) %>% 
           pull(.pred_class))
```

We'll start by looking at five random rows of `house_preds`.

```{r, eval = FALSE}
house_preds %>%
  select(-region, -state) %>%
  sample_n(5)
```

```{r, eval = TRUE, echo = FALSE}
house_preds %>%
  select(-region, -state) %>%
  sample_n(5) %>%
  kable()
```

In these five districts, the Democrats won two and therefore our `modal` prediction, which always predicts a Democratic win, classifies 40% of the observations correctly.  For these districts, the logistic, linear, and classification tree models all classify SC-02 in 2010 correctly as a Democratic loss and misclassify the remaining districts, for an accuracy of 20%.  Finally, the random forest model classifies SC-02 in 2010 correctly as a Democratic loss and also correctly classifies PA-17 in 2002 as a Democratic win, for an accuracy of 40%.

Now let's calculate the overall accuracy of our five models.  We simply need to calculate the proportion of observations where the predicted value equals `dem_win`:

```{r}
tibble(modal = mean(house_preds$modal == house_preds$dem_win),
       logistic = mean(house_preds$logistic == house_preds$dem_win),
       linear = mean(house_preds$linear == house_preds$dem_win),
       tree = mean(house_preds$tree == house_preds$dem_win),
       forest = mean(house_preds$forest == house_preds$dem_win))
```

Here we see that all our models perform better than simply picking the modal outcome, but there isn't much variation in accuracy of the four more sophisticated models.

However, when selecting a model, we may care about more than just accuracy.  There are three concerns in particular that we'll address here:

1. Is accuracy the right measure?
1. Modeling for prediction vs. explanation
1. Out-of-sample predictions

### Is accuracy the right measure?

We've been looking at the *accuracy* of each model.  However, we can come up with more granular measures.  Every prediction can be classified into one of the following four categories:

- True positive (classified as a `1` and actually a `1`)
- True negative (classified as a `0` and actually a `0`)
- False positive (classified as a `1` but actually a `0`)
- False negative (classified as a `0` but actually a `1`)

Accuracy thus is $(\text{True positives } + \text{ True negatives})/n$.  When using accuracy as a metric, therefore, we are implicitly assuming that false positives and false negatives are equally bad.  In the context of election forecasting, that is probably right, since we probably only care about how many seats we accurately predict.


However, there are other scenarios where a false negative and a false positive can have very different costs associated with them, such as when evaluating medical interventions.  (A false negative may mean that a person who actually has a disease receives no treatment, whereas a false positive may mean that a healthy person receives a medical intervention; which is worse depends on the disease and the side effects of the treatment.)  Therefore, there are many metrics  beyond accuracy that may be relevant for different applications, such as *sensitivity* (the proportion of actual $1$s classified as $1$) and *specificity* (the proportion of actual $0$s classified as $0$).

### Modeling for prediction vs. explanation

All things being equal, we would prefer a model with more predictive power to one with less.  However, we have emphasized that models may serve different purposes.  Some focus on attempting to predict an outcome.  Others focus on learning about a *causal effect*.  When one is interested in identifying a causal effect, we may not always prefer the model that has the greatest predictive power.

Why not?  One concern is *interpretability*.  We want to have a sense of the magnitude of the causal effect and our uncertainty about that magnitude; this is much easier with models such as linear and logistic regressions that produce coefficients.

Another concern is *post-treatment bias*.  For example, let's say that we were interested in the effect of a hypertension drug on all-cause mortality.  We cannot use blood pressure measurements taken after the drug was administered as a predictor if we are interested in the effects of the drug on mortality, even though such a variable will surely improve our predictions, since that variable is affected by our treatment.

### Out-of-sample predictions

We saw that our models predicted 64%-65% of the House races from 1976--2018 accurately using `state` and `year` as predictors.  However, these models were fitted on the same data that we used to evaluate their predictions.  These are called *in-sample* predictions.  A more strenuous test of a model's performance is whether it can generate good predictions on data that were not used in fitting the model. For election forecasting, for example, we are much more interested in forecasting the 2020 election results (which we don't know) than the 1976--2018 election results (which we know).  The 2020 predictions are an example of *out-of-sample* predictions.

Making out-of-sample predictions is a great way of measuring unknown unknowns in the world, which is data we haven't yet observed. When making such predictions, it is important to make sure that the conditions used to create the model match the conditions of the world you are extrapolating about. Using 1976--2018 election results to predict election results in 2020, we contend, is a pretty safe extrapolation. An example of similar but unsafe extrapolation would be using UK election results to predict outcomes in the US. 

While we can't evaluate our models' performance on the 2020 elections (yet!), we can take a look at the process by fitting our logistic, classification tree, and random forest models on the data we have pre-2018 and then evaluating their accuracy on the 2018 election results.  Let's start by creating tibbles of our pre-2018 and 2018 data:

```{r}
house_pre_2018 <- house_ch12 %>% filter(year != 2018)
house_2018 <- house_ch12 %>% filter(year == 2018)
```

Next, we'll fit the models on the pre-2018 data:

```{r, warning = FALSE}
house_logistic_pre <- fit(logistic_mod,
                           factor(dem_win) ~ state * year,
                           data = house_pre_2018)

house_tree_pre <- fit(tree_mod,
                       factor(dem_win) ~ state + year,
                       data = house_pre_2018)

house_forest_pre <- fit(forest_mod,
                         factor(dem_win) ~ state + year,
                         data = house_pre_2018)
```

We'll then generate predictions using `new_data = house_2018`:

```{r}
house_2018_preds <- house_2018 %>%
  mutate(logistic = predict(house_logistic_pre, new_data = house_2018) %>% 
           pull(.pred_class),
         tree = predict(house_tree_pre, new_data = house_2018)  %>% 
           pull(.pred_class),
         forest = predict(house_forest_pre, new_data = house_2018)  %>% 
           pull(.pred_class))

```

Finally, let's take a look at the accuracy of the three models:

```{r}
tibble(logistic = mean(house_2018_preds$logistic == house_2018_preds$dem_win),
       tree = mean(house_2018_preds$tree == house_2018_preds$dem_win),
       forest = mean(house_2018_preds$forest == house_2018_preds$dem_win))
```

Here we see that when fitting the models on pre-2018 data and testing them on 2018 data, the random forest specification performs the best and the classification tree the worse, with the logistic regression in-between.
