<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 9 N Parameters | Preceptor’s Primer for Bayesian Data Science" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="davidkane9/PPBDS" />

<meta name="author" content="David Kane" />

<meta name="date" content="2020-07-25" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Chapter 9 N Parameters | Preceptor’s Primer for Bayesian Data Science">

<title>Chapter 9 N Parameters | Preceptor’s Primer for Bayesian Data Science</title>

<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css-0/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.4.0/str_view.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Preceptor's Primer for Bayesian Data Science<p><p class="author">David Kane</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BE; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Cover</a>
<a href="preamble.html">Preamble</a>
<a href="shopping-week.html">Shopping Week</a>
<a href="visualization.html"><span class="toc-section-number">1</span> Visualization</a>
<a href="tidyverse.html"><span class="toc-section-number">2</span> Tidyverse</a>
<a href="rubin-causal-model.html"><span class="toc-section-number">3</span> Rubin Causal Model</a>
<a href="functions.html"><span class="toc-section-number">4</span> Functions</a>
<a href="probability.html"><span class="toc-section-number">5</span> Probability</a>
<a href="sampling.html"><span class="toc-section-number">6</span> Sampling</a>
<a href="one-parameter.html"><span class="toc-section-number">7</span> One Parameter</a>
<a href="two-parameters.html"><span class="toc-section-number">8</span> Two Parameters</a>
<a id="active-page" href="n-parameters.html"><span class="toc-section-number">9</span> N Parameters</a><ul class="toc-sections">
<li class="toc"><a href="#explorator-data-analysis-of-shaming"> Explorator Data Analysis of <code>shaming</code></a></li>
<li class="toc"><a href="#causal-effects-of-treament"> Causal Effects of <code>treament</code></a></li>
<li class="toc"><a href="#formatting-linear-models-in-r"> Formatting Linear Models in R</a></li>
<li class="toc"><a href="#adding-parameters"> Adding parameters</a></li>
<li class="toc"><a href="#interaction-terms"> Interaction terms</a></li>
<li class="toc"><a href="#heterogeneous-treatment-effect"> heterogeneous treatment effect</a></li>
<li class="toc"><a href="#sources-of-uncertainty"> 5 sources of uncertainty</a></li>
<li class="toc"><a href="#rubin-causal-model-2"> Rubin Causal Model</a></li>
<li class="toc"><a href="#cces-data"> cces data</a></li>
<li class="toc"><a href="#presidential-approval-overall-by-year-by-state-by-state-x-year-x-educ"> presidential approval; overall; by year; by state; by state x year x educ</a></li>
<li class="toc"><a href="#need-rstanarm"> need rstanarm</a></li>
<li class="toc"><a href="#rubin-causal-model-3"> Rubin Causal Model</a></li>
</ul>
<a href="continuous-response.html"><span class="toc-section-number">10</span> Continuous Response</a>
<a href="appendices.html">Appendices</a>
<a href="tools.html">Tools</a>
<a href="shiny.html">Shiny</a>
<a href="maps.html">Maps</a>
<a href="animation.html">Animation</a>
<a href="references-1.html">References</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="n-parameters" class="section level1">
<h1>
<span class="header-section-number">Chapter 9</span> N Parameters</h1>
<!-- Outline: -->
<!-- The goal of this chapter is to replicate all of the things we have learned in chapter 8, except with models that allow for more than two parameters, which mostly involves have more than one right-hand side variable. We go through all of themes.Rmd two more times, once with a causal data set and once with a predictive one. So far, nothing new or different. But we are also going to highlight three major problems. We aren't going to solve those problems, but we are going to hint at them. The problems are: a) How do you decide which model is "better?" (Not an easy question! But the ability to predict new data is certainly a big part of the answer. b) How do you avoid overfitting, not least because it makes your predictions horrible? c) How do you establish causality if you don't have a randomized experiment? -->
<!-- 0) Start with a problem-->
<!-- 1) EDA of `shaming`. I think that hh_size is a good variable to check for interactions with, especially with Control. One approach: Create a new variable called `solo` which is TRUE if the `hh_size` is equal to 1 and FALSE otherwise. In other words, does the state of living alone tell us anything? We will interact this term with treatment in our model. It is cool that the p value for the interaction with Self is 0.06. A perfect example for the evils of testing!  -->
<!-- It may be worth mentioning that general_04 is always "yes", unlike all the other voting history variables which have values of "yes" and "no". Why is that? I *think* that this is caused by their sampling plan. They found all the people who voted in the 2004 general election. Then, the authors found their history. As we would expect, those people had sometimes voted in the past and sometime not. Then, the authors sent the mailing. The key dependent variable, primary_06, is coding 0/1, since that makes doing the statistics easier. -->
<!-- 2) `primary_06` as a function of `treatment` and `solo` and of their interaction. We will build up this model step-by-step, very similar to how we explored the effect of treatment in chapter 8. But we go deeper because  we are learning about interactions. Key thing is to go through all the themes.Rmd issues, at least until prediction. Note that this situation is different from Chapter 8 in that fitted values and predicted values are not the same thing! The fitted value, for a combination of values for treatment and solo, is something 0.30, meaning that 30% of the people in this bucket votes. But the predicted value must be 0 or 1. Either you voted or you didn't. This example is clearly causal and so you need a Rubin Table with 4 potential outcome columns. The key difference in this chapter is that we are using lots of right hand side variables, both continuous and discrete. Unlike last chapter, we go through the entire process in this section, all the way through dgm and predictive uncertainty. -->
<!-- 3) EDA of `nes`. Or maybe something else? Let's discuss. This can be fairly quick. Again, we won't use all the variables. Only discuss the ones we do use. One of the variables should be year. We want to show off Gelman's trick by plotting things by year. -->
<!-- 4) We are making a predictive model of what as a function of what other stuff? Want to use a continuous variable and some discrete variables as well. Interactions too.  -->
<!-- 5) Discussion of the difference between predictive and causal, and how we can interpret a model as causal even if it uses observational data. Apply to the nes example model. -->
<!-- 6) Pitfalls. Discuss model selection and overfitting. Again, we are not solving these problems here. They are hard problems. Instead, we are motivating chapter 10, which should perhaps be re-titled.  -->
<!-- Thoughts -->
<!-- * Should we do any Bayesian stuff here. Maybe not? We build the dgm() function by hand, maybe simplifying things a bit by not taking parameter uncertainty seriously. We mentioned that there must be an easier way, and then we do that in chapter 10.  -->
<!-- * Should we discuss overfitting here? I think that the answer is Yes, that we at least have to mention it and why it is a problem. Indeed, we have at least two pages making sure the concepts are clear. Again, we hint at what solutions might look like and promise a resolution in chapter 10. Maybe we mention pooling. Maybe we mention cross-validation. But we don't yet solve the problem.  -->
<!-- * The key part of this chapter is showing the themes.Rmd all the way through two problems, one causal and one not. In that way, it is very similar to chapter 8. Indeed, perhaps we should make these two chapters as alike as possible. -->
<p>Imagine you are running for Governor and want to do a better job of getting your voters to vote. You recently read about a large-scale experiment showing the effect of sending out a voting reminder that “shames” citizens who do not vote. You are considering sending out a “shaming” voting reminder yourself. What will happen if you do? Will more voters show up to the polls? Additionally, on the day of the election a female citizen is randomly selected. What is the probability she will vote?</p>
<p>In this chapter, we will consider models with multiple parameters and the complexities that arise from these additions. We will also learn how to make more accurate predictions using Bayesian methods that will provide us with answers to the questions posed above.</p>
<div id="explorator-data-analysis-of-shaming" class="section level2">
<h2>
<span class="header-section-number">9.1</span> Explorator Data Analysis of <code>shaming</code>
</h2>
<p>Begin by loading packages which we have used in previous chapters.</p>
<p>We will start off this chapter introducing a new data set from the <strong>PPBDS.data</strong> package: shaming. This data set corresponds to an experiment carried out by Gerber, Green, and Larimer (2008) titled “Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment”. This experiment used several hundred thousand registered voters and a series of mailings to determine the effect of social pressure on voter turnout. Four types of treatments were used in the experiment, with voters receiving one type of mailing. All of the mailing treatments carried the message, “DO YOUR CIVIC DUTY - VOTE!”.</p>
<p>The first treatment, Civic Duty, also read, “Remember your rights and responsibilities as a citizen. Remember to vote.” This message acted as a baseline for the other treatments, since it carried a message very similar to the one displayed on all the mailings.</p>
<p>In the second type of treatment, households received a mailing called the Hawthorne effect that told the voters that they were being studied and their voting behavior would be examined through public records. This adds a small amount of social pressure to the households receiving this mailing.</p>
<p>The third type of treatment is Self, in which the mailing includes the recent voting record of each member of the household, placing the word “Voted” next to their name if they did in fact vote in the 2004 election or places a blank space next to the name if they did not. In this mailing, the households were also told, “we intend to mail an updated chart” with the voting record of the members after the 2006 election. By emphasizing the public nature of voting records, this type of mailing exerts more social pressure on voting than the Hawthorne treatment.</p>
<p>The final type of mailing is the Neighbors treatment, which follows the Self mailing by listing the household members’ voting records, as well as the voting records of those who live nearby. This mailing also told recipients, “we intend to mail an updated chart” of who voted in the 2006 election. This “shaming” treatment exerts the most social pressure of all the treatments and broadcasts individuals’ voting habits.</p>
<p>Having created models with one parameter in Chapter @ref(#one-parameter) and two parameters in Chapter @ref(#two-parameters), you are now ready to make the jump to <span class="math inline">\(N\)</span> parameters.</p>
<p>Consider the <code>shaming</code> tibble from <strong>PPBDS.data</strong>.</p>
<!-- Provide a EDA of the shaming tibble, perhaps restricting it to a subset of the provided columns. Follow the lead of chapter 10. Make sure to use `glimpse()`, `sample_n()` and `skim()`. Readers need to know understand what the dependent variable means, along with left-hand side variables like treatment. -->
<pre><code>## Rows: 344,084
## Columns: 10
## $ sex         &lt;chr&gt; "Male", "Female", "Male", "Female", "Female", "Male", "Fe…
## $ birth_year  &lt;int&gt; 1941, 1947, 1951, 1950, 1982, 1981, 1959, 1956, 1968, 196…
## $ primary_02  &lt;chr&gt; "Yes", "Yes", "Yes", "Yes", "Yes", "No", "Yes", "Yes", "Y…
## $ general_02  &lt;chr&gt; "Yes", "Yes", "Yes", "Yes", "Yes", "No", "Yes", "Yes", "N…
## $ primary_04  &lt;chr&gt; "No", "No", "No", "No", "No", "No", "No", "No", "No", "No…
## $ general_04  &lt;chr&gt; "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "…
## $ treatment   &lt;fct&gt; Civic Duty, Civic Duty, Hawthorne, Hawthorne, Hawthorne, …
## $ primary_06  &lt;int&gt; 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, …
## $ hh_size     &lt;int&gt; 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, …
## $ no_of_names &lt;int&gt; 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 2…</code></pre>
<pre><code>## # A tibble: 5 x 10
##   sex   birth_year primary_02 general_02 primary_04 general_04 treatment
##   &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      &lt;fct&gt;    
## 1 Male        1979 No         No         No         Yes        Civic Du…
## 2 Fema…       1937 Yes        Yes        No         Yes        Self     
## 3 Fema…       1964 Yes        Yes        No         Yes        Neighbors
## 4 Male        1948 Yes        Yes        No         Yes        Hawthorne
## 5 Male        1955 No         Yes        Yes        Yes        Control  
## # … with 3 more variables: primary_06 &lt;int&gt;, hh_size &lt;int&gt;, no_of_names &lt;int&gt;</code></pre>
<table style="width: auto;" class="table table-condensed">
<caption>
<span id="tab:unnamed-chunk-558">TABLE 9.1: </span>Data summary
</caption>
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Name
</td>
<td style="text-align:left;">
Piped data
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of rows
</td>
<td style="text-align:left;">
344084
</td>
</tr>
<tr>
<td style="text-align:left;">
Number of columns
</td>
<td style="text-align:left;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
_______________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Column type frequency:
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
character
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
factor
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
numeric
</td>
<td style="text-align:left;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
________________________
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
Group variables
</td>
<td style="text-align:left;">
None
</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead><tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
min
</th>
<th style="text-align:right;">
max
</th>
<th style="text-align:right;">
empty
</th>
<th style="text-align:right;">
n_unique
</th>
<th style="text-align:right;">
whitespace
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:left;">
sex
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0
</td>
</tr></tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead><tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:left;">
ordered
</th>
<th style="text-align:right;">
n_unique
</th>
<th style="text-align:left;">
top_counts
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:left;">
treatment
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Con: 191243, Civ: 38218, Sel: 38218, Haw: 38204
</td>
</tr></tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead><tr>
<th style="text-align:left;">
skim_variable
</th>
<th style="text-align:right;">
n_missing
</th>
<th style="text-align:right;">
complete_rate
</th>
<th style="text-align:right;">
mean
</th>
<th style="text-align:right;">
sd
</th>
<th style="text-align:right;">
p0
</th>
<th style="text-align:right;">
p25
</th>
<th style="text-align:right;">
p50
</th>
<th style="text-align:right;">
p75
</th>
<th style="text-align:right;">
p100
</th>
<th style="text-align:left;">
hist
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:left;">
primary_06
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.32
</td>
<td style="text-align:right;">
0.46
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
▇▁▁▁▃
</td>
</tr></tbody>
</table>
<p>Having created models with one parameter in Chapter @ref(#one-parameter) and two parameters in Chapter @ref(#two-parameters), you are now ready to make the jump to <span class="math inline">\(N\)</span> parameters. The more parameters we include in our models, the more flexible they can become. But we must be careful of <em>overfitting</em>, of making models which are inaccurate because they don’t use enough data to accurately estimate those parameters. The tension between overfitting and underfitting is central to the practice of data science.</p>
</div>
<div id="causal-effects-of-treament" class="section level2">
<h2>
<span class="header-section-number">9.2</span> Causal Effects of <code>treament</code>
</h2>
<!-- This follows the discussion in chapter 8 very closely. See below for some key R code, which must be explained. (Obviously, you can use other code as well.) -->
<pre><code>## 
## Call:
## lm(formula = primary_06 ~ treatment - 1, data = .)
## 
## Coefficients:
## treatmentCivic Duty   treatmentHawthorne     treatmentControl  
##               0.315                0.322                0.297  
##       treatmentSelf   treatmentNeighbors  
##               0.345                0.378</code></pre>
<pre><code>## # A tibble: 5 x 7
##   term                estimate std.error statistic p.value conf.low conf.high
##   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 treatmentCivic Duty    0.315   0.00237      132.       0    0.310     0.319
## 2 treatmentHawthorne     0.322   0.00237      136.       0    0.318     0.327
## 3 treatmentControl       0.297   0.00106      280.       0    0.295     0.299
## 4 treatmentSelf          0.345   0.00237      145.       0    0.340     0.350
## 5 treatmentNeighbors     0.378   0.00237      159.       0    0.373     0.383</code></pre>
<pre><code>## # A tibble: 5 x 4
##   term                conf.low estimate conf.high
##   &lt;chr&gt;                  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 treatmentCivic Duty    0.308    0.315     0.321
## 2 treatmentHawthorne     0.316    0.322     0.328
## 3 treatmentControl       0.294    0.297     0.299
## 4 treatmentSelf          0.339    0.345     0.351
## 5 treatmentNeighbors     0.372    0.378     0.384</code></pre>
<!-- Once we talk about these things --- and, again, this is exactly what we have talked about in chapter 8 --- we can do a bit more. Like discuss how we are using 99%, because there is nothing magical aboyt 95%, other than convention. I also think it would be fun to show a nice graphic of this, highlighting how the estimates for Civic and Hawthorne overlap.  -->
<div id="interactions" class="section level3">
<h3>
<span class="header-section-number">9.2.1</span> Interactions</h3>
<!-- This is new. With only two parameters, we can't really look at interaction effects. Need to discuss interaction effects in general. Also, note that heterogenous treatment effects are the same thing as interaction effects that involve a treatment effect as one of the variables.  -->
<!-- Feel free to build up this code, and other examples, more slowly than I am doing it here. -->
<pre><code>## # A tibble: 10 x 2
##    term                       estimate
##    &lt;chr&gt;                         &lt;dbl&gt;
##  1 sexFemale                   0.306  
##  2 sexMale                     0.323  
##  3 treatmentHawthorne          0.0109 
##  4 treatmentControl           -0.0159 
##  5 treatmentSelf               0.0354 
##  6 treatmentNeighbors          0.0650 
##  7 sexMale:treatmentHawthorne -0.00617
##  8 sexMale:treatmentControl   -0.00406
##  9 sexMale:treatmentSelf      -0.00960
## 10 sexMale:treatmentNeighbors -0.00321</code></pre>
<!-- Takes a while to explain what all this means. -->
<!-- Two key issues: 1) Interpreting lots of parameters in a model. interactions, heterogenous treatment effects. shaming using lm(). 




2) Estimate support for the president among small demographic. How do female, Hispanics in Alabama feel about Obama? You don't want to just use the overall mean, which would underfitting. You don't want to use the mean for just that small group, which would be overfitting.  Using nes data, for 2012, approval for Obama and stan_glm().  -->
<!-- 
intercept

Interactions --- use: income ~ party*something

heterogeneous treatment effects --- use:  att_start ~ treatment*something 
just a fancy way of saying interaction effects, but with a variable which us causal


What problems do we face? All the things that make modeling difficult. Why is this so hard? -->
<!-- Centering. -->
<!-- Might naively just take the value for each bucket. But that overfits! Need to put down some structure, like ordering. -->
<!-- income category, party id, pooling, age, -->
<!-- overfitting/underfitting bias/variance -->
<!-- We must have left bootstrapping behind by now. No more bootstraps, at least for the purpose of calculating uncertainty. (We will use it later for the purpose of out-of-sample testing and avoiding overfitting.) Key lesson is that overfitting is easy. You can't just estimate a value for each cell. You need to smooth and borrow strength. Of course, the only way to do that is with a Bayesian approach. Right? We don't want to get into hacked likelihood approaches. -->
<!-- cces looks perfect for this project. There are 400,000 rows, so it seems like you ought to have plenty of data for anything you want, but once you realize there are 51 states and 10 years, things get sparse fast. We only have 15 observations, for example, for Wyoming in 2007. Once you start subsetting by race and education, you have no choice but to start borrowing strength.  -->
<!-- So, just what will we use? rstanarm(). If so (and if we have not introduced it earlier), we can begin with seeing how it is similar to lm() and then expand. This means that, in one paramter chapter, we should be doing lm( ~ 1). In two parameter, lm( ~ treatment) --- if treatment is zero one --- or, perhaps better, lm( ~ -1 + treatment) if treatment is a factor/character with two levels. We might also have introduced  -->
</div>
</div>
<div id="formatting-linear-models-in-r" class="section level2">
<h2>
<span class="header-section-number">9.3</span> Formatting Linear Models in R</h2>
<p>In the previous chapter, linear models were created using the general format of <code>dependent variable ~ -1 + independent variable</code>. After running the regression on a model of this type, the resulting table would show coefficients for both parameters of the independent variable, for example Democrat and Republican. This method works fine when there are only one or two parameters in consideration, however now we will be moving onto <em>n</em> parameters and we will have to change the formula we use; specifically, we will be removing the -1 from the lm formula.</p>
<p>In order to remove the -1, we first have to understand the function of it in the linear regression formulas that you have seen previously. Using the Enos (2014) <code>trains</code> data, when we use -1 in our formula and regress party on income, the table output lists both parameters and estimates their mean income values.</p>
<pre><code>## # A tibble: 2 x 7
##   term            estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 partyDemocrat    136755.     7545.     18.1  3.12e-35  121808.   151702.
## 2 partyRepublican  167368.    16959.      9.87 6.08e-17  133770.   200967.</code></pre>
<!-- HV: Is there a way to make this output neater in the knit document? More of a typical regression table? -->
<!-- HV: Should I start off introducing rstanarm and use stan_glm for all models in this chapter? -->
<p>This table tells us that Democrats have an average income of $136,755.20, while Republicans have an average income of $167,368.40.</p>
<p>Once we remove the -1 from the formula, the the regression table replaces one of the parameter terms with “(Intercept)”. For example, if we use the same regression as above but remove the -1, the output table changes to this:</p>
<pre><code>## # A tibble: 2 x 7
##   term            estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)      136755.     7545.     18.1  3.12e-35  121808.   151702.
## 2 partyRepublican   30613.    18561.      1.65 1.02e- 1   -6160.    67386.</code></pre>
<p>Although the coefficients in this regression table are different than the ones above, the interpretation stays the same. The intercept still represents the average income of democrats, the default parameter, however the partyRepulican coefficient is now the difference between the mean income of republicans and democrats. The value of the mean republican income remains the same, however, but it is now calculated by adding the partyRepublican coefficient to the intercept estimate.</p>
<pre><code>## [1] 167368</code></pre>
</div>
<div id="adding-parameters" class="section level2">
<h2>
<span class="header-section-number">9.4</span> Adding parameters</h2>
<p>We will now begin adding parameters to our regression models. The simplest way to do so is by adding another independent variable to the regression formula. For example, we can add gender to the same formula we used previously.</p>
<pre><code>## # A tibble: 3 x 7
##   term            estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)      121086.    10710.     11.3  3.10e-20   99867.   142306.
## 2 partyRepublican   31621.    18315.      1.73 8.70e- 2   -4668.    67910.
## 3 genderMale        27856.    13691.      2.03 4.43e- 2     728.    54983.</code></pre>
<div id="understanding-the-model" class="section level3">
<h3>
<span class="header-section-number">9.4.1</span> Understanding the Model</h3>
<p>Models with multiple predictors can become complicated to interpret, since the interpretation of a coefficient somewhat relies on the other variables of the model. The model we have just created summarizes the difference in average income of subjects based on both their political party affiliation and their gender. Using just these two predictors and their coefficients, we have three parameters to describe four types of people: democratic females, democratic males, republican females, and republican males.</p>
<p>We can start off by converting the resulting output into mathematical notation using the simple linear regression equation of $Y =<em>{1}x</em>{1} + <em>{2}x</em>{2} + + $. Using this notation, this model could be written as Income = 121086.46 + 31621.14<em>Republican + 27855.54</em>Male. Both of these predictors, party and gender, are binary and given the output coefficient terms, if a person is Republican it is coded as 1, whereas if they are a Democrat it is coded as 0. The same intuition applies to gender, with male coded as 1 and female coded as 0. Now, we must interpret this model.</p>
<p><em>Intercept</em>: If we think about the intercept in terms of the linear equation we formulated above, it is the value of Y when x is zero, or the income when Republican and Male are both coded zero. Therefore in this model, the intercept represents female democrats and tells us that the mean income for female democrats is $121,086.46.</p>
<p><em>Party coefficient</em>: Alone, this variable compares the incomes of people with different political affiliations. It tells us that Republicans have a mean income that is $31,621.14 more than Democrats. If we look at the model altogether and want the mean income for a female republican, we would add the partyRepublican coefficient to the intercept estimate by coding a 1 to the Republican variable. This gives us a mean income of $152,707.60 for female republicans, compared to the mean income of $121,086.46 for female democrats.</p>
<p><em>Gender coefficient</em>: The genderMale coefficient tells us the difference between the income of a female and a male. If we take out the partyRepublican term, the genderMale coefficient looks specifically at the difference in income of a female democrat versus a male democrat. By adding this coefficient to the intercept value, we find that male democrats have a mean income of $148,942, or $27,855.54 more than female democrats.</p>
<p><em>Using both coefficients</em>: In order to find the mean income of a male republican, we would have to code a 1 to both variables which would lead us to add both the partyRepublican coefficient <em>and</em> the genderMale coefficient to the intercept, since both parameters apply. From this we find the mean income of a republican male is $180,563.10.
<!-- Fix choppy writing in this whole section. Maybe change formatting. --></p>
<p>It is important to note, however, that adding predictors brings about a range of complexities, including which predictors to omit and which to include, interpretations of multiple coefficients, and the uncertainty associated with our models.</p>
</div>
</div>
<div id="interaction-terms" class="section level2">
<h2>
<span class="header-section-number">9.5</span> Interaction terms</h2>
<p>Another way to add parameters to a linear regression formula is by incorporating interaction terms.</p>
</div>
<div id="heterogeneous-treatment-effect" class="section level2">
<h2>
<span class="header-section-number">9.6</span> heterogeneous treatment effect</h2>
<p>fancy way of saying interactions but with a variable that you believe is causal</p>
</div>
<div id="sources-of-uncertainty" class="section level2">
<h2>
<span class="header-section-number">9.7</span> 5 sources of uncertainty</h2>
<p>style.Rmd</p>
<div id="parameter-uncertainty-4" class="section level4">
<h4>
<span class="header-section-number">9.7.0.1</span> 3. Parameter Uncertainty</h4>
<p>estimate of income coefficient
confidence interval gives the range of uncertainty
- when dealing with a new observation, it is not the confidence interval
- this is where you use posterior predict
- difference between uncertainty about parameyer mean estimate but confidence interval for the mean is not equal to the estimate for the new observation</p>
</div>
<div id="unmodeled-variation-3" class="section level4">
<h4>
<span class="header-section-number">9.7.0.2</span> 4. Unmodeled Variation</h4>
</div>
</div>
<div id="rubin-causal-model-2" class="section level2">
<h2>
<span class="header-section-number">9.8</span> Rubin Causal Model</h2>
<p>new female democrat shows up</p>
</div>
<div id="cces-data" class="section level2">
<h2>
<span class="header-section-number">9.9</span> cces data</h2>
<p>In this chapter we will be using the cces data, or Cooperative Congress Election Survey. The CCES is a 50,000+ person national stratified sample survey that consists of a pre- and post- election wave. In the pre-election wave, respondents complete two-thirds of the survey that asks about general political attitudes, various demographic factors, assessment of roll call voting choices, political information, and vote intentions. In the post-election wave, respondents complete the final third of the survey that consists mostly of items related to the election that just occurred.</p>
<p>Some of the key variables that are included in the data set are approval ratings of the elected officials, from the governor to the president, on a scale from “Strongly Disapprove” to “Strongly Approve”. To quantify this data, the new variables <code>approval_pres_num</code>, <code>approval_rep_num</code>, <code>approval_sen1_num</code>, <code>approval_sen2_num</code>, and <code>approval_gov_num</code> have all been created and quantify the approval scale by ordering the responses from a 1 to a 5. Those who answered with “Strongly Disapprove” are a 1 on the approval scale, while those who answered with “Strongly Approve” are a 5 on the numerical approval scale. Those who are neutral answered with “Neither Approve Nor Disapprove” and are quantified as a 3 on the scale. Respondents who answered with “Never Heard / Not Sure” have been removed in order to improve the accuracy of the approval ratings.</p>
<p>Other variables in the cces data include state, race, age, education level, gender, and ideology. The data taken spans from 2006 to 2018, although it should be noted that there are more observations in years with general elections.</p>
</div>
<div id="presidential-approval-overall-by-year-by-state-by-state-x-year-x-educ" class="section level2">
<h2>
<span class="header-section-number">9.10</span> presidential approval; overall; by year; by state; by state x year x educ</h2>
</div>
<div id="need-rstanarm" class="section level2">
<h2>
<span class="header-section-number">9.11</span> need rstanarm</h2>
</div>
<div id="rubin-causal-model-3" class="section level2">
<h2>
<span class="header-section-number">9.12</span> Rubin Causal Model</h2>
<!-- create numeric `rating` 1 to 4. Leave out Never heard. Might use percentage strongly approve. -->
<!-- discuss overall rating for entire date set. One parameter. Discuss. For each year. For each state. -->
<!-- basic lm -->
<!-- lm(data = cces, age ~ 1) %>% tidy(conf.int = TRUE) -->
<!-- lm(data = cces, age ~ -1 + state) %>% tidy(conf.int = TRUE) -->
<!-- lm(data = cces, age ~ -1 + as.factor(year)) %>% tidy(conf.int = TRUE) -->
<!-- obj <- lm(data = cces, age ~ -1 + state*as.factor(year)) %>% tidy(conf.int = TRUE) -->
<!-- Connecting parameters to real world concepts. What are we measuring? validity. -->
<!-- estimands -->

</div>
</div></body></html>

<p style="text-align: center;">
<a href="two-parameters.html"><button class="btn btn-default">Previous</button></a>
<a href="continuous-response.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-07-25
</p>
</div>
</div>



</body>
</html>
