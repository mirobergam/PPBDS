---
output_yaml:
  - _output.yml
---

# Continuous Response {#continuous-response}


<!-- Use nes with ideology as the lefthand side variable? (Should we just do 2016?) Models to use: lm(), stan_glm(), gam() -->

<!-- Overview: The main structure of the this chapter is three full scale linear modeling approaches to the same data set, where the response variable is continuous. (Maybe we have two data sets, and apply all three models to each one?) In each case, we use the tidymodels framework. I am not sure what the best dataset would be. I am not sure what the three approaches should be. Presumably lm() and stan_glm() would be included. Maybe gam() also. Or loess(). Or robust_lm(). Could be something fancier like svm(). Or neural network. -->


<!-- 0) Preamble. Plan out to chapter. Nothing more to learn. Just apply. Review themes.Rmd, and the three problems which we confronted in 9 and solved in 10. What specific questions are we going to answer? -->

<!-- 1) EDA of nes. drop na. -->

<!-- 2) lm. Use all all variables. Go through the process. Use CV to provide a measure of how well it does. Use new_data. And make some predictions. Not noticing that this is a bad model because of overrfitting. -->

<!-- 3) stan_glm Do the same thing. Notice that CV measures are much better! Oh! Lesson: Be wary of overfitting. Maybe go back and re-estimate lm with fewer variables. Or not. -->

<!-- 4) Neural network. https://www.tidymodels.org/learn/models/parsnip-nnet/ All the same things apply. How well in a CV. predictions with new data.  -->

<!-- 5) How do we select among the models. -->

<!-- 6) Answer the questions we started with. -->


<!-- EC: Chapter Organization
Since Ch. 14 might be partially absorbed into this chapter, I'm wondering how to organize it.
Concepts in this chapter are multivariate regression, interaction models, parallel slope models, model selection, 3D scatterplots & regression planes. Concepts in Ch 14 are using tidymodels, machine learning, and cross validation. Here's a rough outline;
1. Multivariate regression and tidymodels
2. Interaction models, parallel slope models, model selection
3. 3D scatterplots & regression planes
4. Introducing machine learning
5. Using tidymodels for cross validation-->



<!-- 2. Use a different model, like loess, to solve the exact same problem. Go through the same overview, but more quickly. Not everything will work. For example, there are no parameter estimates for loess, or at least none that are easily visible. -->

<!-- 3. How do we decide? lm() or loess() or something? Incorporate (i.e., copy and paste) chapter 14 material. Don't do anything with tidymodel syntax. All that falls to chapter 11. But you are explaining every concept. -->

<!-- In loess section, grab a copy of this xkcd and use it: https://xkcd.com/2048/ -->

In the last two chapters, we covered regressions and common pitfalls. This chapter will be all about applying these concepts using the `tidymodels` framework.

The dataset we will be using is `nes` from the `PPBDS.data` package. `nes` contains data from the American National Election Survey, conducted every presidential election cycle. Along with demographic details, such as race, gender, and age, the survey also contains respondents' ideological identification. Because `ideology` is measured on a scale from 1 to 7, we can treat it as our continuous outcome variable.

Wouldn't it be interesting to predict `ideology` based off of other variables? This question has wide-reaching consequences for political polling and election outcomes. Throughout this chapter, we seek to answer this question: Which factors influences one's ideology, and how so?

## Exploratory Data Analysis

Load `PPBDS.data` and `skimr`. `skim()` the dataset.

```{r}
library(PPBDS.data)
library(skimr)
skim(nes)
```

Great! Before we dive in, let's identify the variables of interest:

* `year`
* `state` 
* `gender`
* `race`
* `income`
* `age`
* `education`
* `ideology` as the continuous outcome variable

As you may have gleaned, these variables comprise a person's background. If we were given a new person who was not surveyed, and the above variables, our goal is to predict their ideology correctly. Let's subset the data for only these variables.

```{r eval=FALSE}
nes %>%
  select(year, state, gender, race, income, age, education, ideology)
```

As you also saw, there are a lot of data points in this dataset. `nes` covers 1954 through 2016, but for our sake, we will narrow our scope to just 2016. Because of the changing nature of what makes someone ideologically liberal or ideologically conservative over time, it is best to construct a model in which we eliminate as much of this variance as possible. Keep in mind that, when feeding in new data to our model, it is most accurately applied to voters in or around 2016. Indeed, a 1954 voter might act very differently than a 2016 voter.

Filter the data to only show `year == 2016`.

```{r eval=FALSE}
nes %>%
  select(year, state, gender, race, income, age, education, ideology) %>%
  filter(year == 2016)
```

Note from our previous skimming of the data that all of the variables are incomplete, meaning they contain `NA` values. While there are methods to impute missing data, we will simply remove these values for now. We will save this filtered and cleaned dataset as `nes_2016`.

```{r}
nes_2016 <- nes %>%
  select(year, state, gender, race, income, age, education, ideology) %>%
  filter(year == 2016) %>%
  select(-year) %>%
  drop_na()

nes_2016
```

With this cleaned data, let's dive into the first model we will create, which will create a linear regression.

# `lm()`

First, load the ***tidymodels*** library and use `initial_split()` to split the data into testing and training sets. Don't worry about the testing set  - for now,  we will build our model off of the training set.

```{r}
library(tidymodels)
library(rsample)

nes_split <- initial_split(nes_2016)

nes_training <- nes_split %>% training()
nes_testing <- nes_split %>% testing()
```

Next, construct the linear regression model.

```{r}
lm_model <-
    linear_reg() %>%
    set_engine("lm") %>%
    set_mode("regression")
```

Fit the model to the data. We will first create a regression formula and save it as an R object for future recall.

```{r}
lm_formula <- formula(ideology ~ state + gender + race + income + age + education)
```

```{r echo=FALSE}
fit(lm_model, lm_formula, nes_training)
```

Note that we can use `tidy()`, just like we did in previous chapters, to take a look at the results:

```{r}
fit(lm_model, lm_formula, nes_training) %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, conf.low, conf.high)
```

How do we know if this is a good model? We can check using cross-validation. Cross-validation seeks to minimizes the MSE (mean squared error) of its predicted and real outcomes. There are two important characteristics of the MSE we should always keep in mind:

1. We can think our estimate of the MSE is a random variable. For example, the dataset we have may be a random sample from a larger population. An algorithm may have a lower apparent error than another algorithm due to luck.

2. If we train an algorithm on the same dataset that we use to compute the MSE, we might be overtraining. In general, when we do this, the apparent error will be an underestimate of the true error.

Cross validation is a technique that permits us to alleviate both these problems. To understand cross validation, it helps to think of the *true error*, a theoretical quantity, as the average of many *apparent errors* obtained by applying the algorithm to new random samples of the data, none of them used to train the algorithm. 

However, we only have available one set of outcomes: the ones we actually observed. Cross validation is based on the idea of generating a series of different random samples on which to apply our algorithm. There are several approaches we can use, but the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error.

### cross-validation using ***rsample***

Generally speaking, a machine learning challenge starts with a dataset. We need to build an algorithm using this dataset that will eventually be used in completely independent datasets.

So to imitate this situation, we carve out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a _training set_ and a _test set_. We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes.

We usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain a stable estimate of the loss without fitting an impractical number of models. The `initial_split()` function reserves 25% of the data for testing by default. 

Remember, we cannot touch the testing set! One way we can check whether the model we created works is to use cross-validation, which avoids the problem of overtraining by splitting the data into smaller sections. We'll show you how to do using the ***rsample*** package.

<!-- EC: How do you select what value of `v` to use? Discussion here. -->

```{r}
nes_folds <- nes_training %>%
  vfold_cv(v = 5)
```

How can we work with the `nes_folds` object?  **tidymodels** makes it easy by using the `fit_resamples()` function in the **tune** package. The `fit_resamples()` function takes as its first argument a model specification (such as `lm_model`), then a formula as its second argument, called `preprocessor`.^[Or a recipe, if you delve more deeply into **tidymodels**.]  Finally, the `resamples` argument is where you input the cross-validation dataset.

```{r}
fit_resamples(lm_model,
              recipe(lm_formula, data = nes_training),
              nes_folds)
```

<!-- EC: Getting an error? https://github.com/cimentadaj/tidyflow/issues/12. Insert new paragraph that explains what recipe does. -->

To inspect the average metrics across all the folds, we can use the `collect_metrics()` function:

```{r}
fit_resamples(lm_model,
              recipe(lm_formula, data = nes_training),
              nes_folds) %>%
  collect_metrics()
```

Now that we've viewed the cross-validation metrics, it's time to use new data: namely, the testing set. 

```{r}
lm_model %>%
  fit(lm_formula, data = nes_training) %>%
  predict(new_data = nes_testing)
```

To extract the rmse, we set the "truth" to `ideology` so this function can compare our predicted values to the true values.

```{r}
lm_model %>%
  fit(lm_formula, data = nes_training) %>%
  predict(new_data = nes_testing) %>%
  bind_cols(nes_testing) %>%
  rmse(truth = ideology, estimate = .pred)
```

<!-- EC: Insert additional interpretation. -->

## Using `stan_glm()`

The ***rstanarm*** package contains a lot of powerful functions that conduct Bayesian data analysis by using priors. One such function is `stan_glm()`, which you can think of as the Bayesian way of fitting a regression model. 

We will be following the exact same steps as before. First, we will construct a ***parsnip*** model that uses the "stan" engine.

```{r}
library(rstanarm)

stan_model <-
    linear_reg() %>%
    set_engine("stan") %>%
    set_mode("regression")
```

```{r}
fit(stan_model, lm_formula, nes_training) %>%
  tidy(conf.int = TRUE) %>%
  select(term, estimate, std.error)
```

### Cross Validation

```{r echo=FALSE, cache=TRUE}
fit_resamples(stan_model,
              recipe(lm_formula, data = nes_training),
              nes_folds)
```

<!-- EC: Bulk Effective Samples Size (ESS) is too low error -->

To inspect the average metrics across all the folds, we can use the `collect_metrics()` function:

```{r cache=TRUE}
fit_resamples(stan_model,
              recipe(lm_formula, data = nes_training),
              nes_folds) %>%
  collect_metrics()
```
Discussion of CV results: the rmse values for both models `stan_model` and `lm_model` are both high, with a lower R-squared. This means that our model is not doing well at mapping the predictor variables to the outcome variable. Recall that there are seven different predictor variables; one of them being `state`, which has fifty different levels. 

What if we created a bunch of different models that used different permutations of predictor variables? For example, ones that would potentially exclude `state`.

First, let's create a basic formula that only takes in `age`.

```{r}
basic_form <- formula(ideology ~ age)
```

Next, we can use `update()` to create the more complicated formulas.  `update()` takes as its first argument a formula and as its second argument the additions you want to make.  To keep all the predictors from the first formula and add more, you will start with `~ . + ` and then add more predictors, like so:

```{r}
age_gender_form <- update(basic_form,
                    ~ . + gender)

demo_form <- update(age_gender_form,
                    ~ . + race + income)

demo_interact_form <- update(basic_form,
                             ~ . + race * educ * pres_gop + 
                               female * age * pres_gop)
```

<!-- EC: Is there a better way to test different permutations of the formula besides manually?  -->

## Using a neural network

### ***parsnip*** 


## Model selection

## Wrap-Up

