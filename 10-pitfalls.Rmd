---
output_yaml:
  - _output.yml
---

# Pitfalls {#pitfalls}

<!-- 1. Get pr_push() working. Send me. -->

<!-- 2. Read chapter 5. Set up Zoom with Vivian. -->

<!-- 3. EDA of qscores. Look at EDAs in chapter 8. -->

<!-- 4. rating ~ hours -->

<!-- 5. Deriving causal estimates from observational data. -->

<!-- 6. Prediction only. rating ~ hours + department + enrollment + enrollment*course + whatever. Follow the Gelman advice on how to make a regression. What model do I make? Why? Think about goals?  Use tidymodels syntax. -->

<!-- 7. underfitting versus overfitting. Big idea. How to deal with? Bayes? -->

<!-- 8. Finish by answering question. -->


```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = FALSE, cache.extra = packageVersion('tufte'))

library(gt)
library(tufte)
```

<!-- DK: I am considering some major changes in this chapter. What if this were a chapter of "Pitfalls", or some such? That would allow us to cover all the remain problems before doing our final tour of models in chapters 11 and 12. It also would make those chapters more parallel in construction.

What might go in this chapter in that event?

Overfitting/underfitting. It would be nice to nail all the complexities of this down. Should we even bother to explore it in chapter 9? Holdout samples?

Tidymodels syntax. Again, get this settled before the "graduation exercise" of chapters 11/12.

M and S errors. This stuff is so important that it deserves placement in the main body of the book. Also, the governors data provides a great example to work with. Just use Gelman's blog post!

Estimating causal relationships which are somewhere between purely observational and perfect experiments: regression discontinuity designs, difference-in-difference and so on.

Dealing with parameter uncertainty correctly. That is, in previous chapters, we treated the betas as estimated perfectly. We did not incorporate uncertainty in their estimation into our prediction intervals. (Unless we did that with rstanarm in chapter 9. Or maybe this is a reason to save rstanarm for this chapter.)

Perhaps all this is enough to both fill this chapter and set the stage for 11 and 12.

-->


<!-- 1. Replace Texas data with qscores: rating ~ hours. Keep all that material and verbiage. Add key items from style.Rmd, e.g., parameter uncertainty, unmodelled variation and so on. -->

<!-- 2. Use a different model, like loess, to solve the exact same problem. Go through the same overview, but more quickly. Not everything will work. For example, there are no parameter estimates for loess, or at least none that are easily visible. -->

<!-- 3. How do we decide? lm() or loess() or something? Incorporate (i.e., copy and paste) chapter 14 material. Don't do anything with tidymodel syntax. All that falls to chapter 11. But you are explaining every concept. -->

<!-- 4. We need a Rubin Causal Model section.  Set up a Zoom with Cass to discuss. She is a good source for information about how to make nice looking tables and about how to think about the issue. We might use qscores and just pretend/assume random assignment or we might use a different data set. Key is to set up a Rubin Table in which the potential outcomes are clear. For example, what would my rating be in a assigned 5 hours of work? What about 10 hours? 15 hours? Each of these is a differnt treatment and, therefore, generates a different potential outcome. -->

<!-- 5. Show the technology for creating multiple models. Instead of a single linear model connecting rating to hours, maybe the relationship is different in different divisions or in different departments or in different class enrollment buckets. Show how to explore this. The governor example in the book is not bad. But nor is it good. -->

<!-- 6. We explicitly avoid talking about Bayesian models here because we are only estimating a one or two parameters in each of the models above. So, there is no occasion for pooling, or any of the other Bayesian magic. But you can see how estimating a 100 rating ~ hours models, one for each department, lends itself to a Bayesian approach. On to chapter 11. -->

<!-- In loess section, grab a copy of this xkcd and use it: https://xkcd.com/2048/ -->


<!-- Packages: tidyverse, broom -->

<!-- Commands:  -->

<!-- How do we solve the problems which were identified in chapter 9? -->

<!-- no bootstrap;  introduce tidymodels here for first time; -->

<!-- holdout sample, cross validation, machine learning, test  -->

<!-- DK: Maybe build this from tidymodels, while also mentioning the traditional way of just using lm()? The problem with raw lm() is that it does not work natively in a pipe since the first argument is a formula instead of the data. (Or maybe a lm(y ~ x, data = .) hack is OK?) -->

<!-- DK: Change the smoking example to political campaigns. If you work on the NYT, all you care about is forecasting election results conditional on campaign spending. If you are a (rich!) candidate, you care about the causal effect of spending on votes. Same model might be estimated by both! But the latter needs to be much more careful in deciding whether or not the results are real. -->

<!-- Albert points out a difficulty in combining the RCM with regression. You can't easily put in a distribution for the unknown potential outcome, even if you have a good regression model. You can't just add to the observed outcome because . . . actually I am confused about this! -->

The fundamental goal of data modeling is to make explicit the relationship between:

* an *outcome variable* $y$, also called a *dependent variable* or response variable, and  
* an *explanatory/predictor variable* $x$, also called an *independent variable* or  covariate.

Another way to state this is using mathematical terminology: we will model the outcome variable $y$ "as a function" of the explanatory/predictor variable $x$. When we say "function" here, we aren't referring to functions in R like the `ggplot()` function, but rather to a mathematical function. But, why do we have two different labels, explanatory and predictor, for the variable $x$? That's because even though the two terms are often used interchangeably, roughly speaking data modeling serves one of two purposes:

1. **Modeling for explanation**: When you want to explicitly describe and quantify the relationship between the outcome variable $y$ and an explanatory variable $x$, determine the importance of any relationships, have measures summarizing these relationships, and possibly identify any *causal* relationships between the variables.  (What's a causal relationship? Remember the [Rubin Causal Model](#rubin-causal-model)! The *causal effect* of $x$ on $y$ is the difference in *potential outcomes* of $y$ given different values of $x$.)
1. **Modeling for prediction**: When you want to predict an outcome variable $y$ based on the information contained in a set of predictor variables $x$. Unlike modeling for explanation, however, you don't care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about $y$ using the information in $x$.

For example, say you are interested in an outcome variable $y$ of whether patients develop lung cancer and information $x$ on their risk factors, such as smoking habits, age, and socioeconomic status. If we are modeling for explanation, we would be interested in both describing and quantifying the effects of the different risk factors. One reason could be that you want to design an intervention to reduce lung cancer incidence in a population, such as increasing family income.  In that case, you would want to know the causal effect of income on the incidence of lung cancer.

If we are modeling for prediction, however, we wouldn't care so much about understanding how all the individual risk factors contribute to lung cancer, but rather only whether we can make good predictions of which people will contract lung cancer.

<!-- DK: Find a way to use this reference: [*An Introduction to Statistical Learning with Applications in R (ISLR)*](http://www-bcf.usc.edu/~gareth/ISL/)  -->

Linear regression involves a *numerical* outcome variable $y$ and explanatory variables $x$ that are either *numerical* or *categorical*. Furthermore, the relationship between $y$ and $x$ is assumed to be linear, or in other words, a line. However, we'll see that what constitutes a "line" will vary depending on the nature of your explanatory variables $x$.

<!-- DK: Could give a better plan overview, including discussion of chapters 11 and 12. Indeed, perhaps also looking backward to sampling and uncertainty. Need to rewrite this if we re-organize the book. Indeed, the introductions (and conclusions) to each chapter should be similar, providing a framework in which that chapter fits. -->

In Section \@ref(model1), the explanatory variable will be numerical. This scenario is known as *simple linear regression*. In Section \@ref(model2), the explanatory variable will be categorical.

In Chapter \@ref(continuous-response) on multiple regression, we'll extend the ideas behind basic regression and consider models with two explanatory variables $x_1$ and $x_2$.  In Section \@ref(model4), we'll have two numerical explanatory variables. In Section \@ref(model3), we'll have one numerical and one categorical explanatory variable. In particular, we'll consider two such models: *interaction* and *parallel slopes* models.

Let's now begin with basic regression, \index{regression!basic} which refers to linear regression models with a single explanatory variable $x$. We'll also discuss important statistical concepts like the *correlation coefficient*, that "correlation isn't necessarily causation," and what it means for a line to be "best-fitting."

Let's now load all the packages needed for this chapter (this assumes you've already installed them). The main packages are ones we have used before. The Advanced Section of the chapter makes use of

1. The **rstanarm** package, which provides an interface to the statistical inference engine, Stan, for Bayesian Regression Modeling. 
1. The **tidybayes** package, which aids in formating Bayesian modeling outputs in a tidy manner and provides ggplot geoms for plotting. 


```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(PPBDS.data)
library(broom)
library(skimr)
library(gapminder)
library(rstanarm)
library(tidybayes)
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text.
library(mvtnorm)
library(kableExtra)
library(patchwork)
```


## Teaching evaluations: one numerical explanatory variable {#model1}

<!-- DK: I think this is good. I like walking through EDA. Should do that each chapter, maybe showing more tricks each time. But in what order? skim() here then calculate your own simply in 12 and then calculate using new dplyr 1.0.0 tricks like across() in 13. Also, stop replying on moderndive. Just include this data in our package or on line. Indeed, do we need our own package? Or just our own dedicated set of Google sheets? We need our own package, since we will be using a bunch of datasets, some created/cleaned by us. -->

<!-- EG: I really like this section- I think that the in-depth explanations of not only how to find correlation coefficients but also interpret them accurately and effectively is great. -->

<!-- DK: Next version, can't use these examples. Need political ones. Let's use an updated version of US congress campaigns. This connects to the new introduction. Can use latest data. Can include variables like last election results, allowing us to follow Gelman's version. Can also include spending data, so that we can discuss a causal model. Perhaps in this chapter, we do one model with each, the second going faster. Then, next chapter, put both variables in the regression. This example would also be nice because we could use list columns for the secret trick, estimating models by year and by country region. -->

<!-- EG: I'll change this to qscores, along with an adjusted EDA for that dataset and more explanation of how correlation != causation. I'll also provide more investigation into the many ways confounding variables could impact why students provide certain qscores rather than simply hours of work, along with more language of comparison. -->

<!-- DK: I have skimmed the next section, since I plan on cutting it all. -->

Why do some professors and instructors at universities and colleges receive high teaching evaluations scores from students while others receive lower ones? Are there differences in teaching evaluations between instructors of different demographic groups? Could there be an impact due to student biases? These are all questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which instructors and professors get promoted.

Researchers at the University of Texas in Austin, Texas (UT Austin) tried to answer the following research question: what factors explain differences in instructor teaching evaluation scores? To this end, they collected instructor and course information on 463 courses. A full description of the study can be found at [openintro.org](https://www.openintro.org/data/index.php?data=evals).

In this section, we'll keep things simple for now and try to explain differences in instructor teaching scores as a function of one numerical variable: the instructor's "beauty" score (we'll describe how this score was determined shortly). Could it be that instructors with higher "beauty" scores also have higher teaching evaluations? Could it be instead that instructors with higher "beauty" scores tend to have lower teaching evaluations? Or could it be that there is no relationship between "beauty" score and teaching evaluations? We'll answer these questions by modeling the relationship between teaching scores and "beauty" scores using *simple linear regression* \index{regression!simple linear} where we have:

1. A numerical outcome variable $y$ (the instructor's teaching score) and
1. A single numerical explanatory variable $x$ (the instructor's "beauty" score).


### Exploratory data analysis {#model1EDA}

The data on the 463 courses at UT Austin can be found in the `evals` data frame included in the **moderndive** package. However, to keep things simple, let's `select()` only the subset of the variables we'll consider in this chapter, and save this data in a new data frame called `evals_ch11`:

```{r}
library(moderndive)

evals_ch11 <- evals %>%
  select(ID, score, bty_avg, age)
```

A crucial step before doing any kind of analysis or modeling is performing an *exploratory data analysis*, \index{data analysis!exploratory} or EDA for short. EDA gives you a sense of the distributions of the individual variables in your data, whether any potential relationships exist between variables, whether there are outliers and/or missing values, and (most importantly) how to build your model. Here are three common steps in an EDA:

<!-- DK: Good stuff. We should keep this and follow it, each chapter. -->

1. Most crucially, looking at the raw data values.
1. Computing summary statistics, such as means, medians, and interquartile ranges.
1. Creating data visualizations.

Let's perform the first common step in an exploratory data analysis: looking at the raw data values. Because this step seems so trivial, unfortunately many data analysts ignore it. However, getting an early sense of what your raw data looks like can often prevent many larger issues down the road. 

You can do this by using RStudio's spreadsheet viewer or by using the `glimpse()` function as introduced in Subsection \@ref(exploredataframes) on exploring data frames:

<!-- DK: Add summary() -->

```{r}
glimpse(evals_ch11) %>%
  summary()

```

<!-- EG: I can change some of this language later to reflect the vocab of glimpse with summary as opposed to just glimpse, if we like this option. -->

Observe that `Observations: 463` indicates that there are 463 rows/observations in `evals_ch11`, where each row corresponds to one observed course at UT Austin. It is important to note that the *observational unit* \index{observational unit} is an individual course and not an individual instructor. Recall from Subsection \@ref(exploredataframes) that the observational unit is the "type of thing" that is being measured by our variables. Since instructors teach more than one course in an academic year, the same instructor will appear more than once in the data. Hence there are fewer than 463 unique instructors being represented in `evals_ch11`.

A full description of all the variables included in `evals` can be found at [openintro.org](https://www.openintro.org/data/index.php?data=evals) or by reading the associated help file (run `?evals` in the console). However, let's fully describe only the `r ncol(evals_ch11)` variables we selected in `evals_ch11`:

1. `ID`: An identification variable used to distinguish between the 1 through 463 courses in the dataset.
1. `score`: A numerical variable of the course instructor's average teaching score, where the average is computed from the evaluation scores from all students in that course.  Teaching scores of 1 are lowest and 5 are highest. This is the outcome variable $y$ of interest.
1. `bty_avg`: A numerical variable of the course instructor's average "beauty" score, where the average is computed from a separate panel of six students. "Beauty" scores of 1 are lowest and 10 are highest. This is the explanatory variable $x$ of interest.
1. `age`: A numerical variable of the course instructor's age. 

An alternative way to look at the raw data values is by choosing a random sample of the rows in `evals_ch11` by piping it into the `sample_n()` \index{dplyr!sample\_n()} function from the **dplyr** package. Here we set the `size` argument to be `5`, indicating that we want a random sample of 5 rows. We display the results below. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

```{r, eval=FALSE}
evals_ch11 %>%
  sample_n(size = 5)
```
```{r, echo=FALSE, fig.cap="A random sample of 5 out of the 463 courses at UT Austin"}
evals_ch11 %>%
  sample_n(5) 
```

Now that we've looked at the raw values in our `evals_ch11` data frame and got a preliminary sense of the data, let's move on to the next common step in an exploratory data analysis: computing summary statistics. Let's start by computing the mean and median of our numerical outcome variable `score` and our numerical explanatory variable "beauty" score denoted as `bty_avg`. We'll do this by using the `summarize()` function from `dplyr` along with the `mean()` and `median()` summary functions we saw in Section \@ref(summarize).

```{r, eval=TRUE}
evals_ch11 %>%
  summarize(mean_bty_avg = mean(bty_avg),
            mean_score = mean(score),
            median_bty_avg = median(bty_avg),
            median_score = median(score))
```

<!-- DK: This is nice. Having motivated the use of skim() once, we can just go straight to using it in other chapters. And/or show other tricks each chapter, like across(). -->

However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles? 

Typing out all these summary statistic functions in `summarize()` would be long and tedious. Instead, let's use the convenient `skim()` function from the **skimr** package. This function takes in a data frame, "skims" it, and returns commonly used summary statistics. Let's take our `evals_ch11` data frame, `select()` only the outcome and explanatory variables teaching `score` and `bty_avg`, and pipe them into the `skim()` function:

```{r}
evals_ch11 %>% 
  select(score, bty_avg) %>% 
  skim()
```


For the numerical variables teaching `score` and `bty_avg` it returns:

- `n_missing`: the number of missing values
- `complete_rate`: the percentage of non-missing or complete values
- `mean`: the average
- `sd`: the standard deviation
- `p0`: the 0th percentile: the value at which 0% of observations are smaller than it (the *minimum* value)
- `p25`: the 25th percentile: the value at which 25% of observations are smaller than it (the *1st quartile*)
- `p50`: the 50th percentile: the value at which 50% of observations are smaller than it (the *2nd* quartile and more commonly called the *median*)
- `p75`: the 75th percentile: the value at which 75% of observations are smaller than it (the *3rd quartile*)
- `p100`: the 100th percentile: the value at which 100% of observations are smaller than it (the *maximum* value)

Looking at this output, we can see how the values of both variables are distributed. For example, the mean teaching score was 4.17 out of 5, whereas the mean "beauty" score was 4.42 out of 10. Furthermore, the middle 50% of teaching scores was between 3.80 and 4.6 (the first and third quartiles), whereas the middle 50% of "beauty" scores falls within 3.17 to 5.5 out of 10.

<!-- DK: Keep this. -->

The `skim()` function only returns what are known as *univariate* \index{univariate} summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist *bivariate* \index{bivariate} summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the \index{correlation (coefficient)} *correlation coefficient*. Generally speaking, *coefficients* are quantitative expressions of a specific phenomenon.  A *correlation coefficient* is a quantitative expression of the *strength of the linear relationship between two numerical variables*. Its value ranges between -1 and 1 where:

* -1 indicates a perfect *negative relationship*: As one variable increases, the value of the other variable tends to go down, following a straight line.
* 0 indicates no relationship: The values of both variables go up/down independently of each other.
* +1 indicates a perfect *positive relationship*: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion.

The following figure gives examples of 9 different correlation coefficient values for hypothetical numerical variables $x$ and $y$. For example, observe in the top right plot that for a correlation coefficient of -0.75 there is a negative linear relationship between $x$ and $y$, but it is not as strong as the negative linear relationship between $x$ and $y$ when the correlation coefficient is -0.9 or -1.

```{r, echo=FALSE, fig.cap="Nine different correlation coefficients."}
correlation <- c(-0.9999, -0.9, -0.75, -0.3, 0, 0.3, 0.75, 0.9, 0.9999)
n_sim <- 100
values <- NULL
for(i in seq_along(correlation)){
  rho <- correlation[i]
  sigma <- matrix(c(5, rho * sqrt(50), rho * sqrt(50), 10), 2, 2)
  # EG- should we consider explaining more about what is happening within "sim"? I personally haven't used rmvnorm before and am unsure whether it's been discussed in a previous chapter, or whether some explanation/clarification could be helpful.
  sim <- rmvnorm(
    n = n_sim,
    mean = c(20,40),
    sigma = sigma
    ) %>%
    as.data.frame() %>% 
    as_tibble() %>%
    mutate(correlation = round(rho,2))

  values <- bind_rows(values, sim)
}

corr_plot <- ggplot(data = values, mapping = aes(V1, V2)) +
  geom_point() +
  facet_wrap(~ correlation, ncol = 3) +
  labs(x = "x", y = "y") +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank())

if(knitr::is_latex_output()){
  corr_plot +
  theme(
    strip.text = element_text(colour = 'black'),
    strip.background = element_rect(fill = "grey93")
  )
} else {
  corr_plot
}
```


The correlation coefficient can be computed using the `cor()` summary function within a `summarize()`:

```{r, eval=FALSE}
evals_ch11 %>% 
  summarize(correlation = cor(score, bty_avg))
```

```{r, echo=FALSE}
cor_ch11 <- evals_ch11 %>%
  summarize(correlation = cor(score, bty_avg)) %>% 
  round(3) %>% 
  pull()
```

In our case, the correlation coefficient of `r cor_ch11` indicates that the relationship between teaching evaluation score and "beauty" average is "weakly positive." There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren't close to the extreme values of -1, 0, and 1. 

Let's now perform the last of the steps in an exploratory data analysis: creating data visualizations. Since both the `score` and `bty_avg` variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let's do this using `geom_point()` and display the result. Furthermore, let's highlight the six points in the top right of the visualization in a box.

```{r, eval=FALSE}
evals_ch11 %>%
  ggplot(aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "Beauty Score", 
       y = "Teaching Score",
       title = "Scatterplot of relationship of teaching and beauty scores")
```

```{r, warning=FALSE, echo=FALSE, fig.cap="Instructor evaluation scores at UT Austin.", fig.height=4.5}
# Define orange box
margin_x <- 0.15
margin_y <- 0.075
box <- tibble(
  x = c(7.83, 8.17, 8.17, 7.83, 7.83) + c(-1, 1, 1, -1, -1) * margin_x,
  y = c(4.6, 4.6, 5, 5, 4.6) + c(-1, -1, 1, 1, -1) * margin_y
  )

evals_ch11 %>%
  ggplot(aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "Beauty Score", 
       y = "Teaching Score",
       title = "Scatterplot of relationship of teaching and beauty scores") +
  geom_path(data = box, aes(x=x, y=y), col = "orange", size = 1)
```

Observe that most "beauty" scores lie between 2 and 8, while most teaching scores lie between 3 and 5. Furthermore, while opinions may vary, it is our opinion that the relationship between teaching score and "beauty" score is "weakly positive." This is consistent with our earlier computed correlation coefficient of `r cor_ch11`.

Furthermore, there appear to be six points in the top-right of this plot highlighted in the box. However, this is not actually the case, as this plot suffers from *overplotting*. Recall from Subsection \@ref(overplotting) that overplotting occurs when several points are stacked directly on top of each other, making it difficult to distinguish them. So while it may appear that there are only six points in the box, there are actually more.  This fact is only apparent when using `geom_jitter()` in place of `geom_point()`. We display the resulting plot along with the same small box as before.

```{r, eval=FALSE}
evals_ch11 %>%
  ggplot(aes(x = bty_avg, y = score)) +
  geom_jitter() +
  labs(x = "Beauty Score", y = "Teaching Score",
       title = "Scatterplot of relationship of teaching and beauty scores")
```

```{r, warning=FALSE, echo=FALSE, fig.cap="Instructor evaluation scores at UT Austin.", fig.height=4.2}
ggplot(evals_ch11, aes(x = bty_avg, y = score)) +
  geom_jitter() +
  labs(x = "Beauty Score", y = "Teaching Score",
       title = "(Jittered) Scatterplot of relationship of teaching and beauty scores") +
  geom_path(data = box, aes(x = x, y = y), col = "orange", size = 1)
```

It is now apparent that there are `r evals_ch11 %>% filter(score > 4.5 & bty_avg > 7.5) %>% nrow()` points in the area highlighted in the box and not six as originally suggested. Recall from Subsection \@ref(overplotting) on overplotting that jittering adds a little random "nudge" to each of the points to break up these ties. Furthermore, recall that jittering is strictly a visualization tool; it does not alter the original values in the data frame `evals_ch11`. To keep things simple going forward, however, we'll only present regular scatterplots rather than their jittered counterparts.

Let's build on the unjittered scatterplot by adding a "best-fitting" line: of all possible lines we can draw on this scatterplot, it is the line that "best" fits through the cloud of points. We do this by adding a new `geom_smooth(method = "lm", se = FALSE)` layer to the `ggplot()` code that created the scatterplot. The `method = "lm"` argument sets the line to be a "`l`inear `m`odel." The `se = FALSE` \index{ggplot2!geom\_smooth()} argument suppresses _standard error_ uncertainty bars. (We defined the concept of _standard error_ in Subsection \@ref(sampling-definitions).)

```{r, warning=FALSE, fig.cap="Regression line."}
evals_ch11 %>%
  ggplot(aes(x = bty_avg, y = score)) +
  geom_point() +
  labs(x = "Beauty Score", y = "Teaching Score",
       title = "Relationship between teaching and beauty scores") +  
  geom_smooth(method = "lm", se = FALSE)
```

The line in the resulting figure is called a "regression line." The regression line \index{regression!line} is a visual summary of the relationship between two numerical variables, in our case the outcome variable `score` and the explanatory variable `bty_avg`. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of `r cor_ch11` suggesting that there is a positive relationship between these two variables: as instructors have higher "beauty" scores, so also do they receive higher teaching evaluations. We'll see later, however, that while the correlation coefficient and the slope of a regression line always have the same sign (positive or negative), they typically do not have the same value.

Furthermore, a regression line is "best-fitting" in that it minimizes some mathematical criteria. We present these mathematical criteria in Section \@ref(leastsquares), but we suggest you read this subsection only after first reading the rest of this section on regression with one numerical explanatory variable.


### Simple linear regression {#model1table}

<!-- DK: Make this with just one decimal. Indeed, can we make that consistent everywhere? Also, bty_avg should be renamed beauty. We don't really care if it is an average. 

The train example? With att_start and att_end? Or some other data set? Which one? Or qscores?
-->

<!-- EG: I like the train example. It's simple, politically-focused, and works well with the tricks we're trying to show here. -->

You may recall from secondary/high school algebra that the equation of a line is $y = a + b\cdot x$. (Note that the $\cdot$ symbol is equivalent to the $\times$ "multiply by" mathematical symbol. We'll use the $\cdot$ symbol in the rest of this book as it is more succinct.) It is defined by two coefficients $a$ and $b$. The intercept coefficient $a$ is the value of $y$ when $x = 0$. The slope coefficient $b$ for $x$ is the increase in $y$ for every increase of one in $x$. This is also called the "rise over run."

However, when defining a regression line, we use slightly different notation: the equation of the regression line is $\widehat{y} = b_0 + b_1 \cdot x$ \index{regression!equation of a line}. The intercept coefficient is $b_0$, so $b_0$ is the value of $\widehat{y}$ when $x = 0$. The slope coefficient for $x$ is $b_1$, i.e., the increase in $\widehat{y}$ for every increase of one in $x$. Why do we put a "hat" on top of the $y$? It's a form of notation commonly used in regression to indicate that we have a \index{regression!fitted value} "fitted value," or the value of $y$ on the regression line for a given $x$ value. We'll discuss this more in the upcoming Subsection \@ref(model1points).

<!-- DK: Is this a good introduction to hat notation? This stikes me as a subtle point that should be wesved throughout the book.  -->

<!-- EG: Honestly don't think that this introduction is bad, but certainly think that it could happen much earlier in the book. Many students may already be familiar with hat notation and it is key to regression language. -->

We know that the regression line we plotted has a positive slope $b_1$ corresponding to our explanatory $x$ variable `bty_avg`. Why? Because instructors with higher `bty_avg` scores tend to have higher teaching evaluation `scores`. However, what is the numerical value of the slope $b_1$? What about the intercept $b_0$?  Let's not compute these two values by hand, but rather let's use a computer!

We can obtain the values of the intercept $b_0$ and the slope for `btg_avg` $b_1$ in two steps:

1. "Fit" the linear regression model using the `lm()` function and save it in `score_model`. We put the name of the outcome variable on the left-hand side of the `~` "tilde" sign, while putting the name of the explanatory variable on the right-hand side. This is known as R's \index{R!formula notation} *formula notation*.
1. Apply the `tidy()` \index{broom!tidy()} function from the **broom** package to `score_model`.

<!-- EG: Should we take a sentence to explain why tidy is being used? Something as simple as "This will create the regression table." -->

```{r, eval=FALSE}
# Fit regression model:

score_model <- lm(score ~ bty_avg, data = evals_ch11)

# Get regression table:

score_model %>% 
  tidy(conf.int = TRUE)
```
```{r, echo=FALSE}
score_model <- lm(score ~ bty_avg, data = evals_ch11)
evals_line <- score_model %>%
  tidy() %>%
  pull(estimate)
```
```{r, echo=FALSE, fig.cap="Linear regression table"}
tidy(score_model,
     conf.int = TRUE) 
```

Note that we used the argument `conf.int = TRUE` in the `tidy()` function; that will be important for later.

How did this code work?  First, we "fit" the linear regression model to the `data` using the `lm()` \index{lm()} function and saved this as `score_model`. When we say "fit", we mean  "find the best fitting line to this data." `lm()` stands for "linear model" and is used as follows: `lm(y ~ x, data = data_frame_name)` where:

* `y` is the outcome variable, followed by a tilde `~`. In our case, `y` is set to `score`.
* `x` is the explanatory variable. In our case, `x` is set to `bty_avg`.
* The combination of `y ~ x` is called a *model formula*. (Note the order of `y` and `x`.) In our case, the model formula is `score ~ bty_avg`.
* `data_frame_name` is the name of the data frame that contains the variables `y` and `x`. In our case, `data_frame_name` is the `evals_ch11` data frame.

Second, we take the saved model in `score_model` and apply the `tidy()` function from the **broom** package to it to obtain the regression table. 

We can also do this with pipes:

```{r}
evals_ch11 %>% 
  lm(score ~ bty_avg, data = .) %>% 
  tidy(conf.int = TRUE) %>% 
  select(term, estimate, conf.low, conf.high)
```

Recall from our discussion in Chapter \@ref(tidyverse) that we can use "." to refer to the tibble that has been passed in by the proceeding pipe. In this case, that is the `eval_ch11` tibble. Using "." to refer to the passed-down tibble will be a trick which we use again and again in the coming chapters.

You may wonder why this code does not work:

```{r, eval=FALSE}
evals_ch11 %>% 
  lm(score ~ bty_avg) 
```

<!-- DK: How can we show the error which happens when this code runs? -->

<!-- EG: Not totally sure what is meant by "show the error," but I think the explanation below the code is sufficient for people to understand why lm does not work with piped in data. -->

With most of the functions we have seen, we don't need the "hack" of referring to the passed-down tibble with a ".". Things "just work." Functions like `filter()` and `arrange()` just know to work on the tibble which is coming there way. Why doesn't `lm()` do that?

The answer is that `lm()` is, like many parts of R, an old function, created before the Tidyverse came into style. Look at the help page with `?lm`. As you can see, the first argument is `formula`. The automatic pass-down trick only works with functions in which the first argument is the data to be used. This is (almost) always the case in Tidyverse functions.

### Interpreting regression coefficients

In the `estimate` column are the intercept $b_0$ = `r evals_line[1]` and the slope $b_1$ = `r evals_line[2]` for `bty_avg`. Thus the equation of the regression line follows:

$$
\begin{aligned}
\widehat{y} &= b_0 + b_1 \cdot x\\
\widehat{\text{score}} &= b_0 + b_{\text{bty}\_\text{avg}} \cdot\text{bty}\_\text{avg}\\
&= 3.880 + 0.067\cdot\text{bty}\_\text{avg}
\end{aligned}
$$

<!-- DK: "beauty" rather than "bty_avg" would be a better variable name. Don't need to recall that the beauty score is an average. After all, the score is an average also! Also: Need a discussion of potential outcomes and RCM here. Can we manipulate beauty? On one hand, no. On the other hand, yes. Makeup, lighting and so on. Could provide an interesting complex case to discuss. -->

<!-- EG: I agree that RCM and potential outcomes would fit well within this discussion.Took a look at chapter 3 and noticed that the trains example is used there to explain potential outcomes. Should we stick with using the UT data here and simply apply the language of potential outcomes to this example as well? -->

The intercept $b_0$ = `r evals_line[1]` is the average teaching score $\widehat{y}$ = $\widehat{\text{score}}$ for those courses where the instructor had a "beauty" score `bty_avg` of 0. Or in graphical terms, it's where the line intersects the $y$ axis when $x$ = 0. Note, however, that while the \index{regression!equation of a line!intercept} intercept of the regression line has a mathematical interpretation, it has no *practical* interpretation here, since observing a `bty_avg` of 0 is impossible; it is the average of six panelists' "beauty" scores ranging from 1 to 10. Furthermore, looking at the scatterplot with the regression line, no instructors had a "beauty" score anywhere near 0.

<!-- DK: Add discussion of "hat". Recall the p hat which we estimated last time. y hat is like that! It is not a variable that we can observe. It is an estimate. (Indeed, it is an estimate of a potential outcome!) This is different form x, which has no hat, because it is real data, something we can see. Side note: not sure if the hat versus no hat distinction works well with b_0/b_1 being things we can't see and need to estimate, but for which we do not use hat notation. -->

<!-- EG: I'll add a discussion of y hat as I start writing. -->

Of greater interest is the \index{regression!equation of a line!slope} slope $b_1$ = $b_{\text{bty_avg}}$ of `r evals_line[2]`. The "bty_avg" subscript indicates that this number summarizes the relationship between the teaching and average beauty score variables. Note that the sign is positive, suggesting a positive relationship between these two variables, meaning teachers with higher `bty_avg` scores also tend to have higher teaching scores. Recall from earlier that the correlation coefficient is `r cor_ch11`. They both have the same positive sign, but have a different value. Recall further that the correlation's interpretation is the "strength of linear association". The \index{regression!interpretation of the slope} slope's interpretation is a little different:

<!-- DK: Let's make sure that these interpretations are highly consistent with Gelman and across the book. -->

> For every increase of 1 unit in `bty_avg`, there is an *associated* increase of, *on average*, `r evals_line[2]` units of `score`.

We say that this associated increase is *on average* `r evals_line[2]` units of teaching `score`, because you might have two instructors whose `bty_avg` scores differ by 1 unit, but their difference in teaching scores won't necessarily be exactly `r evals_line[2]`. What the slope of `r evals_line[2]` is saying is that across all possible courses, the *average* difference in teaching score between two instructors whose "beauty" scores differ by one is `r evals_line[2]`.

Furthermore, we only state that there is an *associated* increase and not necessarily a *causal* increase. For example, perhaps it's not that higher "beauty" scores directly cause higher teaching scores per se. Instead, the following could hold true: individuals from wealthier backgrounds tend to have stronger educational backgrounds and hence have higher teaching scores, while at the same time these wealthy individuals also tend to have higher "beauty" scores. In other words, just because two variables are strongly associated, it doesn't necessarily mean that one causes the other. This is summed up in the often quoted phrase, "correlation is not necessarily causation."

Consider an example: a not-so-great medical doctor goes through medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares, "Sleeping with shoes on causes headaches!"

```{r, echo=FALSE, fig.cap="Does sleeping with shoes on cause headaches?"}
knitr::include_graphics("10-pitfalls/images/shoes_headache.png")
```

However, there is a good chance that if someone is sleeping with their shoes on, it's potentially because they are intoxicated from alcohol. Furthermore, higher levels of drinking leads to more hangovers, and hence more headaches. The amount of alcohol consumption here is what's known as a *confounding* variable\index{confounding variable}. It lurks behind the scenes, confounding the causal relationship (if any) of "sleeping with shoes on" with "waking up with a headache." We can summarize this with a *causal graph* where:

* Y is a *response* variable; here it is "waking up with a headache." \index{variables!response / outcome / dependent}
* X is a *treatment* variable whose causal effect we are interested in; here it is "sleeping with shoes on."\index{variables!treatment}

```{r, echo=FALSE, fig.cap="Causal graph."}
knitr::include_graphics("10-pitfalls/images/flowchart.009-cropped.png")
```

To study the relationship between Y and X, we could use a regression model where the outcome variable is set to Y and the explanatory variable is set to be X, as you've been doing throughout this chapter. However, the causal graph also includes a third variable with arrows pointing at both X and Y:

* Z is a *confounding* variable \index{variables!confounding} that affects both X and Y, thereby "confounding" their relationship. Here the confounding variable is alcohol.

Alcohol will cause people to be both more likely to sleep with their shoes on as well as be more likely to wake up with a headache. We can see why this is a problem under the potential outcomes framework: both whether you receive the treatment "sleeping with shoes on" and your potential outcome depend on whether you consumed alcohol, which means that you can't estimate the treatment effect of "sleeping with shoes on" simply by comparing the observed outcome under treatment and the observed outcome under control.  Thus any regression model of the relationship between X and Y should also use Z as an explanatory variable. In other words, our doctor needs to take into account who had been drinking the night before. In the next chapter, we'll start covering multiple regression models that allow us to incorporate more than one variable in our regression models.

<!-- DK: In the next chapter? Not if we re-organize. -->

Establishing causation is a tricky problem and frequently takes either randomized controlled trials or methods to adjust for the effects of confounding variables. Both these approaches attempt, as best they can, either to take all possible confounding variables into account or negate their impact. This allows researchers to focus only on the relationship of interest: the relationship between the outcome variable Y and the treatment variable X.

As you read news stories, be careful not to fall into the trap of thinking that correlation necessarily implies causation.  Check out the [Spurious Correlations](http://www.tylervigen.com/spurious-correlations) website for some rather comical examples of variables that are correlated, but are definitely not causally related.

<!--AR: trying to be precise that it is correlation with the treatment and *potential* outcomes that matters, not correlation with the treatment and observed outcomes.  What's a better way to explain this?
DK: Agreed! Indeed, we need to hit the notion of potential outcomes each chapter. Potential outcomes != observed outcomes! Not mentioning RCM for 4 weeks is a mistake. -->   

Let's say, however, that we were confident that there is no confounding: that is, that there are no variables such as socioeconomic background that correlate both with a teacher's beauty score and the teacher's potential outcomes. (We shouldn't be confident in this, but let's play along for the moment.)  Then, we can interpret the slope in terms of the [Rubin Causal Model](#rubin-causal-model).  The slope coefficient on `bty_avg` of `r evals_line[2]` then means that the *average treatment effect* of increasing a teacher's "beauty" score by 1 is `r evals_line[2]`.  In the absence of randomization, however, this is likely not a good interpretation of this regression!  Adding additional variables, as we'll do in Chapter \@ref{multiple-regression}, may make it more plausible to interpret the regression causally.

Keeping with the causal interpretation, let's say that a teacher named Joe had a "beauty" score of 7 and a teaching score of 4. We are curious about how much moving from a "beauty" score of 7 versus "beauty" score of 8 would have affected Joe's teaching score.  Thus, we are comparing the *potential outcome* for Joe when `bty_avg` is 7 versus the *potential outcome* when `bty_avg` is 8.  Only one of these potential outcomes was observed.

<!-- DK: This is good stuff. Belongs in every chapter. -->

<!-- EG: Should we be using kable here? Or are we trying to make the switch to gt? -->

<!-- EG: I'm going to switch this to gt next pass around. -->

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`Subject` = "Joe",
       `$Y_{bty\\_avg = 7}$` = c("$4$"),
       `$Y_{bty\\_avg = 8}$` = c("$?$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

We can use our slope coefficient from the linear regression, which is our best estimate of the average treatment effect (ATE), to fill in the missing potential outcome:

```{r, echo = FALSE}
# First, we create a tibble with the values we want for the table

tibble(`Subject` = "Joe",
       `$Y_{bty\\_avg = 7}$` = c("$4$"),
       `$Y_{bty\\_avg = 8}$` = c("$4.067$"),
       `$\\widehat{ATE}$` = c("$0.067$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

Thus, our best guess of Joe's potential outcome under the "treatment" of $score = 8$ is $4.067$.

## Uncertainty in simple linear regressions

<!-- EG: Will provide more explanation of parameter undertainty using tidy(). -->

You might be wondering what the remaining five columns created by `tidy()` are: `std.error`, `statistic`, `p.value`, `conf.low` and `conf.high`. They are the _standard error_, _test statistic_, _p-value_, _lower 95% confidence interval bound_, and _upper 95% confidence interval bound_. We will focus on the confidence interval bounds. We have already explored the concept of confidence intervals in Chapter \@ref(confidence-intervals).  These bounds are an application of the same concept, but applied to the slope of our simple linear regression.

Let's load the **infer** package.  We're going to use the function `rep_sample_n()` to resample from our data 1,000 times:

```{r}
library(infer)

x <- evals_ch11 %>% 
  select(score, bty_avg) %>% 
  rep_sample_n(size = nrow(evals_ch11), replace = TRUE, reps = 1000)

x
```

For each `replicate`, we have `r nrow(evals_ch11)` resamples of `score` and `bty_avg`.

Next, we are going to introduce a new function: `nest()`:

```{r}
x <- x %>% 
  group_by(replicate) %>% 
  nest()

x
```

`nest()` comes from the **tidyr** package.  It's easiest to understand what `nest()` does by looking at its output. After grouping by `replicate` and using `nest()`, we now have a dataset of 1,000 rows and a list column named `data`.  What's in this list column?  Each element of `data` is a tibble consisting of one of the resampled datasets we created using `rep_sample_n()`.  `nest()` is thus a useful function when you have a series of bootstrapped samples and want to run the same function on each sample.

<!-- DK: This transition goes a bit fast. We need to take a tour of the tibbles in data. Look at one. Look at another. See that they are different. Explain how this is a more industrial strength way of using rep_sample(). But maybe that means that rep_sample_n(), used alone, is a bad hack. Maybe we should have done it this way, with nest(), in chapters 9 and 10. In fact, maybe we could get rid of rep_sample_n and use something in **rsample** to accomplish these two steps. -->

<!-- EG: I agree with the comment above. Nest() is a tricky function that takes a while to understand (and also to figure out how to use properly and when it is necessary!) I think comparison is a great idea here. Slowly going through the differences in using nest() and rep_sample_n(). I'm not sure about cutting the mention of r_sample_n() entirely (it is indeed a useful hack to know), but I think it is important to go over in detail when nest() ought to be used in the place of rep_sample_n(). -->

Now, we can use `map()` to run our linear regression for each dataset:

<!-- DK: Whoah! Note that this use of "." is different than the "." that we saw before. Or, rather, a "." within a map function refers to the next item in the list which is driving the map. -->

```{r}
x <- x %>% 
  mutate(mod = map(data, ~ lm(score ~ bty_avg, data = .)))

x
```

Note that the use of "." in this call to `map()` is somewhat different to the use of "." that we have seen before. In a normal pipe, a "." refers to the tibble which was passed down from the previous line. However, *within a map function*, a "." refers to an element in the .x object (the first argument) which the `map()` function is iterating over. 

In this case, `map()` is iterating over `data`, which is a column in the tibble `x`. `map()` goes through each item of `data`, passing each to the function which is its `.f` input, in this case an anonymous function built around `lm()`. So, each time `lm()` runs it is using a different row from the `data` variable. `map()` is iterating over `data`, passing each element in `data` as an argument to `lm()`.  

Now we have a new list column, `mod`, that contains the model objects created by `lm()`.  We will now want to `tidy()` the object created by `lm()`:

<!-- DK: Again, too fast! Need to look at mod. Show us the first model. Show us the second. They are different. Why? Is this what you expected? -->

<!-- EG: Agreed. This is too fast. Nest() and map() should both be discussed in more length- it is challenging to explain them effectively in few explanations/examples. -->

```{r}
x <- x %>% 
  mutate(reg_results = map(mod, ~ tidy(.)))

x
```

`tidy()` stores the coefficients in the `estimate` column, with each coefficient named in the `term` column.  Thus, if we `filter()` by `term` and `pull(estimate)`, we can get the regression coefficient for each bootstrap sample:

<!-- DK: Show us this! And talk about this anonymous function, built from two parts. -->

```{r}
x <- x %>%
  mutate(disp_coef = map_dbl(reg_results, ~ filter(., term == "bty_avg") %>% pull(estimate)))

x
```

Now that we have 1,000 estimates from our bootstrap samples, we can construct a percentile-based confidence interval easily:

```{r}
x %>%
 pull(disp_coef) %>%
 quantile(c(0.025, 0.5, 0.975))
```

Now let's return to our example for Joe.  Since our slope coefficient is measured with some uncertainty, our estimate of his potential outcome under the "treatment" of having a "beauty" score of 8 is also measured with some uncertainty, and that should be reflected in our table:

```{r, echo=FALSE}
# First, we create a tibble with the values we want for the table

tibble(`Subject` = "Joe",
       `$Y_{bty\\_avg = 7}$` = c("$4$"),
       `$Y_{bty\\_avg = 8}$` = c("$$4.067$ $(4.034,$ $4.099)$"),
       `$\\widehat{ATE}$` = c("$0.067$ $(0.034,$ $0.099)$")) %>%
  
  # Then, we use the kable function to make it pretty
  
  kable() %>%
  kable_styling(bootstrap_options = "striped",
                full_width = FALSE,
                position = "left") %>%
  column_spec(1,
              bold = TRUE,
              border_right = TRUE)
```

<!-- DK: Not bad. But, again, we want to discuss hypthesis tests (and p-values) each chapter, perhaps growing more sophisticated each time, perhaps ending with the ASA statement. -->

Instead of looking at confidence intervals, a common alternative approach is to conduct *hypothesis tests*, where one hypothesis is called the "null hypothesis" (in a regression context, generally that the regression coefficient is equal to zero) and the result of the test is either *rejecting* the null hypothesis (so you'd conclude that the regression coefficient probably is not zero) or *failing to reject* the null hypothesis.  The decision whether to reject the null hypothesis is generally made with reference to a *p-value*, a measure of how likely one would observe results at least as extreme as the results actually observed if the null hypothesis were true.  A *p*-value cutoff, often 0.05, is employed: if the *p*-value is lower, the null hypothesis is rejected, otherwise the hypothesis is not rejected.

We think this is a bad way to make decisions.  Two very similar datasets could produce *p*-values of $p = 0.04$ and $p = 0.06$ for a coefficient of interest. If you would make one decision in the former case and a totally different decision in the latter case, then there's something wrong with your decision-making process!  Rather, we think it is more sensible to look at the data, construct models to summarize important features of the data, and make decisions based on those models that take into account the uncertainty in the models' estimates.

### Using `lm()` and `tidy()` as a shortcut

While this process is relatively straightforward, there's a simple shortcut we can use.  Take a look at the results from `score_model %>% tidy(conf.int = TRUE)`:

```{r}
score_model %>% 
  tidy(conf.int = TRUE) %>%
  filter(term == "bty_avg") %>%
  select(estimate, conf.low, conf.high)
```

<!-- DK: Don't hard code numbers! -->

The confidence intervals reported by `lm()` and `tidy()` are very similar to our bootstrap confidence intervals.  Thus, we can interpret these confidence intervals the same way we did in Chapter \@ref(confidence-intervals). There is a 95% chance that the true value of the coefficient of `bty_avg` is between, roughly, 0.07 and 0.1.  From now on, we'll just use `lm()` because it is simpler, but when considering how to interpret the confidence intervals, remember that you'd obtain very similar results from the bootstrap method.

### Observed/fitted values and residuals {#model1points}

We just saw how to get the value of the intercept and the slope of a regression line from the `estimate` column of a regression table generated by the `tidy()` function. Now instead say we want information on individual observations. For example, let's focus on the 21st of the 463 courses in the `evals_ch11` data frame:

```{r, echo=FALSE}
index <- which(evals_ch11$bty_avg == 7.333 & evals_ch11$score == 4.9)
target_point <- score_model %>%
  augment() %>%
  slice(index)
x <- target_point$bty_avg
y <- target_point$score
y_hat <- target_point$.fitted
resid <- target_point$.resid
evals_ch11 %>%
  select(-age) %>% 
  slice(index) %>%
  knitr::kable(
    digits = 2,
    caption = "Data for the 21st course out of 463",
    booktabs = TRUE,
    linesep = ""
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("hold_position"))
```

What is the value $\widehat{y}$ on the regression line corresponding to this instructor's `bty_avg` "beauty" score of `r x`? We will mark three values corresponding to the instructor for this 21st course and give their statistical names:

* Circle: The *observed value* $y$ = `r y` is this course's instructor's actual teaching score.
* Square: The *fitted value* $\widehat{y}$ is the value on the regression line for $x$ = `bty_avg` = `r x`. This value is computed using the intercept and slope in the previous regression table: 

$$\widehat{y} = b_0 + b_1 \cdot x = `r evals_line[1]` + `r evals_line[2]` \cdot `r x` = `r y_hat`$$

* Arrow: The length of this arrow is the *residual* \index{regression!residual} and is computed by subtracting the fitted value $\widehat{y}$ from the observed value $y$. The residual can be thought of as a model's error or "lack of fit" for a particular observation.  In the case of this course's instructor, it is $y - \widehat{y}$ = `r y` - `r y_hat` = `r resid`.

```{r, echo=FALSE, warning=FALSE, fig.cap="Example of observed value, fitted value, and residual."}
best_fit_plot <- ggplot(evals_ch11, aes(x = bty_avg, y = score)) +
  geom_point(color = "grey") +
  labs(x = "Beauty Score", y = "Teaching Score",
       title = "Relationship of teaching and beauty scores") +
  geom_smooth(method = "lm", se = FALSE) +
  annotate("point", x = x, y = y_hat, col = "red", shape = 15, size = 4) +
  annotate("segment", x = x, xend = x, y = y, yend = y_hat, color = "blue",
           arrow = arrow(type = "closed", length = unit(0.04, "npc"))) +
  annotate("point", x = x, y = y, col = "red", size = 4)

best_fit_plot
```

Now say we want to compute both the fitted value $\widehat{y} = b_0 + b_1 \cdot x$ and the residual $y - \widehat{y}$ for *all* 463 courses in the study. Recall that each course corresponds to one of the 463 rows in the `evals_ch11` data frame and also one of the 463 points in the regression plot.

We could repeat the previous calculations we performed by hand 463 times, but that would be tedious and time consuming. Instead, let's do this using a computer with the `augment()` function in the **broom** package. Let's apply the `augment()` function to `score_model`, which is where we saved our `lm()` model in the previous section. We present the results of only the 21st through 24th courses for brevity's sake.

```{r, eval=FALSE}
regression_points <- score_model %>%
  augment() %>%
  select(score, bty_avg, .fitted, .resid)
regression_points
```

```{r, echo=FALSE}
set.seed(76)
regression_points <- augment(score_model) %>%
  select(score, bty_avg, .fitted, .resid)
regression_points %>%
  slice(c(index, index + 1, index + 2, index + 3)) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (for only the 21st through 24th courses)",
    booktabs = TRUE,
    linesep = ""
  )
```

Let's inspect the individual columns and match them with the elements of our regression plot:

* The `score` column represents the observed outcome variable $y$. This is the y-position of the `r nrow(regression_points)` black points.
* The `bty_avg` column represents the values of the explanatory variable $x$. This is the x-position of the `r nrow(regression_points)` black points.
* The `.fitted` column represents the fitted values $\widehat{y}$. This is the corresponding value on the regression line for the `r nrow(regression_points)` $x$ values.
* The `.resid` column represents the residuals $y - \widehat{y}$. This is the `r nrow(regression_points)` vertical distances between the `r nrow(regression_points)` black points and the regression line.

Just as we did for the instructor of the 21st course in the `evals_ch11` dataset (in the first row of the table), let's repeat the calculations for the instructor of the 24th course (in the fourth row):

* `score` = 4.4 is the observed teaching `score` $y$ for this course's instructor.
* `bty_avg` = 5.50 is the value of the explanatory variable `bty_avg` $x$ for this course's instructor.
* `.fitted` = 4.25 = `r evals_line[1]` + `r evals_line[2]` $\cdot$ 5.50 is the fitted value $\widehat{y}$ on the regression line for this course's instructor.
* `.resid` = 0.153 =  4.4 - 4.25 is the value of the residual for this instructor. In other words, the model's fitted value was off by 0.153 teaching score units for this course's instructor.

At this point, you can skip ahead if you like to Section \@ref(leastsquares) to learn about the processes behind what makes "best-fitting" regression lines. As a primer, a "best-fitting" line refers to the line that minimizes the *sum of squared residuals* out of all possible lines we can draw through the points. In Section \@ref(model2), we'll discuss another common scenario of having a categorical explanatory variable and a numerical outcome variable.

Constructing a measure of uncertainty around the fitted values can also be done using the `augment()` function.  The simplest way to do this is through the standard error method that we learned in the last chapter.  Note that `augment()` saves the standard errors of the fitted values in a column called `.se.fit`:

```{r}
score_model %>%
  augment() %>%
  select(score, bty_avg, .fitted, .se.fit, .resid)
```

Now that we have the standard error of the fitted values, we can construct a confidence interval easily:

```{r}
score_model %>%
  augment() %>%
  mutate(conf.low = .fitted - 2 * .se.fit,
         conf.high = .fitted + 2 * .se.fit) %>%
  select(score, bty_avg, .fitted, conf.low, conf.high, .resid)
```

Note that the uncertainty for *particular predictions* is higher than the uncertainty for our estimate of the coefficient on `bty_avg`.  We are more confident in the *average* effect of "beauty" scores on teaching evaluations than we are on any predictions we would make for a *particular person*.

<!-- AR: We can't really bring Joe back in here, because we are filling in the
potential outcome table with estimates of the treatment effect + Joe's observed
outcome, not with the predicted value for a particular bty_avg.-->

## Case study: 2018 gubernatorial forecasts

<!-- AR: beginning of working with list columns -->

<!-- EG: Will practice using augment with lm() to make predictions. Additionally, will add a discussion about Gelman's concern about the assumption of beta being perfectly estimated, but how this is likely an acceptable assumption to make. -->

Now that we know how to run simple linear regressions, let's run many at once!  

We'll use the `governor_state_forecast` data from the **fivethirtyeight** package.  This dataset has the day-by-day forecasts FiveThirtyEight published in the 2018 gubernatorial races from October 11 to November 6.  Which Republican candidate saw his or her probability of winning increase the most during that time?  We can get a sense of this by running a series of linear models, with the outcome variable being FiveThirtyEight's predicted probability of the Republican candidate winning and the explanatory variable being the number of days since the forecast began.

Let's first load the `governor_state_forecast` data and get it in the form we'll need:

```{r}
library(fivethirtyeight)

gov <- governor_state_forecast %>%
  filter(model == "classic",
         party == "R") %>%
  select(forecastdate, state, candidate, party, win_probability) %>%
  mutate(days = as.numeric(forecastdate) - min(as.numeric(forecastdate)),
         statecand = paste(candidate, " (", state, ")", sep = ""))

glimpse(gov)
```

We `filter`ed to Republicans as well as to `model == "classic"` because FiveThirtyEight presented three versions of their model and we want to limit our analysis to one.  We also `mutate`d to create a variable `days` that is the number of days since the forecast began (October 11) and `statecand` which combines the name of the Republican candidate with the state abbreviation, which will be useful for plotting.

### Fitting multiple models using `map()`

Next, we need to use `nest()` like we did when creating bootstrapped confidence intervals, except this time we are `nest`ing the data by `statecand`:

```{r}
gov <- gov %>%
  group_by(statecand) %>%
  nest()
```

Now each observation in our dataset is a candidate and we have a list column `data` that consists of the rest of the data for each candidate.  We can use `map_*` functions just as in the bootstrapping example to get the coefficient on `days` for each candidate:

```{r}
gov <- gov %>%
  mutate(mod = map(data, ~ lm(win_probability ~ days, data = .)),
         reg_results = map(mod, ~ tidy(.)),
         disp_coef = map_dbl(reg_results, ~ filter(., term == "days") %>% pull(estimate)))

glimpse(gov)
```

We now have the data to answer the question about which candidate saw his or her estimated probability go up the most.  Let's use the `slice()` function in the **dplyr** package to answer this.  Note that because we have grouped data, we'll have to `ungroup()` before we can `slice()`:

```{r}
gov_top5 <- gov %>%
  ungroup() %>%
  arrange(desc(disp_coef)) %>%
  slice(1:5)

gov_top5 %>%
  select(statecand, disp_coef)
```

We can show so much more, however, if we presented the results graphically.  To do this, we'll need to learn the companion function to `nest()`: `unnest()`.  While `nest()` collapsed the data so that each observation was a `statecand`, `unnest(data)` will return the data to its original unit of analysis, with each row one day's forecast for a particular candidate:

```{r}
gov_top5 %>%
  unnest(data) %>%
  glimpse()
```

Note that `unnest`ing flattens out the `data` list column but keeps everything else we've created.  With the data in this form, it's easy to use `ggplot()` to create a scatterplot for each of the five candidates with the greatest `disp_coef`:

```{r}
gov_top5 %>%
  unnest(data) %>%
  ggplot(aes(x = days, y = win_probability)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~ statecand) +
  labs(x = "Days since forecast began",
       y = "FiveThirtyEight's win probability",
       caption = "Data source: FiveThirtyEight") +
  theme_classic()
```

We could easily replicate this process if we wanted to find the five candidates who saw their fortunes decline the most:

```{r}
gov %>%
  ungroup() %>%
  arrange(disp_coef) %>%
  slice(1:5) %>%
  unnest(data) %>%
  ggplot(aes(x = days, y = win_probability)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~ statecand) +
  labs(x = "Days since forecast began",
       y = "FiveThirtyEight's win probability",
       caption = "Data source: FiveThirtyEight") +
  theme_classic()
```


## Best-fitting line {#leastsquares}

Regression lines are also known as "best-fitting" lines. But what do we mean by "best"? Let's unpack the criteria that is used in regression to determine "best." Recall the plot where for an instructor with a beauty score of $x = 7.333$ we mark the *observed value* $y$ with a circle, the *fitted value* $\widehat{y}$ with a square, and the *residual* $y - \widehat{y}$ with an arrow. We re-display that plot in the top-left plot of the next figure in addition to three more arbitrarily chosen course instructors:

```{r, echo=FALSE, warning=FALSE, fig.cap="Example of observed value, fitted value, and residual."}
# First residual

best_fit_plot <- ggplot(evals_ch11, aes(x = bty_avg, y = score)) +
  geom_point(size = 0.8, color = "grey") +
  labs(x = "Beauty Score", y = "Teaching Score") +
  geom_smooth(method = "lm", se = FALSE) +
  annotate("point", x = x, y = y, col = "red", size = 2) +
  annotate("point", x = x, y = y_hat, col = "red", shape = 15, size = 2) +
  annotate("segment", x = x, xend = x, y = y, yend = y_hat, color = "blue",
           arrow = arrow(type = "closed", length = unit(0.02, "npc")))

p1 <- best_fit_plot + labs(title = "First instructor's residual")

# Second residual

index <- which(evals_ch11$bty_avg == 2.333 & evals_ch11$score == 2.7)
target_point <- augment(score_model) %>%
  slice(index)
x <- target_point$bty_avg
y <- target_point$score
y_hat <- target_point$.fitted
resid <- target_point$.resid

best_fit_plot <- best_fit_plot +
  annotate("point", x = x, y = y, col = "red", size = 2) +
  annotate("point", x = x, y = y_hat, col = "red", shape = 15, size = 2) +
  annotate("segment", x = x, xend = x, y = y, yend = y_hat, color = "blue",
           arrow = arrow(type = "closed", length = unit(0.02, "npc")))

p2 <- best_fit_plot + labs(title = "Adding second instructor's residual")

# Third residual

index <- which(evals_ch11$bty_avg == 3.667 & evals_ch11$score == 4.4)
score_model <- lm(score ~ bty_avg, data = evals_ch11)
target_point <- augment(score_model) %>%
  slice(index)
x <- target_point$bty_avg
y <- target_point$score
y_hat <- target_point$.fitted
resid <- target_point$.resid

best_fit_plot <- best_fit_plot +
  annotate("point", x = x, y = y, col = "red", size = 2) +
  annotate("point", x = x, y = y_hat, col = "red", shape = 15, size = 2) +
  annotate("segment", x = x, xend = x, y = y, yend = y_hat,
           color = "blue",
           arrow = arrow(type = "closed", length = unit(0.02, "npc")))
p3 <- best_fit_plot + labs(title = "Adding third instructor's residual")

index <- which(evals_ch11$bty_avg == 6 & evals_ch11$score == 3.8)
score_model <- lm(score ~ bty_avg, data = evals_ch11)
target_point <- augment(score_model) %>%
  slice(index)
x <- target_point$bty_avg
y <- target_point$score
y_hat <- target_point$.fitted
resid <- target_point$.resid

best_fit_plot <- best_fit_plot +
  annotate("point", x = x, y = y, col = "red", size = 2) +
  annotate("point", x = x, y = y_hat, col = "red", shape = 15, size = 2) +
  annotate("segment", x = x, xend = x, y = y, yend = y_hat, color = "blue",
           arrow = arrow(type = "closed", length = unit(0.02, "npc")))

p4 <- best_fit_plot + labs(title = "Adding fourth instructor's residual")

p1 + p2 + p3 + p4 + plot_layout(nrow = 2)
```

The three other plots refer to:

1. A course whose instructor had a "beauty" score $x$ = 2.333 and teaching score $y$ = 2.7. The residual in this case is $2.7 - 4.036 = -1.336$, which we mark with a new `r if_else(!knitr::is_latex_output(), "blue", "")` arrow in the top-right plot.
1. A course whose instructor had a "beauty" score $x = 3.667$ and teaching score $y = 4.4$. The residual in this case is $4.4 - 4.125 = 0.2753$, which we mark with a new `r if_else(!knitr::is_latex_output(), "blue", "")` arrow in the bottom-left plot.
1. A course whose instructor had a "beauty" score $x = 6$ and teaching score $y = 3.8$. The residual in this case is $3.8 - 4.28 = -0.4802$, which we mark with a new `r if_else(!knitr::is_latex_output(), "blue", "")` arrow in the bottom-right plot.


Now say we repeated this process of computing residuals for all 463 courses' instructors, then we squared all the residuals, and then we summed them. We call this quantity the *sum of squared residuals*\index{sum of squared residuals}; it is a measure of the _lack of fit_ of a model. Larger values of the sum of squared residuals indicate a bigger lack of fit. This corresponds to a worse fitting model.

If the regression line fits all the points perfectly, then the sum of squared residuals is 0. This is because if the regression line fits all the points perfectly, then the fitted value $\widehat{y}$ equals the observed value $y$ in all cases, and hence the residual $y-\widehat{y}$ = 0 in all cases, and the sum of even a large number of 0's is still 0. 

Furthermore, of all possible lines we can draw through the cloud of 463 points, the regression line minimizes this value. In other words, the regression and its corresponding fitted values $\widehat{y}$ minimizes the sum of the squared residuals:

$$
\sum_{i=1}^{n}(y_i - \widehat{y}_i)^2
$$

Let's use our data wrangling tools from Chapter \@ref(tidyverse) to compute the sum of squared residuals exactly:

```{r}
# Fit regression model:

score_model <- lm(score ~ bty_avg, 
                  data = evals_ch11)

# Get regression points:

regression_points <- score_model %>%
  augment()
regression_points

# Compute sum of squared residuals

regression_points %>%
  mutate(squared_residuals = .resid^2) %>%
  summarize(sum_of_squared_residuals = sum(squared_residuals))
```

Any other straight line drawn in the figure would yield a sum of squared residuals greater than 132. This is a mathematically guaranteed fact that you can prove using calculus and linear algebra. That's why alternative names for the linear regression line are the *best-fitting line* and the *least-squares line*. Why do we square the residuals (i.e., the arrow lengths)? So that both positive and negative deviations of the same amount are treated equally.

<!-- EG: Honestly, I'm not sure that this section on Bayesian regression belongs in this chapter. Jumping so quickly from introducting simple lm() modeling to Bayesian interpretations could be confusing, especially when so many new concepts and functions are being introduced. I personally think that this ought to be its own chapter, with in-depth exploration of examples and plenty of emphasis on how to properly interpret the meaning of EVERY part of a Bayesian model. If we add all that into this chapter, it would get very long. -->

<!-- not sure yet how to best order these sections  -->

## Advanced: Bayesian Regression 
## Introduction to rstanarm

In this chapter, you have learned how to use `lm()` to model the relationship between outcome and explanatory variables using *simple linear regression*. Furthermore, you learned how to interpret regresssion in a bayesian manner. Recall that instead of bootstrapping, we used `lm()` and `tidy()` to produce similar confidence intervals.

<!-- fit model in lm() intepret in bayesian way as refresher  -->

```{r}
lm_score_model <- lm(score ~ bty_avg, data = evals_ch11)

lm_score_model %>% 
  tidy(conf.int = TRUE) %>%
  filter(term == "bty_avg") %>%
  select(estimate, conf.low, conf.high)
```

From what we learned in Chapter 7, we intepreted this confidence intervals as a 95% chance that the true value of the coefficient of `bty_avg` is between, roughly, 0.03 and 0.1. Thus, we have taken our simple `lm()` model and interpreted it in a Bayesian way. However, is this a full fledge bayesian analysis? No! For example, `lm()` does not allow the user to incorporate priors. As mentioned before, `lm()` is an old function and to perform bayesian analysis we must use a Bayesian analysis tool. 

<!-- EG: Do students know what Bayesian regression is? How it differs fundamentally from frequentist modeling? Why we might consider using a Bayesian model instead of a frequentist model? If that hasn't already been introduced somewhere else in the book, I think it should happen here. -->

The tool we need to perform Bayesian Regression is the `rstanarm` package. `rstanarm` is an interface to connect with the Stan probabilistic programming language. <!-- EG: should we have some explanation of what the Stan probabilistic programming language is?-->You will see the `rstanarm` package is created to mirror functions like `lm()` and it simply requires adding a `stan_` prefix before common functions like `lm()`. We will focus on `stan_glm()` for the remainder of this section. 


## Bayesian Regression with a Continuous Variable

Let us take another look at the the data on the 463 courses at UT Austin, which can be found in the `evals` data frame included in the **moderndive** package.  We will once again model the relationship between teaching scores and “beauty” to help us highlight the similarities and differences between `lm()` and `stan_glm()`. 

We can obtain the values of the intercept $b_0$ and the slope for `btg_avg` $b_1$ in two steps:

1. "Fit" the Bayesian regression model using `stan_glm()` function and save it in `bayes_score`. 
1. Applying the `print()` function and indicating how many siginificant digits to include.


```{r}

library(rstanarm)

bayes_score <- stan_glm(score ~ bty_avg, data = evals_ch11, refresh = 0)

print(bayes_score, digits = 2)


```

First, we "fit" the bayesian regression model to the `data` using the `stan_glm()` \index{lm()} function and saved this as `bayes_score`. Notice `stan_glm()` is used just as `lm()` was: `stan_glm(y ~ x, data = data_frame_name)` where:

* `y` is the outcome variable, followed by a tilde `~`.
* `x` is the explanatory variable.
* The model formula is `score ~ bty_avg`.
* `data_frame_name` is the name of the data frame that contains the variables `y` and `x`. In our case, `data_frame_name` is the `evals_ch11` data frame.
* `refresh = 0` is optional. Setting refresh equal to 0 suppresses the printing of the sampling algorithm from the model. We will touch more on this later. 

### Interpreting Regression Coefficients 

Notice, in the `print()` output, the estimates are referred to as the `Median` and the uncertainty as the `MAD_SD`. This is due to the fact that Bayesian Regression does not provide just a point estimate, but a distribution. The `Median` is the chosen summary statistic for the distribution because median-based-summaries are more stable over simulations. However, even though the estimates and uncertainty of the parameters are calculated differently, the coefficients of the `stan_glm()` model can be intepretted in the same manner as they were previously when using `lm()`. 

The intercept $b_0$ = `r bayes_score$coefficient[1]` is the average teaching score $\widehat{y}$ = $\widehat{\text{score}}$ for those courses where the instructor had a "beauty" score `bty_avg` of 0. Again, while the \index{regression!equation of a line!intercept} intercept of the regression line has a mathematical interpretation, it has no *practical* interpretation here, since observing a `bty_avg` of 0 is impossible.

<!-- DK: Add discussion of "hat". Recall the p hat which we estimated last time. y hat is like that! It is not a variable that we can observe. It is an estimate. (Indeed, it is an estimate of a potential outcome!) This is different form x, which has no hat, because it is real data, something we can see. Side note: not sure if the hat versus no hat distinction works well with b_0/b_1 being things we can't see and need to estimate, but for which we do not use hat notation. -->

The slope $b_1$ = $b_{\text{bty_avg}}$ of `r bayes_score$coefficient[2]`. The "bty_avg" subscript indicates that this number summarizes the relationship between the teaching and average beauty score variables. The slope's interpretation is different:

<!-- DK: Let's make sure that these interpretations are highly consistent with Gelman and across the book. -->

<!-- EG: To make sure these interpretations are consistent, who should I talk to/what sections should I look at? -->

> For every increase of 1 unit in `bty_avg`, there is an *associated* increase of, *on average*, `r bayes_score$coefficient[2]` units of `score`.

What the slope of `r bayes_score$coefficient[2]` is saying is that across all possible courses, the *average* difference in teaching score between two instructors whose "beauty" scores differ by one is `r bayes_score$coefficient[2]` when holding all other things equal.

In the output, we are also given another parameter, $sigma$ =  `r round(sigma(bayes_score),2)`. $sigma$ represents the residual standard error, the deviation between the predicted value and the observed value. Thus, it shows us the uncertainity when our model predicts the `score` using `bty_avg`. Our $sigma$ 
tells us that a teacher's score will be between plus or minus `r round(sigma(bayes_score),2)` the prediction for 68% of the data and between plus or minus two sigma, `r 2*round(sigma(bayes_score),2)`, of the prediction for about 95% of the data. Here is a visual representation:

```{r, echo=FALSE}


evals_ch11 %>%
  add_predicted_draws(bayes_score) %>%
  ggplot(aes(x = bty_avg, y = score)) +
  stat_lineribbon(aes(y = .prediction), .width = c(.99, .95, .68), color = "#08519C") +
  geom_point(data = evals_ch11, size = 2) +
  scale_fill_brewer() + 
  labs(title = "68% of Data Between +-1 Sigma from Line of Best Fit\n 95% of Data Between +-2 Sigma from Line of Best Fit")
```


```{r, echo=FALSE}
intervals<-posterior_interval(bayes_score, prob = 0.95)

```

Finally, for all of the coefficients, we are provided a `MAD_SD`. The `MAD SD`, which equals $1.483 * (\mathbf{median}^n_{i=1} |z_i - M|)$, summarizes the uncertainty in the model parameters. Since we are used to using the standard deviations to measure variation, the MAD SD is rescaled to mirror the standard error of the normal distribution. The `MAD_SD` can be used to retrieve our coefficient's credible intervals for each parameter. Thus, for `bty_avg`, the 95% credible interval is (`r intervals[2,]`). This tells us that there is a 95% probability that the true value of the parameter for `bty_avg` falls within the interval. 

<!-- EG: Should we include an explanation of why the language of posteriors is so important/so heavily used within Bayesian modeling? -->

Instead of trying to calcualate these by hand, the easiest way to retrieve our model's credible intervals is using the function `posterior_interval()` on our model object `bayes_score`.

```{r}
posterior_interval(bayes_score)

```

The default for the function is a 90% interval, but that can be changed by adding the `prob` input. Thus, we can replicate the 95% intervals that we have worked with in the past:

```{r}
posterior_interval(bayes_score, prob = 0.95)

```

Now, let us call `summary()` on our `bayes_score` model. Using the `summary()` is an alternative to `print` and it provides more information about our `stan_glm()` model. 

```{r}
summary(bayes_score)
```

In the Estimates section, we get the same information as we did from `print()`. However, there is a lot of new information that we have not seen before under Fit Diagnostics and MCMC Diagnostics. We can use this information provided to further assess the model. 

In the Fit Diagnostics section, we are provided information about the `mean_ppd`, the sample mean of the posterior predictive distribution of the outcome variable. A quick check of our model is that we hope the `mean_ppd` is close to the mean of our depedent variable. The mean of the `score` variable is `r mean(evals_ch11$score)`. Thus, the `mean_ppd` is definitely on par with the mean of the `bayes_score` dependent variable. 

In MCMC diagnostics, we are provided with:

* `log-posterior`, which is the log of the combined posterior distributions. 
* `mcse`, which stands for the Monte Carlo standard error. Markov Chain Monte Carlo is the algorithm used to draw from the posterior distribution.
* `Rhat`, which indicates whether or not the model converges.
* `n_eff`, a measure of the effective sample size.

The only thing that you need to be concerned about is the `Rhat`. When we know whether or not the model converges, we know whether or not the results are reliable. We hope to get values for `Rhat` as close to 1 as possible. Rhat values less than 1.1 indicate model convergence and that the model is reliable. Thus, since all the `Rhat` for our model are 1.0, `bayes_score` is reliable.

### Uncertainty in Bayesian Inference
You may be wondering, what exactly is `stan_glm()` doing? Let us take a look under the hood of `stan_glm()`. This will enable us to further understand the usefulness of Bayesian Regression. Remember that we included `refresh = 0` in the `bayes_score` model. Let us run the same model, but leave out the `refresh` input.

```{r}
stan_glm(score ~ bty_avg, data = evals_ch11)
```

Without `refresh`, the output displays everything `stan_glm()` is doing in the background, which is a sampling algorithm known as Markov Chain Monte Carlo (MCMC). In fact, `stan_glm()` is producing thousands of simulations after sampling from the posterior distribution using this MCMC algorithm. We can conveniently access all of the simulations of the the model parameters (intercept, slope, and sigma) from the posterior distribution as a matrix. Let's take a look at some of these simulations.

```{r}

sims <- as.matrix(bayes_score)


head(sims)
nrow(sims)
``` 

As you can see, each row or iteration has a slightly different value for our `bayes_score` model's three parameters. Each combination of possible values of the parameters was used to try to fit the data. Also, looking at the dimensions of the matrix, there are 4000 rows. Thus, `stan_glm()` tried `4000` different combinations of possible values of the parameters to try to model the data. 

Now, you may be wondering what is going on because there are `4000` simulations, but when we print the model, we get a single value for each parameter. Recall that what `lm()` refers to as the estimate, when we printed our `bayes_score` model, the column was headed as `Median`. Let's calculate the median value for each column of our `sims` matrix. We can do this using the `apply()` function, which takes form: `apply(Object, Margin, Function)`. We will be applying the `median` function to the `sims` matrix and the `Margin` will be 2, which indicates columns instead of rows.

```{r}
apply(sims, 2, median)
```

These are the exact values of the coefficients of the printed `bayes_score` model. Thus, `stan_glm()` tried 4000 iterations to model the data. All `4000` were summarized using the `Median` to produce point estimates of the parameters.

Now, we understand where the point-estimate comes from when we call `print()` on a `stan_glm()` model; however, a simple linear regression using `lm()` provides us with a point-estimate, or in other words,  a single line of best fit. The power of Bayesian Regression comes from the ability to analyze uncertainty using the entire posterior distribution, all 4000 simulations cumulatively. Again, the posterior distribution is a set of plausible values for each parameter and each observation or row is referred to as a posterior draw. Let us take a look at the posterior distrubtion for the `bty_avg` variable, the slope, by graphing it. Once again we can use the `sims` matrix, but we must convert it into a tibble using `as_tibble()` to graph. 


```{r, message= FALSE, echo=FALSE}
library(bayesplot)
library(tidybayes)
library(cowplot)
```

```{r}



posterior_draws<-sims%>%as_tibble()


posterior_draws%>%
  ggplot(aes(x = bty_avg)) +
  geom_histogram()+
   geom_vline(xintercept=median(posterior_draws$bty_avg), color="red", size=1) + labs(y = "Frequency", title = "Posterior Distribution of bty_avg Parameter", subtitle = "The Red Line Represents the Median")
```

If you noticed, `stan_glm()` does not have any p-values, t-values or degrees of freedom like `lm()`. The crux of Bayesian Modeling is that everything we need to know can be found within the posterior distribution. We can get a point estimate using the `Median` and we saw in the previous section that we can get a 95% credible interval for `bty_avg` using the `posterior_interval` function. We can also graph the posterior distribution of all the other parameters from the model, the intercept and sigma. 

```{r echo=FALSE}

bty_avg<-posterior_draws%>%
  ggplot(aes(x = bty_avg)) +
  geom_histogram()+
   geom_vline(xintercept=median(posterior_draws$bty_avg), color="red", size=1) + labs(y = "Frequency", title = "Posterior Distribution of bty_avg Parameter", subtitle = "The Red Line Represents the Median")

sigma<-posterior_draws%>%
  ggplot(aes(x = sigma)) +
  geom_histogram()+
   geom_vline(xintercept=median(posterior_draws$sigma), color="red", size=1)

intercept<-posterior_draws%>%
  ggplot(aes(x = `(Intercept)`)) +
  geom_histogram()+
   geom_vline(xintercept=median(posterior_draws$`(Intercept)`), color="red", size=1)


cowplot::plot_grid(intercept, bty_avg, sigma, nrow = 1)



    

```

Another way to think of each posterior draw, each row of the `sims` matrix, is that each creates a regression line. This is something that may get lost in translation with the posterior distrubution and the 4000 simulations; however, the goal of `stan_glm()` is still to fit lines to data. 

```{r}

medians<-tibble(`(Intercept)` = median(posterior_draws$`(Intercept)`), bty_avg = median(posterior_draws$bty_avg))

posterior_draws%>%
  ggplot(aes(x = `(Intercept)`, y = bty_avg)) +
  geom_point() + geom_point(data = medians, color = "red", size = 5)+ 
  labs(title = "Every Combination of Intercept and bty_avg From Posterior Distribution", x = "Intercept", subtitle = "Red Dot Represents the Median for Each Parameter")

```
Each intercept and slope (bty_avg) combination will create a different line. Furthermore, we can view all of these various lines. In doing so, we can again express and understand uncertainty.

We will take our first look at the **tidybayes** package. The purpose of the **tidybayes** package is to aid in formating Bayesian modeling outputs in a tidy manner. It also provides **ggplot** geoms to easily plot Bayesian Models.

Now back to the matter at hand of the various posterior draws and the lines they create. If you recall the `augment()` function we used for `lm()`, we were able to retrieve the fitted values yˆ. The `add_fitted_draws()` function from the **tidybayes** package is very similar, but for `stan_glm()`. The `add_fitted_draws()` takes a `stan_glm()` object, such as `bayes_score`, and a `n` parameter, which stands for the number of posterior draws to calculate the fitted values for. Let us try `n=5`.

```{r}
five_draws<-evals_ch11%>%
  add_fitted_draws(bayes_score, n = 5)

nrow(five_draws)

```
If you recall, the `evals_ch11` dataset has 463 rows. `2315/5 = 463`. Thus, we can see that `add_fitted_draws()` calculated the fitted values 5 times. Let us see what `five_draws` looks like.

```{r}
head(five_draws, n = 10)
```

`add_fitted_draws`reports:

* `ID` and `.row` which correspond to the row or specific teacher from the `evals_ch11` dataset.
* `score`, `bty_avg` and `age` are the observed values for a given teacher. 
* `.draw` is the randomly selected posterior draw used to fit the data. The number corresponds to the row from the `sims` matrix used. 
* `.value` is the fitted value. In this case, it represents the prediction for the `score` based on a teacher's `bty_avg` for the specific parameters from the `.draw`. 

We are able to plot these different draws.

```{r}

five_draws%>%
  ggplot(aes(x = bty_avg, y = score)) +
  geom_line(aes(y = .value, group = .draw), color = "blue") +
  geom_point()


```

Now, you can see how each posterior draw creates a different line. With just these 5 draws, there is a lot of variability. Let us see what `n=100` looks like.

```{r}
evals_ch11%>%
  add_fitted_draws(bayes_score, n = 100)%>%
  ggplot(aes(x = bty_avg, y = score)) +
  geom_line(aes(y = .value, group = .draw), alpha = 0.6, color = "blue") +
  geom_point()
  #transition_states(.draw, 0, 1)+
  #shadow_mark(past = TRUE, future = TRUE, alpha = 1/20, color = "gray50")
```
We see a the lines become more concentrated. The power of Bayesian Modeling comes from the shear number of draws. Now remember, `stan_glm()` by default produces 4000 simulations and we have only plotted 100 of them. The more draws there are, the better your estimation of the posterior distribution and the better we are able to assess the uncertainty in our parameters. 


###Predictions

Finally, the last step of Bayesian Inference is being able to make prediction about new data using our `stan_glm()` model. Let us create a new dataset of teacher's beauty scores that mirrors the range of values in the `eval_ch11` dataset. To do this, we will create a tibble of `bty_avg` and use `seq` to generate a row for each `bty_avg` from 1.5 to 8.5 by 0.5. 

```{r}

new <- tibble(bty_avg = seq(1.5, 8.5, 0.5))

new
```

Now, we will start with the simplest prediction, the point prediction. These are based on the fitted model or as we have come to know, the median value for each parameter (intercept, bty_avg, sigma). We will use the `predict` function to find predicted scores for each beauty rating from our `new` dataset. 
```{r}

new%>%
  mutate(y_hat = predict(bayes_score, newdata = .))%>%
  ggplot(aes(x = bty_avg, y = y_hat)) + geom_point() + geom_line()


```

Great! We have predicted values for each  `bty_avg` from the `new` dataset and they follow line created by the fitted model. However, this is nothing special to `stan_glm()`. We can do the exact same thing with an `lm()` model. Recall the `score_model` we created at the beginning of Chapter 10. It is the same as the `bayes_score` model, but it uses `lm()` instead of `stan_glm()`. Here are the point predictions for the `score_model` at each value of `bty_avg`in the `new` dataset.

```{r}
new%>%
  mutate(y_hat = predict(score_model, newdata = .))%>%
  ggplot(aes(x = bty_avg, y = y_hat)) + geom_point() + geom_line()
```

We get the exact same thing. This is because a point prediction ignores uncertainty. As we have been exploring in this section, where Bayesian Regression differs from Simple Linear Regression is in it's ability to express uncertainty in not only the model, but also it's parameters. Remember we had a `sims` matrix of 4000 rows of parameters. The posterior distributions serves us better and gives us a lot more information than a single point estimate ever could. 

Thus, our predicitions should also take uncertainty into account. The first type is linear predictors. We perform this type of prediction using the `posterior_linepred` function. `posterior_linepred` takes a stan model like our `bayes_score` and a dataset of new points, `new`. With the function, we will be able to represent the distribution of uncertainty in regards to the parameters at each `bty_avg` from the `new` dataset. Let us take a look.

```{r}
linepred<-posterior_linpred(bayes_score, newdata = new)



head(linepred)
```

`posterior_linepred` returns a matrix of posterior simulations wher each column of the `linepred` corresponds to each row of the `new` dataset. If we were to take the `mean` of each column, we would get the point prediction corresponding to that `bty_avg`. Also, if we were to take the `sd`, you would get the uncertainty in the fitted model. Thus `posterior_linepred` mimics what we were doing the previous section with the matrix of simulated parameters. In fact, we plot `posterior_linepred` using the same `add_fitted_draws` function without specifying n to factor in all 4000 posterior draws. `added_fitted_draws` from the **tidybayes** package puts the data in a much easier format than the matrix `posterior_linepred` outputs. First, we will recreate the point prediction plot using `add_fitted_draws` and use `mutate` to get the mean prediction for each `bty_avg`. 

```{r}

new %>%
  add_fitted_draws(model = bayes_score, .)%>%
  group_by(bty_avg)%>%
  mutate(point_estimate = mean(.value))%>%
  ggplot(aes(x = bty_avg, y = point_estimate)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(1.5,8.5,0.5)) +
  labs(y = "Predicted Scores", color = "Uncertainty Level", title = "Posterior Linear Prediction Without Uncertainty")


    


```

Now, we will add to this plot using the `stat_interval` geom from the **tidybayes** package to add uncertainty to the visualization.

```{r}
new %>%
  add_fitted_draws(model = bayes_score, .)%>%
  group_by(bty_avg)%>%
  mutate(point_estimate = mean(.value))%>%
  ggplot(aes(x = bty_avg, y = point_estimate)) +
  stat_interval(aes(x = bty_avg, y = .value), .width = c(0.68, 0.95)) +
  geom_point(aes(x = bty_avg, y = point_estimate)) +
  geom_line() +
  scale_x_continuous(breaks = seq(1.5,8.5,0.5))+
  labs(y = "Predicted Scores", color = "Uncertainty Level", title = "Posterior Linear Prediction With Uncertainty")
```

Linear Predictions can provide us the point estimate predictions while also telling us about the uncertainty in the parameters. As you can see, the intervals get a lot smaller towards the center of the `bty_avg` variable. This is because more data is concentrated towards the center. Furthermore, the predictions are more accurate/less varied because the posterior distribution has more data to draw from. Towards the extremes, we see the bands are a lot wider. Thus, the point predictions are a lot less reliable because the posterior distribution does not have a lot of data from the original `evals_ch11` to be built off of. 

Finally, the last type of prediction we will cover is the predictive distribution. Whereas linear predictions focused on the uncertainty in the parameters coefficients, the predictive distribution represents uncertainty surrounding the predicted value. In other words, it "is the distribution of the outcome implied by the model after using the observed data to update our beliefs about the unknown parameters in the model." To perform this, we will use the `posterior_predict()` function. It works in the same way as `posterior_linpred`. 

```{r}

pred<-posterior_predict(bayes_score, newdata = new)
  
head(pred)

```

Again, in the matrix that is returned, each column corresponds to one new value of  `bty_avg` from the `new` dataset. We can take a quick look at the predictive distribution for one of the `bty_avg` using the `hist()` function. 
```{r}
hist(pred[,1])
```

However, **tidybayes** also has a function to make our lives easier for graphing the predictive distributions where we can compare all the distributions cumulatively in one plot. First, we have the `add_predicted_draws()` function which does the same thing as `posterior_predict()`, but `add_predicted_draws()` puts the data in a tidy format. 


```{r}

new %>%
  add_predicted_draws(., model = bayes_score)%>%
  ggplot(aes(x = .prediction, y = bty_avg)) +
  stat_halfeyeh() + 
  scale_y_continuous(breaks = seq(1.5,8.5,0.5)) +
  labs(x = "Predicted Score", title = "Predictive Distribution for Score at Each Value of bty_avg")
```

With `stat_halfh()` we are able to view density plots for all the predictions from the `new` dataset together. Thus, we can compare all the predictive distributions. As you can see, there is a lot of overlap. This signifies there is a lot of uncertainty. Even though the  `bty_avg` has a slightly postive slope, `bty_avg` does not seem to be very predictive when it comes to the teacher's `score`.


## Bayesian Regression with Categorical Variable 

In the previous section we focused on Bayesian Regression with one continuous variable. We will now take a look at a model wtih one categorical variable. We will follow many of the same steps as we did in the previous section, thus we will go through the steps of Bayesian Inference quicker in the section while highlighting the differences when regressing on a categorical variable. We will use the same data from the `gaminder` package on the 142 countries from 2007. 

```{r}
library(gapminder)

gapminder2007 <- gapminder %>%
  filter(year == 2007) %>%
  select(country, lifeExp, continent, gdpPercap)

head(gapminder2007)
```

As we did before, we will study the relationship between continents and life expectancy. We will “fit” the bayesian regression using the `stan_glm(y ~ x, data)` function and save it in `bayes_lifeExp`.

```{r}


bayes_lifeExp<-stan_glm(lifeExp ~ continent, data = gapminder2007, refresh = 0)

print(bayes_lifeExp, digits = 2, detail = FALSE)
```

#### Interpreting Coefficients 

Remember, now that we are using a categorical explanatory variable `continent`, our model will not yield a “best-fitting” regression line, but rather offsets relative to a baseline for comparison. Remeber, also that each of these coefficients is the median of the posterior distribution for each parameter.

For example, we can take a look at the posterior distribution for the `continentAmericas` parameter. Calling `as.matrix` on the model, we can access all the posterior draws to then plot in a histogram:

```{r}
sims <- as.matrix(bayes_lifeExp)



posterior_draws<-sims%>%as_tibble()


Americas<-posterior_draws%>%
  ggplot(aes(x = continentAmericas)) +
  geom_histogram()+
   geom_vline(xintercept=median(posterior_draws$continentAmericas), color="red", size=1) + labs(y = "Frequency", title = "Posterior Distribution", subtitle = "The Red Line Represents the Median")

Americas

```

```{r, echo=FALSE, eval=FALSE}

# code for all the parameters posterior distributions

# posterior_draws%>%
#   pivot_longer(cols = c("(Intercept)", "sigma", starts_with("continent")), 
#                names_to = "Parameter", 
#                values_to = "draws")%>%
#   ggplot(aes(x = draws)) + 
#   geom_histogram() +
#   facet_wrap(~Parameter)

```

Now, let us break down the coefficients from the `print()` function:

```{r}
print(bayes_lifeExp, digits = 2, detail = FALSE)
```

```{r, echo=FALSE}
# fitting the model 

bayes_lifeExp%>%
tidy(conf.int = TRUE) 


estimates_2 <- bayes_lifeExp %>%
  tidy() %>%
  pull(estimate)
```
The coefficients can be interpreted as follows:

1. `intercept` corresponds to the mean life expectancy of countries in Africa of `r round(estimates_2[1],2)`.
1. `continentAmericas` corresponds to countries in the Americas and the value +`r round(estimates_2[2],2)` is the same as the difference in mean life expectancy relative to Africa. In other words, the mean life expectancy of countries in the Americas is `r round(estimates_2[1],2)` + `r round(estimates_2[2],2)`= `r round(estimates_2[1],2) + round(estimates_2[2],2)`.
1. `continentAsia` corresponds to countries in Asia and the value + `r round(estimates_2[3],2)` is the same as the difference in mean life expectancy relative to Africa. The mean life expectancy of countries in Asia is `r round(estimates_2[1],2) + round(estimates_2[3],2)`. 
1. `continentEurope` corresponds to countries in Europe and the value +`r round(estimates_2[4],2)` is the same as the difference in mean life expectancy relative to Africa. The mean life expectancy of countries in Europe is `r round(estimates_2[1],2) + round(estimates_2[4],2)`.
1. `continentOceania` corresponds to countries in Oceania and the value +`r round(estimates_2[5],2)` is the same as the difference in mean life expectancy relative to Africa. The mean life expectancy of countries in Oceania is `r round(estimates_2[1],2) + round(estimates_2[5],2)`.


The model also has a sigma, or residual standard error, of `r round(sigma(bayes_lifeExp),2)`. Thus, sigma tells us that a country's life expectancy will be between plus or minus `r round(sigma(bayes_lifeExp),2)` the prediction based on what continent the country is in for 68% of the data. Aslo, a country's life expectancy will be between plus or minus two sigma, `r 2*round(sigma(bayes_lifeExp),2)`, of the prediction for about 95% of the data given which continent the country is in.

Finally, for all of the coefficients, we are provided a MAD_SD. We can use it to create each coefficient's credible interval. 

```{r}
posterior_interval(bayes_lifeExp, prob = 0.95)
```


Each of these tell us there is a 95% probability that the true value of each parameter will fall within the respective interval.

Now, we can call `summary()` on our `bayes_lifeExp` model:
```{r}
summary(bayes_lifeExp)
```
Again, the few things to check from summary other than tht estimates are that the `mean_ppd` is in accords with the mean of the outcome variable, in our case `life_Exp`, and that all the `Rhat`s are close to 1. The mean of the outcome variable is `r mean(gapminder2007$lifeExp)`, thus the `mean_ppd` is very close. Also all of the `Rhat`s are 1.0, signifying that our model converged and that it is reliable. 

###Prediction

Finally, we will make prediction about new data using our `stan_glm()` model. Let us create a new dataset of continents. To do this, we will create a tibble containing the five various continents from the `gaminder2007` dataset. 

```{r}

new <- tibble(continent = c("Asia", "Africa", "Americas", "Europe", "Oceania"))

new
```

We know a regression with categorical variable does not yield a line, but rather offsets relative to a baseline for comparison. Thus, `posterior_linepred()` will help us quantify uncertainty in the paramets, but it cannot be interpreted using lines. Remember, we can apply `add_fitted_draws()` instead of `posterior_linepred` because it provides the data in a tidy manner. Also, remember that the `mean` for each group of predictions, `continent`, corresponds to the point estimate. 

```{r}
new %>%
  add_fitted_draws(model = bayes_lifeExp, .)%>%
  group_by(continent)%>%
  mutate(point_estimate = mean(.value))%>%
  ggplot(aes(x = continent, y = point_estimate)) +
  stat_interval(aes(x = continent, y = .value), .width = c(0.68, 0.95)) +
  geom_point(aes(x = continent, y = point_estimate)) +
  geom_line() +
  labs(y = "Predicted Life Expectancy", color = "Uncertainty Level", title = "Posterior Linear Prediction With Uncertainty") + coord_flip()
```

From the graph, we can see variation in the point estimates for each `continent` with Africa having a signicantly lower life expectancy. From the printed output, we continentOceania had the highest coefficient, meaning the greatest difference in mean life expectancy relative to Africa. Thus, we would expect Oceania to have highest life expectancy in comparison to the other continents; however, we see their is a very wide interval. This suggests their is a lot of uncertainty in the Oceania parameter. This is perhaps due to there not being as many countries on the continent of Oceania and there are vast differences in life expectancy between the countries in the dataset. Remember that the more data their is, the more accurate our posterior predictions will be and as we can see from above, the effect of the absence of data is shown in the uncertainty in the Oceania linear predictions. 


Alternatively, we can use `posterior_predict()` to understand the posterior predicitive distribution for each continent. Remember we can perform `add_predicted_draws()` instead of `posterior_predict` because it provides the data in a tidy manner. 
```{r}


new %>%
  add_predicted_draws(., model = bayes_lifeExp)%>%
  ggplot(aes(x = .prediction, y = continent)) +
  stat_halfeyeh() + 
  labs(x = "Predicted Life Expectancy", title = "Posterior Predictive Distribution For Each Continent's Life Expectancy")

```

Unlike our predictive distribution from `bayes_score` model in the previous section, we see a lot more differentiation between the categories. Although there is overlap at the tail, African Life Expectancy appears to be significantly lower than Life Expectancy on other continents. This plot shows the predictions are more stable. We can be more assured that there are actaul differences in predicted life expectancy between continents, which is something we would not be privy to using only point predictions.

Finally, one other way to display the posterior predictive distribution is using `stat_intervalh()`. 

```{r}



new %>%
  add_predicted_draws(., model = bayes_lifeExp)%>%
  group_by(continent) %>%
  mutate(median_prediction = median(.prediction)) %>%
  ggplot(aes(x = .prediction, y = continent)) +
  stat_intervalh() + 
  geom_point(aes(x = median_prediction, y = continent)) + 
  labs(x = "Predicted Life Expectancy", title = "Posterior Predictive Distribution For Each Continent's Life Expectancy")
```
By default the uncertainty levels are `0.5,0.8, 0.95` but those can be altered with the `.width` input to your specification such as `.width = c(0.68, 0.95)`.


## Conclusion


In this chapter, you've studied the term _basic regression_, where you fit models that only have one explanatory variable. In Chapter \@ref(continuous-response), we'll study *multiple regression*, where our regression models can now have more than one explanatory variable! In particular, we'll consider two scenarios: regression models with one numerical and one categorical explanatory variable and regression models with two numerical explanatory variables. This will allow you to construct more sophisticated and more powerful models, all in the hopes of better explaining your outcome variable $y$.
