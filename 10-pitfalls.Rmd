---
output_yaml:
  - _output.yml
---

<!-- DK: To discuss. What belongs here? Overfitting? Type m/s errors? Diff-in-diff, RDD, et cetera? -->

<!-- To Do: -->

<!-- 1. Read chapter 5. Set up Zoom with Vivian.   -->

<!-- 2. Read the latest version of themes.Rmd. Some of it is inspired but what was in the previous version of this chapter. But it is now more serious. So, it might be that we just cut everything here and start again. -->

<!-- Outline: -->

<!-- Preamble: Life is hard. Things were difficult in Chapter 9. Let us teach you some stuff to make your life easier! And you will get to practice in Chapters 11 and 12. Review themes.Rmd briefly, in two or three sentences. Review the key dificulties which were discussed in chapter 9: model selection, overfitting and causal inference. Finish with specific questions. A new professor is planning to teach a course. She knows the work she will assign and the enrollment. What will her Q scores be?   -->

<!-- 1. EDA of qscores. Keep all the rows for now. Change rating to rating*100. (That will make coefficients easy to deal with later.) -->

<!-- 2. rating ~ term + enrollment + hours. Do this first for just MUSIC classes. All with rstanarm. Fully explain.  All the way through posterior_linpred() and posterior_predict(). Add key items from style.Rmd, e.g., parameter uncertainty, unmodelled variation and so on. -->

<!-- 3. Explore the issue of which model to use. We did the above, and answered the question, using just data from Music courses. That seems sensible if it is a Music professor who asked the question. But isn't the other data relevant also? What would be the answer to her question if we used all the data to answer it? The answer will be different. Which is correct? Discuss. Key: There is not a single best model. Indeed, there is not a single best set of data to include. There is only a process, and the assumptions we make. -->

<!-- 4. Figuring out causal effects from observational data. Doing so stupidly. Doing so smart. We need a Rubin Causal Model section.  Set up a Zoom with Cass to discuss. She is a good source for information about how to make nice looking tables and about how to think about the issue. We might use qscores and just pretend/assume random assignment or we might use a different data set. Key is to set up a Rubin Table in which the potential outcomes are clear. For example, what would my rating be in a assigned 5 hours of work? What about 10 hours? 15 hours? Each of these is a differnt treatment and, therefore, generates a different potential outcome. -->

<!-- 5. The other key tool set is the one for tidymodels. We need a thorough discussion of them. I think we can put stan_glm within the tidymodels framework. So, do that! -->

<!-- 6. Use tidymodels framework to overfitting/underfitting and how we use bootstrap and cross-validation to fight against it.  -->

<!-- Other comments: -->

<!-- The key goal of this chapter is to walk students through all the most common pitfalls with creating models and then show them how to deal with those pitfalls. Then, in chapters 11 and 12, students put what we have taught them to work. The problem is that I only have a hazy idea of what the key pitfalls are and how to deal with them. -->


<!-- Type M/S errors. Seeing effects when none are there. -->


<!-- 5. Deriving causal estimates from observational data. -->

<!-- 6. Prediction only. rating ~ hours + department + enrollment + enrollment*course + whatever. Follow the Gelman advice on how to make a regression. What model do I make? Why? Think about goals? Teach tidymodels syntax. -->

<!-- 7. underfitting versus overfitting. Big idea. How to deal with? Bayes? Overfitting/underfitting. It would be nice to nail all the complexities of this down. Should we even bother to explore it in chapter 9? Holdout samples? -->

<!-- 8. Finish by answering question. -->


<!-- 

Tidymodels syntax. Again, get this settled before the "graduation exercise" of chapters 11/12.

M and S errors. This stuff is so important that it deserves placement in the main body of the book. Also, the governors data provides a great example to work with. Just use Gelman's blog post!

Estimating causal relationships which are somewhere between purely observational and perfect experiments: regression discontinuity designs, difference-in-difference and so on.

Dealing with parameter uncertainty correctly. That is, in previous chapters, we treated the betas as estimated perfectly. We did not incorporate uncertainty in their estimation into our prediction intervals. (Unless we did that with rstanarm in chapter 9. Or maybe this is a reason to save rstanarm for this chapter.)

Perhaps all this is enough to both fill this chapter and set the stage for 11 and 12.

-->


<!-- 6. We explicitly avoid talking about Bayesian models here because we are only estimating a one or two parameters in each of the models above. So, there is no occasion for pooling, or any of the other Bayesian magic. But you can see how estimating a 100 rating ~ hours models, one for each department, lends itself to a Bayesian approach. On to chapter 11. -->

<!-- Packages: tidyverse, broom -->

<!-- Commands:  -->

<!-- How do we solve the problems which were identified in chapter 9? -->

<!-- no bootstrap;  introduce tidymodels here for first time; -->

<!-- holdout sample, cross validation, machine learning, test  -->

<!-- DK: Maybe build this from tidymodels, while also mentioning the traditional way of just using lm()? The problem with raw lm() is that it does not work natively in a pipe since the first argument is a formula instead of the data. (Or maybe a lm(y ~ x, data = .) hack is OK?) -->

<!-- DK: Change the smoking example to political campaigns. If you work on the NYT, all you care about is forecasting election results conditional on campaign spending. If you are a (rich!) candidate, you care about the causal effect of spending on votes. Same model might be estimated by both! But the latter needs to be much more careful in deciding whether or not the results are real. -->

<!-- Albert points out a difficulty in combining the RCM with regression. You can't easily put in a distribution for the unknown potential outcome, even if you have a good regression model. You can't just add to the observed outcome because . . . actually I am confused about this!  -->

# Pitfalls {#pitfalls}

The fundamental goal of data modeling is to make explicit the relationship between:

* an *outcome variable* $y$, also called a *dependent variable* or response variable, and  
* an *explanatory/predictor variable* $x$, also called an *independent variable* or  covariate.

Another way to state this is using mathematical terminology: we will model the outcome variable $y$ "as a function" of the explanatory/predictor variable $x$. When we say "function" here, we aren't referring to functions in R like the `ggplot()` function, but rather to a mathematical function. But, why do we have two different labels, explanatory and predictor, for the variable $x$? That's because even though the two terms are often used interchangeably, roughly speaking data modeling serves one of two purposes:

1. **Modeling for explanation**: When you want to explicitly describe and quantify the relationship between the outcome variable $y$ and an explanatory variable $x$, determine the importance of any relationships, have measures summarizing these relationships, and possibly identify any *causal* relationships between the variables.  (What's a causal relationship? Remember the [Rubin Causal Model](#rubin-causal-model)! The *causal effect* of $x$ on $y$ is the difference in *potential outcomes* of $y$ given different values of $x$.)
1. **Modeling for prediction**: When you want to predict an outcome variable $y$ based on the information contained in a set of predictor variables $x$. Unlike modeling for explanation, however, you don't care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about $y$ using the information in $x$.

For example, say you are interested in an outcome variable $y$ of whether patients develop lung cancer and information $x$ on their risk factors, such as smoking habits, age, and socioeconomic status. If we are modeling for explanation, we would be interested in both describing and quantifying the effects of the different risk factors. One reason could be that you want to design an intervention to reduce lung cancer incidence in a population, such as increasing family income.  In that case, you would want to know the causal effect of income on the incidence of lung cancer.

If we are modeling for prediction, however, we wouldn't care so much about understanding how all the individual risk factors contribute to lung cancer, but rather only whether we can make good predictions of which people will contract lung cancer.

<!-- DK: Find a way to use this reference: [*An Introduction to Statistical Learning with Applications in R (ISLR)*](http://www-bcf.usc.edu/~gareth/ISL/)  -->

Linear regression involves a *numerical* outcome variable $y$ and explanatory variables $x$ that are either *numerical* or *categorical*. Furthermore, the relationship between $y$ and $x$ is assumed to be linear, or in other words, a line. However, we'll see that what constitutes a "line" will vary depending on the nature of your explanatory variables $x$.

<!-- DK: Could give a better plan overview, including discussion of chapters 11 and 12. Indeed, perhaps also looking backward to sampling and uncertainty. Need to rewrite this if we re-organize the book. Indeed, the introductions (and conclusions) to each chapter should be similar, providing a framework in which that chapter fits. -->

In Section \@ref(model1), the explanatory variable will be numerical. This scenario is known as *simple linear regression*. In Section \@ref(model2), the explanatory variable will be categorical.

In Chapter \@ref(continuous-response) on multiple regression, we'll extend the ideas behind basic regression and consider models with two explanatory variables $x_1$ and $x_2$.  In Section \@ref(model4), we'll have two numerical explanatory variables. In Section \@ref(model3), we'll have one numerical and one categorical explanatory variable. In particular, we'll consider two such models: *interaction* and *parallel slopes* models.

Let's now begin with basic regression, \index{regression!basic} which refers to linear regression models with a single explanatory variable $x$. We'll also discuss important statistical concepts like the *correlation coefficient*, that "correlation isn't necessarily causation," and what it means for a line to be "best-fitting."

Let's now load all the packages needed for this chapter (this assumes you've already installed them). The main packages are ones we have used before. The Advanced Section of the chapter makes use of

1. The **rstanarm** package, which provides an interface to the statistical inference engine, Stan, for Bayesian Regression Modeling. 
1. The **tidybayes** package, which aids in formating Bayesian modeling outputs in a tidy manner and provides ggplot geoms for plotting. 
1. The **broom.mixed** package, which provides broom-type functions for the output objects generated by **rstanarm**. 

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(PPBDS.data)
library(broom)
library(broom.mixed)
library(skimr)
library(gapminder)
library(rstanarm)
library(tidybayes)
```


```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text.
library(mvtnorm)
library(kableExtra)
library(patchwork)
```


## Teaching evaluations: one numerical explanatory variable {#model1}


<!-- EG: I really like this section- I think that the in-depth explanations of not only how to find correlation coefficients but also interpret them accurately and effectively is great. -->

<!-- EG: I'll change this to qscores, along with an adjusted EDA for that dataset and more explanation of how correlation != causation. I'll also provide more investigation into the many ways confounding variables could impact why students provide certain qscores rather than simply hours of work, along with more language of comparison. -->


Why do some professors and instructors at universities and colleges receive high teaching evaluations scores from students while others receive lower ones? Are there differences in teaching evaluations between instructors of different demographic groups? Could there be an impact due to student biases? These are all questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which instructors and professors get promoted.

In this section, we'll keep things simple for now and try to explain differences in instructor ratings within the Harvard music department based on average hourly workload for that class. Could it be that instructors with lower hourly workloads also have higher ratings? Could it be instead that instructors with lower hourly workloads tend to have lower ratings? Or could it be that there is no relationship between workload and teaching evaluations? We'll answer these questions by modeling the relationship between rating and workload using *simple linear regression* \index{regression!simple linear} where we have:

1. A numerical outcome variable $y$ (the instructor's teaching rating) and
1. A single numerical explanatory variable $x$ (the average hourly workload for the class).

### Exploratory data analysis {#model1EDA}

The data on the Q Guide Music Department ratings can be found in the `qscores` data frame included in the **PPBDS.data** package. However, to keep things simple, let's `select()` only the subset of the variables we'll consider in this chapter, and save this data in a new data frame called `qscores_ch10`:

```{r}
library(PPBDS.data)

qscores_ch10 <- qscores %>%
  mutate(rating = rating*100) %>%
  filter(department == "MUSIC") %>%
  select(number, rating, hours, enrollment)

```

A crucial step before doing any kind of analysis or modeling is performing an *exploratory data analysis*, \index{data analysis!exploratory} or EDA for short. EDA gives you a sense of the distributions of the individual variables in your data, whether any potential relationships exist between variables, whether there are outliers and/or missing values, and (most importantly) how to build your model. Here are three common steps in an EDA:

<!-- DK: Good stuff. We should keep this and follow it, each chapter. -->

1. Most crucially, looking at the raw data values.
1. Computing summary statistics, such as means, medians, and interquartile ranges.
1. Creating data visualizations.

Let's perform the first common step in an exploratory data analysis: looking at the raw data values. Because this step seems so trivial, unfortunately many data analysts ignore it. However, getting an early sense of what your raw data looks like can often prevent many larger issues down the road. 

You can do this by using RStudio's spreadsheet viewer or by using the `glimpse()` function as introduced in Subsection \@ref(exploredataframes) on exploring data frames:

<!-- DK: Add summary() -->

```{r}
glimpse(qscores_ch10)

glimpse(qscores_ch10) %>%
  summary()

```

<!-- EG: Should I just keep glimpse() with summary(), or should I include an analysis of glimpse() alone before doing both? -->

Observe that `Rows: 14` indicates that there are 14 rows/observations in `qscores_ch10`, where each row corresponds to one observed music course at Harvard. It is important to note that the *observational unit* \index{observational unit} is an individual course and not an individual instructor. Recall from Subsection \@ref(exploredataframes) that the observational unit is the "type of thing" that is being measured by our variables. Since instructors teach more than one course in an academic year, the same instructor will appear more than once in the data. Hence there are fewer than 748 unique instructors being represented in `qscores_ch10`.

To further explore the data, we can add a second function to our initial call of `glimpse()`: the `summary()` function. `summary()` can provide us with useful result summaries of all the observations in a dataset for each variable. Calling `glimpse()` on `qscores_ch10` and then `summary()` allows us to see the minimum and maximum values along with several other quantiles for numeric variables and tells us the number of observations, class, and mode of categorical variables. For instance, examining the `hours` column in the `summary()` results shows that the minimum number of workload hours reported for a Harvard music class in the Q Guide was 2.5 hours, while the median was 3.5 hours and the maximum was 5.2 hours.

<!-- EG: Commenting this out for the moment "A full description of all the variables included in `evals` can be found at [openintro.org](https://www.openintro.org/data/index.php?data=evals) or by reading the associated help file (run `?evals` in the console)." Is there a place where we have descriptions of the PPBDS library data like this? --> 

However, let's fully describe only the `r ncol(qscores_ch10)` variables we selected in `qscores_ch10`:

1. `number`: An identification variable used to distinguish among courses within the same department. Courses in different departments may have the same number.
1. `rating`: A numerical variable of the overall quality of a course, where the average is computed from the evaluation scores from all the students who choose to provide feedback for that course.  Ratings of 1 are lowest and 5 are highest. This is the outcome variable $y$ of interest.
1. `hours`: A numerical variable of the amount of work students put into a course per week in hours, where the average is computed from the evaluation scores from all students who choose to provide feedback for that course. This is the explanatory variable $x$ of interest.
1. `enrollment`: A numerical variable of the amount of students enrolled in a course.

An alternative way to look at the raw data values is by choosing a random sample of the rows in `qscores_ch10` by piping it into the `sample_n()` \index{dplyr!sample\_n()} function from the **dplyr** package. Here we set the `size` argument to be `5`, indicating that we want a random sample of 5 rows. We display the results below. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

```{r, eval=FALSE}
qscores_ch10 %>%
  sample_n(size = 5)
```

```{r, echo=FALSE, fig.cap="A random sample of 5 out of the 748 courses at Harvard"}
qscores_ch10 %>%
  sample_n(5) 
```

<!-- EG: Should we still include summarize() if we're including summary() with glimpse() above? Summary() seems to include more information and is generally more useful. Or should we simply compare the two and advise when one could be more useful than the other? -->

Now that we've looked at the raw values in our `qscores_ch10` data frame and got a preliminary sense of the data, let's move on to the next common step in an exploratory data analysis: computing summary statistics. Let's start by computing the mean and median of our numerical outcome variable `rating` and our numerical explanatory variable `hours`. We'll do this by using the `summarize()` function from `dplyr` along with the `mean()` and `median()` summary functions we saw in Section \@ref(summarize).

```{r, eval=TRUE}
qscores_ch10 %>%
  summarize(mean_hours = mean(hours),
            mean_rating = mean(rating),
            median_hours = median(hours),
            median_rating = median(rating))
```

<!-- DK: This is nice. Having motivated the use of skim() once, we can just go straight to using it in other chapters. And/or show other tricks each chapter, like across(). -->

However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles? 

Typing out all these summary statistic functions in `summarize()` would be long and tedious. Instead, let's use the convenient `skim()` function from the **skimr** package. This function takes in a data frame, "skims" it, and returns commonly used summary statistics. Let's take our `qscores_ch10` data frame, `select()` only the outcome and explanatory variables teaching `score` and `hours`, and pipe them into the `skim()` function:

```{r}
qscores_ch10 %>% 
  select(rating, hours) %>% 
  skim()
```

For the numerical variables teaching `rating` and `hours` it returns:

- `n_missing`: the number of missing values
- `complete_rate`: the percentage of non-missing or complete values
- `mean`: the average
- `sd`: the standard deviation
- `p0`: the 0th percentile: the value at which 0% of observations are smaller than it (the *minimum* value)
- `p25`: the 25th percentile: the value at which 25% of observations are smaller than it (the *1st quartile*)
- `p50`: the 50th percentile: the value at which 50% of observations are smaller than it (the *2nd* quartile and more commonly called the *median*)
- `p75`: the 75th percentile: the value at which 75% of observations are smaller than it (the *3rd quartile*)
- `p100`: the 100th percentile: the value at which 100% of observations are smaller than it (the *maximum* value)

Looking at this output, we can see how the values of both variables are distributed. For example, the mean music course rating was 4.41 out of 5, whereas the mean workload per week was 3.46 hours. Furthermore, the middle 50% of course ratings was between 4.2 and 4.8 (the first and third quartiles), whereas the middle 50% of hours of work falls within 2.87 to 3.7, with a maximum reported workload of 5.2 hours per week.

<!-- DK: Keep this. -->

The `skim()` function only returns what are known as *univariate* \index{univariate} summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist *bivariate* \index{bivariate} summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the \index{correlation (coefficient)} *correlation coefficient*. Generally speaking, *coefficients* are quantitative expressions of a specific phenomenon.  A *correlation coefficient* is a quantitative expression of the *strength of the linear relationship between two numerical variables*. Its value ranges between -1 and 1 where:

* -1 indicates a perfect *negative relationship*: As one variable increases, the value of the other variable tends to go down, following a straight line.
* 0 indicates no relationship: The values of both variables go up/down independently of each other.
* +1 indicates a perfect *positive relationship*: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion.

The following figure gives examples of 9 different correlation coefficient values for hypothetical numerical variables $x$ and $y$. For example, observe in the top right plot that for a correlation coefficient of -0.75 there is a negative linear relationship between $x$ and $y$, but it is not as strong as the negative linear relationship between $x$ and $y$ when the correlation coefficient is -0.9 or -1.

```{r, echo=FALSE, fig.cap="Nine different correlation coefficients."}
correlation <- c(-0.9999, -0.9, -0.75, -0.3, 0, 0.3, 0.75, 0.9, 0.9999)
n_sim <- 100
values <- NULL
for(i in seq_along(correlation)){
  rho <- correlation[i]
  sigma <- matrix(c(5, rho * sqrt(50), rho * sqrt(50), 10), 2, 2)
  # EG- should we consider explaining more about what is happening within "sim"? I personally haven't used rmvnorm before and am unsure whether it's been discussed in a previous chapter, or whether some explanation/clarification could be helpful.
  sim <- rmvnorm(
    n = n_sim,
    mean = c(20,40),
    sigma = sigma
    ) %>%
    as.data.frame() %>% 
    as_tibble() %>%
    mutate(correlation = round(rho,2))

  values <- bind_rows(values, sim)
}

corr_plot <- ggplot(data = values, mapping = aes(V1, V2)) +
  geom_point() +
  facet_wrap(~ correlation, ncol = 3) +
  labs(x = "x", y = "y") +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank())

if(knitr::is_latex_output()){
  corr_plot +
  theme(
    strip.text = element_text(colour = 'black'),
    strip.background = element_rect(fill = "grey93")
  )
} else {
  corr_plot
}
```


The correlation coefficient can be computed using the `cor()` summary function within a `summarize()`:

```{r, eval=FALSE}
qscores_ch10 %>% 
  summarize(correlation = cor(rating, hours))
```

```{r, echo=FALSE}
cor_ch10 <- qscores_ch10 %>%
  summarize(correlation = cor(rating, hours)) %>% 
  round(3) %>% 
  pull()
```

In our case, the correlation coefficient of `r cor_ch10` indicates that the relationship between overall course rating and average weekly workload in hours is negative. There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren't close to the extreme values of -1, 0, and 1. 

Let's now perform the last of the steps in an exploratory data analysis: creating data visualizations. Since both the `rating` and `hours` variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let's do this using `geom_point()` and display the result. Furthermore, let's highlight the six points in the top right of the visualization in a box.

```{r, eval=FALSE}
qscores_ch10 %>%
  ggplot(aes(x = hours, y = rating)) +
  geom_point() +
  labs(x = "Hours of Work Per Week", 
       y = "Q Guide Rating",
       title = "Scatterplot of relationship between Q Scores and Weekly Workload")
```

```{r, warning=FALSE, echo=FALSE, fig.cap="Q Guide Scores at Harvard", fig.height=4.5}
# Define orange box
# EG: I need to fix this later- how should I define the box?
margin_x <- 0.15
margin_y <- 0.075
box <- tibble(
  x = c(7.83, 8.17, 8.17, 7.83, 7.83) + c(-1, 1, 1, -1, -1) * margin_x,
  y = c(4.6, 4.6, 5, 5, 4.6) + c(-1, -1, 1, 1, -1) * margin_y
  )

qscores_ch10 %>%
  ggplot(aes(x = hours, y = rating)) +
  geom_point() +
  labs(x = "Hours of Work Per Week", 
       y = "Q Guide Rating",
       title = "Scatterplot of relationship between Q Scores and Weekly Workload") +
  geom_path(data = box, aes(x=x, y=y), col = "orange", size = 1)
```
<!-- EG: Need to be more precise with this in the future. Will come back and edit later. -->

Observe that most courses have reported average workloads between 2 and 10 hours per week, while most teaching scores lie between 3 and 5. Furthermore, while opinions may vary, it is our opinion that the relationship between Q guide rating and weekly workload in hours is "weakly negative." This is consistent with our earlier computed correlation coefficient of `r cor_ch10`.

<!-- EG: I'll change this language below as well once I figure out how to work on the box. -->

Furthermore, there appear to be six points in the top-right of this plot highlighted in the box. However, this is not actually the case, as this plot suffers from *overplotting*. Recall from Subsection \@ref(overplotting) that overplotting occurs when several points are stacked directly on top of each other, making it difficult to distinguish them. So while it may appear that there are only six points in the box, there are actually more.  This fact is only apparent when using `geom_jitter()` in place of `geom_point()`. We display the resulting plot along with the same small box as before.

```{r, eval=FALSE}
qscores_ch10 %>%
  ggplot(aes(x = hours, y = rating)) +
  geom_jitter() +
  labs(x = "Hours of Work Per Week", y = "Q Guide Score",
       title = "Scatterplot of relationship between Q Scores and Weekly Workload")
```

```{r, warning=FALSE, echo=FALSE, fig.cap="Q Guide Scores at Harvard.", fig.height=4.2}
qscores_ch10 %>%
  ggplot(aes(x = hours, y = rating)) +
  geom_jitter() +
  labs(x = "Hours of Work Per Week", y = "Q Guide Score",
       title = "Scatterplot of relationship between Q Scores and Weekly Workload for Music Classes") +
  geom_path(data = box, aes(x = x, y = y), col = "orange", size = 1)
```

<!-- EG: I'll change this section as well after changing the box size. -->

It is now apparent that there are ?? points in the area highlighted in the box and not six as originally suggested. Recall from Subsection \@ref(overplotting) on overplotting that jittering adds a little random "nudge" to each of the points to break up these ties. Furthermore, recall that jittering is strictly a visualization tool; it does not alter the original values in the data frame `qscores_ch10`. To keep things simple going forward, however, we'll only present regular scatterplots rather than their jittered counterparts.

Let's build on the unjittered scatterplot by adding a "best-fitting" line: of all possible lines we can draw on this scatterplot, it is the line that "best" fits through the cloud of points. We do this by adding a new `geom_smooth(method = "lm", se = FALSE)` layer to the `ggplot()` code that created the scatterplot. The `method = "lm"` argument sets the line to be a "`l`inear `m`odel." The `se = FALSE` \index{ggplot2!geom\_smooth()} argument suppresses _standard error_ uncertainty bars. (We defined the concept of _standard error_ in Subsection \@ref(sampling-definitions).)

```{r, warning=FALSE, fig.cap="Regression line."}
qscores_ch10 %>%
  ggplot(aes(x = hours, y = rating)) +
  geom_jitter() +
  labs(x = "Hours of Work Per Week", y = "Q Guide Score",
       title = "Scatterplot of relationship between Q Scores and Weekly Workload for Music Classes") + 
  geom_smooth(method = "lm", se = FALSE)
```

The line in the resulting figure is called a "regression line." The regression line \index{regression!line} is a visual summary of the relationship between two numerical variables, in our case the outcome variable `rating` and the explanatory variable `hours`. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of `r cor_ch10` suggesting that there is a negative relationship between these two variables: as students report higher average weekly workloads for music classes, courses receive lower teaching evaluations. We'll see later, however, that while the correlation coefficient and the slope of a regression line always have the same sign (positive or negative), they typically do not have the same value.

Furthermore, a regression line is "best-fitting" in that it minimizes some mathematical criteria. We present these mathematical criteria in Section \@ref(leastsquares), but we suggest you read this subsection only after first reading the rest of this section on regression with one numerical explanatory variable.
